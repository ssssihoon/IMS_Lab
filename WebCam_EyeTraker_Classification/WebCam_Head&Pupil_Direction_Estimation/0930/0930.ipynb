{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "081d5afb-b0e9-45dd-ade4-3327d399915a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 155ms/step - accuracy: 0.7678 - loss: 1.2115 - val_accuracy: 0.8700 - val_loss: 0.3818\n",
      "Epoch 2/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 157ms/step - accuracy: 0.9405 - loss: 0.2058 - val_accuracy: 0.9800 - val_loss: 0.0565\n",
      "Epoch 3/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 158ms/step - accuracy: 0.9864 - loss: 0.0584 - val_accuracy: 0.9900 - val_loss: 0.0258\n",
      "Epoch 4/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 155ms/step - accuracy: 0.9825 - loss: 0.0862 - val_accuracy: 0.9829 - val_loss: 0.0451\n",
      "Epoch 5/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 153ms/step - accuracy: 0.9869 - loss: 0.0464 - val_accuracy: 0.9857 - val_loss: 0.0436\n",
      "Epoch 6/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m418s\u001b[0m 149ms/step - accuracy: 0.9914 - loss: 0.0300 - val_accuracy: 0.7600 - val_loss: 1.3844\n",
      "Epoch 7/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 156ms/step - accuracy: 0.9903 - loss: 0.0295 - val_accuracy: 0.9886 - val_loss: 0.0318\n",
      "Epoch 8/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 153ms/step - accuracy: 0.9941 - loss: 0.0197 - val_accuracy: 0.9871 - val_loss: 0.0313\n",
      "Epoch 9/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 155ms/step - accuracy: 0.9964 - loss: 0.0194 - val_accuracy: 0.9843 - val_loss: 0.0761\n",
      "Epoch 10/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 154ms/step - accuracy: 0.9913 - loss: 0.0325 - val_accuracy: 0.9714 - val_loss: 0.1247\n",
      "Epoch 11/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 151ms/step - accuracy: 0.9921 - loss: 0.0278 - val_accuracy: 0.9800 - val_loss: 0.0794\n",
      "Epoch 12/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 150ms/step - accuracy: 0.9987 - loss: 0.0051 - val_accuracy: 0.9700 - val_loss: 0.0979\n",
      "Epoch 13/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 151ms/step - accuracy: 0.9930 - loss: 0.0460 - val_accuracy: 0.9743 - val_loss: 0.0816\n",
      "Epoch 14/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 152ms/step - accuracy: 0.9977 - loss: 0.0154 - val_accuracy: 0.9786 - val_loss: 0.0887\n",
      "Epoch 15/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 152ms/step - accuracy: 0.9980 - loss: 0.0081 - val_accuracy: 0.9886 - val_loss: 0.0329\n",
      "Epoch 16/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 149ms/step - accuracy: 0.9980 - loss: 0.0074 - val_accuracy: 0.9929 - val_loss: 0.0240\n",
      "Epoch 17/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 151ms/step - accuracy: 0.9936 - loss: 0.0446 - val_accuracy: 0.9871 - val_loss: 0.0438\n",
      "Epoch 18/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 3.6908e-05 - val_accuracy: 0.9871 - val_loss: 0.0503\n",
      "Epoch 19/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 153ms/step - accuracy: 0.9990 - loss: 0.0068 - val_accuracy: 0.9800 - val_loss: 0.0727\n",
      "Epoch 20/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 152ms/step - accuracy: 0.9994 - loss: 0.0030 - val_accuracy: 0.9871 - val_loss: 0.0497\n",
      "Epoch 21/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 148ms/step - accuracy: 0.9946 - loss: 0.0307 - val_accuracy: 0.9871 - val_loss: 0.0852\n",
      "Epoch 22/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 148ms/step - accuracy: 0.9997 - loss: 0.0013 - val_accuracy: 0.9786 - val_loss: 0.0892\n",
      "Epoch 23/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 149ms/step - accuracy: 0.9967 - loss: 0.0258 - val_accuracy: 0.9629 - val_loss: 0.7078\n",
      "Epoch 24/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 147ms/step - accuracy: 0.9993 - loss: 0.0034 - val_accuracy: 0.9686 - val_loss: 0.1302\n",
      "Epoch 25/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 9.2357e-05 - val_accuracy: 0.9700 - val_loss: 0.1988\n",
      "Epoch 26/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 149ms/step - accuracy: 0.9966 - loss: 0.0196 - val_accuracy: 0.9957 - val_loss: 0.0177\n",
      "Epoch 27/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 147ms/step - accuracy: 0.9980 - loss: 0.0148 - val_accuracy: 0.9714 - val_loss: 0.6222\n",
      "Epoch 28/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m411s\u001b[0m 147ms/step - accuracy: 0.9980 - loss: 0.0073 - val_accuracy: 0.9786 - val_loss: 0.2130\n",
      "Epoch 29/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 149ms/step - accuracy: 0.9986 - loss: 0.0104 - val_accuracy: 0.9843 - val_loss: 0.1586\n",
      "Epoch 30/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 155ms/step - accuracy: 0.9999 - loss: 6.3047e-05 - val_accuracy: 0.9700 - val_loss: 0.2407\n",
      "Epoch 1/30\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 148ms/step - accuracy: 0.7635 - loss: 1.3627 - val_accuracy: 0.9100 - val_loss: 0.3978\n",
      "Epoch 2/30\n",
      "\u001b[1m1334/2796\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3:30\u001b[0m 144ms/step - accuracy: 0.9325 - loss: 0.2181"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m model_left\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     94\u001b[0m history_right \u001b[38;5;241m=\u001b[39m model_right\u001b[38;5;241m.\u001b[39mfit(right_images_train, labels_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(right_images_val, labels_val))\n\u001b[1;32m---> 95\u001b[0m history_left \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_left\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_images_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mleft_images_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m train_accuracy_right \u001b[38;5;241m=\u001b[39m history_right\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     98\u001b[0m val_accuracy_right \u001b[38;5;241m=\u001b[39m history_right\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\cam\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cam\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:318\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    317\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 318\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    320\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cam\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cam\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cam\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cam\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\cam\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\.conda\\envs\\cam\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\cam\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\cam\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32m~\\.conda\\envs\\cam\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=(128, 128)):\n",
    "    image = load_img(image_path, color_mode='grayscale', target_size=target_size)\n",
    "    image_array = img_to_array(image)\n",
    "    image_array /= 255.0\n",
    "    return image_array\n",
    "\n",
    "def resnet_block(x, filters):\n",
    "    shortcut = x\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "    \n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, kernel_size=(1, 1), padding='same')(shortcut)\n",
    "    \n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation(mish)(x)\n",
    "    return x\n",
    "\n",
    "def create_resnet_model(input_shape, num_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = layers.Conv2D(64, kernel_size=(7, 7), padding='same')(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
    "    \n",
    "\n",
    "    filter_sizes = [64, 128, 256, 512]\n",
    "    for filters in filter_sizes:\n",
    "        for _ in range(2):\n",
    "            x = resnet_block(x, filters)\n",
    "        x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    output_layer = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "folder_path = r\"C:\\Users\\IMS\\Desktop\\Hwangsihoon\\WebCam\\Pupil\\crop_img\"\n",
    "results = pd.read_excel(\"results.xlsx\", header=None)\n",
    "files = glob.glob(os.path.join(folder_path, '*.jpg'))\n",
    "\n",
    "right_images = [file for file in files if 'right' in os.path.basename(file).lower()]\n",
    "left_images = [file for file in files if 'left' in os.path.basename(file).lower()]\n",
    "\n",
    "right_images = np.array([load_and_preprocess_image(img_path) for img_path in right_images])\n",
    "left_images = np.array([load_and_preprocess_image(img_path) for img_path in left_images])\n",
    "labels = results[0].values\n",
    "num_classes = len(np.unique(labels))\n",
    "labels = to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_train_accuracies = []\n",
    "fold_val_accuracies = []\n",
    "\n",
    "model_save_folder = r\"C:\\Users\\IMS\\Desktop\\Hwangsihoon\\WebCam\\0930_resnet_models\"\n",
    "if not os.path.exists(model_save_folder):\n",
    "    os.makedirs(model_save_folder)\n",
    "\n",
    "csv_save_folder = r\"C:\\Users\\IMS\\Desktop\\Hwangsihoon\\WebCam\\0930_resnet_predictions\"\n",
    "if not os.path.exists(csv_save_folder):\n",
    "    os.makedirs(csv_save_folder)\n",
    "\n",
    "for fold_idx, (train_index, val_index) in enumerate(kf.split(right_images)):\n",
    "    right_images_train, right_images_val = right_images[train_index], right_images[val_index]\n",
    "    left_images_train, left_images_val = left_images[train_index], left_images[val_index]\n",
    "    labels_train, labels_val = labels[train_index], labels[val_index]\n",
    "\n",
    "    right_eye_model = create_resnet_model((128, 128, 1))\n",
    "    left_eye_model = create_resnet_model((128, 128, 1))\n",
    "\n",
    "    combined_input = layers.concatenate([right_eye_model.output, left_eye_model.output])\n",
    "\n",
    "    x = layers.Dense(256, activation=mish)(combined_input)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output_layer = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    combined_model = Model(inputs=[right_eye_model.input, left_eye_model.input], outputs=output_layer)\n",
    "\n",
    "    combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = combined_model.fit([right_images_train, left_images_train], labels_train, \n",
    "                                 epochs=30, batch_size=1, validation_data=([right_images_val, left_images_val], labels_val))\n",
    "\n",
    "    train_accuracy = history.history['accuracy'][-1]\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    fold_train_accuracies.append(train_accuracy)\n",
    "    fold_val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Fold {fold_idx+1} - Train Accuracy: {train_accuracy}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "    combined_model.save(os.path.join(model_save_folder, f\"fold_{fold_idx+1}_combined_model.h5\"))\n",
    "\n",
    "    predictions = combined_model.predict([right_images_val, left_images_val])\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    actual_classes = np.argmax(labels_val, axis=1)\n",
    "\n",
    "    data = {\n",
    "        'Right Image': [os.path.basename(files[idx]) for idx in val_index],\n",
    "        'Left Image': [os.path.basename(files[idx]) for idx in val_index],\n",
    "        'Actual Class': actual_classes,\n",
    "        'Predicted Class': predicted_classes\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(os.path.join(csv_save_folder, f\"fold_{fold_idx+1}_predictions.csv\"), index=False)\n",
    "\n",
    "avg_train_accuracy = np.mean(fold_train_accuracies)\n",
    "avg_val_accuracy = np.mean(fold_val_accuracies)\n",
    "\n",
    "print(f'Average Train Accuracy: {avg_train_accuracy}')\n",
    "print(f'Average Validation Accuracy: {avg_val_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3b486e-a1fe-4433-b6b4-abb2add75938",
   "metadata": {},
   "source": [
    "# 20에폭, RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c9ed5a7-53da-4240-9029-d1d065abc0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m677s\u001b[0m 236ms/step - accuracy: 0.6962 - loss: 0.9760 - val_accuracy: 0.8057 - val_loss: 0.6668\n",
      "Epoch 2/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m668s\u001b[0m 239ms/step - accuracy: 0.9437 - loss: 0.1704 - val_accuracy: 0.9014 - val_loss: 0.4072\n",
      "Epoch 3/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m654s\u001b[0m 234ms/step - accuracy: 0.9686 - loss: 0.1180 - val_accuracy: 0.8829 - val_loss: 0.8537\n",
      "Epoch 4/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 231ms/step - accuracy: 0.9745 - loss: 0.1363 - val_accuracy: 0.8614 - val_loss: 1.8013\n",
      "Epoch 5/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m647s\u001b[0m 231ms/step - accuracy: 0.9779 - loss: 0.1014 - val_accuracy: 0.8086 - val_loss: 2.6446\n",
      "Epoch 6/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m647s\u001b[0m 231ms/step - accuracy: 0.9722 - loss: 0.1363 - val_accuracy: 0.9186 - val_loss: 1.1451\n",
      "Epoch 7/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m650s\u001b[0m 232ms/step - accuracy: 0.9916 - loss: 0.0414 - val_accuracy: 0.8200 - val_loss: 2.5410\n",
      "Epoch 8/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m657s\u001b[0m 235ms/step - accuracy: 0.9870 - loss: 0.0716 - val_accuracy: 0.8500 - val_loss: 3.0349\n",
      "Epoch 9/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m657s\u001b[0m 235ms/step - accuracy: 0.9868 - loss: 0.0795 - val_accuracy: 0.7800 - val_loss: 9.4744\n",
      "Epoch 10/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m655s\u001b[0m 234ms/step - accuracy: 0.9918 - loss: 0.0456 - val_accuracy: 0.8114 - val_loss: 1.9988\n",
      "Epoch 11/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m672s\u001b[0m 240ms/step - accuracy: 0.9922 - loss: 0.0442 - val_accuracy: 0.8743 - val_loss: 2.9911\n",
      "Epoch 12/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m674s\u001b[0m 241ms/step - accuracy: 0.9959 - loss: 0.0249 - val_accuracy: 0.7743 - val_loss: 15.5840\n",
      "Epoch 13/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m670s\u001b[0m 239ms/step - accuracy: 0.9893 - loss: 0.0676 - val_accuracy: 0.7900 - val_loss: 9.8413\n",
      "Epoch 14/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m672s\u001b[0m 240ms/step - accuracy: 0.9922 - loss: 0.0443 - val_accuracy: 0.7957 - val_loss: 14.6070\n",
      "Epoch 15/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m669s\u001b[0m 239ms/step - accuracy: 0.9928 - loss: 0.0528 - val_accuracy: 0.7957 - val_loss: 6.0306\n",
      "Epoch 16/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 241ms/step - accuracy: 0.9875 - loss: 0.0585 - val_accuracy: 0.7500 - val_loss: 12.9257\n",
      "Epoch 17/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m671s\u001b[0m 240ms/step - accuracy: 0.9980 - loss: 0.0260 - val_accuracy: 0.7971 - val_loss: 10.7822\n",
      "Epoch 18/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m671s\u001b[0m 240ms/step - accuracy: 0.9954 - loss: 0.0305 - val_accuracy: 0.7986 - val_loss: 19.3103\n",
      "Epoch 19/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m671s\u001b[0m 240ms/step - accuracy: 0.9941 - loss: 0.0367 - val_accuracy: 0.9271 - val_loss: 1.3915\n",
      "Epoch 20/20\n",
      "\u001b[1m2796/2796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 241ms/step - accuracy: 0.9952 - loss: 0.0308 - val_accuracy: 0.9757 - val_loss: 0.2289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Train Accuracy: 0.9960657954216003, Validation Accuracy: 0.9757142663002014\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 254ms/step\n",
      "Epoch 1/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m677s\u001b[0m 236ms/step - accuracy: 0.6759 - loss: 1.0553 - val_accuracy: 0.9657 - val_loss: 0.1934\n",
      "Epoch 2/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m662s\u001b[0m 237ms/step - accuracy: 0.9483 - loss: 0.1821 - val_accuracy: 0.9442 - val_loss: 0.2390\n",
      "Epoch 3/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m661s\u001b[0m 236ms/step - accuracy: 0.9737 - loss: 0.0828 - val_accuracy: 0.9142 - val_loss: 0.3589\n",
      "Epoch 4/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m662s\u001b[0m 237ms/step - accuracy: 0.9792 - loss: 0.0826 - val_accuracy: 0.8612 - val_loss: 0.6351\n",
      "Epoch 5/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m665s\u001b[0m 238ms/step - accuracy: 0.9777 - loss: 0.0957 - val_accuracy: 0.8755 - val_loss: 0.8562\n",
      "Epoch 6/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m662s\u001b[0m 237ms/step - accuracy: 0.9923 - loss: 0.0303 - val_accuracy: 0.9499 - val_loss: 0.6573\n",
      "Epoch 7/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m661s\u001b[0m 236ms/step - accuracy: 0.9851 - loss: 0.0662 - val_accuracy: 0.9328 - val_loss: 0.4485\n",
      "Epoch 8/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m662s\u001b[0m 237ms/step - accuracy: 0.9905 - loss: 0.0579 - val_accuracy: 0.8641 - val_loss: 2.3241\n",
      "Epoch 9/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m664s\u001b[0m 238ms/step - accuracy: 0.9953 - loss: 0.0302 - val_accuracy: 0.9742 - val_loss: 0.2871\n",
      "Epoch 10/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m664s\u001b[0m 237ms/step - accuracy: 0.9944 - loss: 0.0503 - val_accuracy: 0.8712 - val_loss: 2.4130\n",
      "Epoch 11/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m648s\u001b[0m 232ms/step - accuracy: 0.9940 - loss: 0.0422 - val_accuracy: 0.9757 - val_loss: 0.3947\n",
      "Epoch 12/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m649s\u001b[0m 232ms/step - accuracy: 0.9986 - loss: 0.0080 - val_accuracy: 0.9642 - val_loss: 0.6835\n",
      "Epoch 13/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m651s\u001b[0m 233ms/step - accuracy: 0.9897 - loss: 0.0593 - val_accuracy: 0.9843 - val_loss: 0.2171\n",
      "Epoch 14/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m651s\u001b[0m 233ms/step - accuracy: 0.9973 - loss: 0.0117 - val_accuracy: 0.9742 - val_loss: 0.3362\n",
      "Epoch 15/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m652s\u001b[0m 233ms/step - accuracy: 0.9899 - loss: 0.1006 - val_accuracy: 0.8970 - val_loss: 2.1045\n",
      "Epoch 16/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m654s\u001b[0m 234ms/step - accuracy: 0.9967 - loss: 0.0117 - val_accuracy: 0.9371 - val_loss: 1.5929\n",
      "Epoch 17/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m654s\u001b[0m 234ms/step - accuracy: 0.9959 - loss: 0.0473 - val_accuracy: 0.8913 - val_loss: 2.9146\n",
      "Epoch 18/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 234ms/step - accuracy: 0.9884 - loss: 0.1418 - val_accuracy: 0.9585 - val_loss: 1.2149\n",
      "Epoch 19/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 234ms/step - accuracy: 0.9965 - loss: 0.0326 - val_accuracy: 0.8512 - val_loss: 4.9008\n",
      "Epoch 20/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m655s\u001b[0m 234ms/step - accuracy: 0.9925 - loss: 0.0871 - val_accuracy: 0.8913 - val_loss: 4.3777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Train Accuracy: 0.994637131690979, Validation Accuracy: 0.8912732601165771\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 254ms/step\n",
      "Epoch 1/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m691s\u001b[0m 241ms/step - accuracy: 0.6808 - loss: 1.0241 - val_accuracy: 0.9671 - val_loss: 0.1156\n",
      "Epoch 2/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 241ms/step - accuracy: 0.9534 - loss: 0.1690 - val_accuracy: 0.8212 - val_loss: 0.6215\n",
      "Epoch 3/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m671s\u001b[0m 240ms/step - accuracy: 0.9755 - loss: 0.0935 - val_accuracy: 0.6309 - val_loss: 3.5666\n",
      "Epoch 4/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 241ms/step - accuracy: 0.9696 - loss: 0.1212 - val_accuracy: 0.9313 - val_loss: 0.6975\n",
      "Epoch 5/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m672s\u001b[0m 240ms/step - accuracy: 0.9754 - loss: 0.1002 - val_accuracy: 0.8813 - val_loss: 1.1605\n",
      "Epoch 6/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 241ms/step - accuracy: 0.9845 - loss: 0.0653 - val_accuracy: 0.8884 - val_loss: 1.2517\n",
      "Epoch 7/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m674s\u001b[0m 241ms/step - accuracy: 0.9956 - loss: 0.0290 - val_accuracy: 0.8627 - val_loss: 1.1916\n",
      "Epoch 8/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 241ms/step - accuracy: 0.9783 - loss: 0.1593 - val_accuracy: 0.7954 - val_loss: 4.9813\n",
      "Epoch 9/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 241ms/step - accuracy: 0.9893 - loss: 0.0512 - val_accuracy: 0.8069 - val_loss: 2.4984\n",
      "Epoch 10/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m676s\u001b[0m 242ms/step - accuracy: 0.9925 - loss: 0.0338 - val_accuracy: 0.7783 - val_loss: 3.2492\n",
      "Epoch 11/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 241ms/step - accuracy: 0.9918 - loss: 0.0583 - val_accuracy: 0.7668 - val_loss: 4.7344\n",
      "Epoch 12/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m676s\u001b[0m 242ms/step - accuracy: 0.9867 - loss: 0.0787 - val_accuracy: 0.8283 - val_loss: 3.0731\n",
      "Epoch 13/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m677s\u001b[0m 242ms/step - accuracy: 0.9951 - loss: 0.0425 - val_accuracy: 0.7711 - val_loss: 4.5000\n",
      "Epoch 14/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m675s\u001b[0m 241ms/step - accuracy: 0.9940 - loss: 0.0526 - val_accuracy: 0.7253 - val_loss: 7.3061\n",
      "Epoch 15/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m676s\u001b[0m 242ms/step - accuracy: 0.9968 - loss: 0.0302 - val_accuracy: 0.9213 - val_loss: 0.7787\n",
      "Epoch 16/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m675s\u001b[0m 241ms/step - accuracy: 0.9947 - loss: 0.0272 - val_accuracy: 0.8083 - val_loss: 1.7144\n",
      "Epoch 17/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m679s\u001b[0m 243ms/step - accuracy: 0.9930 - loss: 0.0601 - val_accuracy: 0.8197 - val_loss: 2.9931\n",
      "Epoch 18/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m680s\u001b[0m 243ms/step - accuracy: 0.9974 - loss: 0.0198 - val_accuracy: 0.9628 - val_loss: 0.4402\n",
      "Epoch 19/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m679s\u001b[0m 243ms/step - accuracy: 0.9915 - loss: 0.0793 - val_accuracy: 0.8083 - val_loss: 4.2873\n",
      "Epoch 20/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m680s\u001b[0m 243ms/step - accuracy: 0.9966 - loss: 0.0374 - val_accuracy: 0.8069 - val_loss: 4.7095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Train Accuracy: 0.994637131690979, Validation Accuracy: 0.8068669438362122\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 259ms/step\n",
      "Epoch 1/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m696s\u001b[0m 243ms/step - accuracy: 0.6969 - loss: 1.0053 - val_accuracy: 0.7668 - val_loss: 0.9088\n",
      "Epoch 2/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m681s\u001b[0m 243ms/step - accuracy: 0.9551 - loss: 0.1703 - val_accuracy: 0.8441 - val_loss: 1.0730\n",
      "Epoch 3/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m679s\u001b[0m 243ms/step - accuracy: 0.9669 - loss: 0.1435 - val_accuracy: 0.9657 - val_loss: 0.2215\n",
      "Epoch 4/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m684s\u001b[0m 245ms/step - accuracy: 0.9795 - loss: 0.0921 - val_accuracy: 0.9814 - val_loss: 0.0855\n",
      "Epoch 5/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m679s\u001b[0m 243ms/step - accuracy: 0.9763 - loss: 0.0814 - val_accuracy: 0.9142 - val_loss: 0.5089\n",
      "Epoch 6/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m679s\u001b[0m 243ms/step - accuracy: 0.9907 - loss: 0.0432 - val_accuracy: 0.9714 - val_loss: 0.2525\n",
      "Epoch 7/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m679s\u001b[0m 243ms/step - accuracy: 0.9881 - loss: 0.0529 - val_accuracy: 0.9299 - val_loss: 0.6527\n",
      "Epoch 8/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m685s\u001b[0m 245ms/step - accuracy: 0.9942 - loss: 0.0406 - val_accuracy: 0.8655 - val_loss: 0.7858\n",
      "Epoch 9/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 244ms/step - accuracy: 0.9878 - loss: 0.0449 - val_accuracy: 0.9313 - val_loss: 0.5580\n",
      "Epoch 10/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m681s\u001b[0m 243ms/step - accuracy: 0.9944 - loss: 0.0338 - val_accuracy: 0.9700 - val_loss: 0.2104\n",
      "Epoch 11/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 244ms/step - accuracy: 0.9918 - loss: 0.0580 - val_accuracy: 0.9671 - val_loss: 0.5284\n",
      "Epoch 12/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 244ms/step - accuracy: 0.9951 - loss: 0.0240 - val_accuracy: 0.8526 - val_loss: 1.3811\n",
      "Epoch 13/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 244ms/step - accuracy: 0.9961 - loss: 0.0221 - val_accuracy: 0.8112 - val_loss: 3.3707\n",
      "Epoch 14/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 245ms/step - accuracy: 0.9930 - loss: 0.0376 - val_accuracy: 0.9528 - val_loss: 0.8855\n",
      "Epoch 15/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m681s\u001b[0m 243ms/step - accuracy: 0.9961 - loss: 0.0452 - val_accuracy: 0.9313 - val_loss: 0.9779\n",
      "Epoch 16/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 245ms/step - accuracy: 0.9923 - loss: 0.0600 - val_accuracy: 0.9614 - val_loss: 0.5999\n",
      "Epoch 17/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m688s\u001b[0m 246ms/step - accuracy: 0.9939 - loss: 0.0316 - val_accuracy: 0.9342 - val_loss: 1.3670\n",
      "Epoch 18/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 245ms/step - accuracy: 0.9945 - loss: 0.0797 - val_accuracy: 0.9199 - val_loss: 1.3676\n",
      "Epoch 19/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m690s\u001b[0m 247ms/step - accuracy: 0.9875 - loss: 0.1316 - val_accuracy: 0.9685 - val_loss: 0.6921\n",
      "Epoch 20/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m688s\u001b[0m 246ms/step - accuracy: 0.9899 - loss: 0.0682 - val_accuracy: 0.9728 - val_loss: 0.5480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - Train Accuracy: 0.9917768836021423, Validation Accuracy: 0.9728183150291443\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 261ms/step\n",
      "Epoch 1/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m701s\u001b[0m 244ms/step - accuracy: 0.7048 - loss: 1.0242 - val_accuracy: 0.8155 - val_loss: 0.9589\n",
      "Epoch 2/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m685s\u001b[0m 245ms/step - accuracy: 0.9564 - loss: 0.1618 - val_accuracy: 0.8956 - val_loss: 0.7067\n",
      "Epoch 3/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m685s\u001b[0m 245ms/step - accuracy: 0.9741 - loss: 0.0837 - val_accuracy: 0.8784 - val_loss: 0.9600\n",
      "Epoch 4/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m685s\u001b[0m 245ms/step - accuracy: 0.9843 - loss: 0.0477 - val_accuracy: 0.7067 - val_loss: 4.5299\n",
      "Epoch 5/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 246ms/step - accuracy: 0.9770 - loss: 0.0817 - val_accuracy: 0.7439 - val_loss: 6.4428\n",
      "Epoch 6/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 245ms/step - accuracy: 0.9844 - loss: 0.0788 - val_accuracy: 0.7368 - val_loss: 11.4279\n",
      "Epoch 7/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 245ms/step - accuracy: 0.9908 - loss: 0.0499 - val_accuracy: 0.7496 - val_loss: 7.6349\n",
      "Epoch 8/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 244ms/step - accuracy: 0.9880 - loss: 0.0899 - val_accuracy: 0.6824 - val_loss: 20.6136\n",
      "Epoch 9/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 244ms/step - accuracy: 0.9936 - loss: 0.0270 - val_accuracy: 0.6438 - val_loss: 18.2245\n",
      "Epoch 10/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 244ms/step - accuracy: 0.9914 - loss: 0.0502 - val_accuracy: 0.8011 - val_loss: 5.1275\n",
      "Epoch 11/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m684s\u001b[0m 244ms/step - accuracy: 0.9921 - loss: 0.0412 - val_accuracy: 0.7725 - val_loss: 12.9468\n",
      "Epoch 12/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m689s\u001b[0m 246ms/step - accuracy: 0.9928 - loss: 0.0298 - val_accuracy: 0.8441 - val_loss: 4.4672\n",
      "Epoch 13/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m690s\u001b[0m 247ms/step - accuracy: 0.9918 - loss: 0.0451 - val_accuracy: 0.7983 - val_loss: 6.9695\n",
      "Epoch 14/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m690s\u001b[0m 247ms/step - accuracy: 0.9927 - loss: 0.0813 - val_accuracy: 0.8584 - val_loss: 5.2117\n",
      "Epoch 15/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m690s\u001b[0m 247ms/step - accuracy: 0.9896 - loss: 0.0702 - val_accuracy: 0.6609 - val_loss: 22.2007\n",
      "Epoch 16/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m689s\u001b[0m 246ms/step - accuracy: 0.9919 - loss: 0.0622 - val_accuracy: 0.7139 - val_loss: 18.2943\n",
      "Epoch 17/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m689s\u001b[0m 246ms/step - accuracy: 0.9983 - loss: 0.0115 - val_accuracy: 0.7697 - val_loss: 10.7426\n",
      "Epoch 18/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m691s\u001b[0m 247ms/step - accuracy: 0.9948 - loss: 0.0667 - val_accuracy: 0.9185 - val_loss: 3.4540\n",
      "Epoch 19/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m691s\u001b[0m 247ms/step - accuracy: 0.9981 - loss: 0.0181 - val_accuracy: 0.9585 - val_loss: 1.3917\n",
      "Epoch 20/20\n",
      "\u001b[1m2797/2797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m689s\u001b[0m 246ms/step - accuracy: 0.9923 - loss: 0.0718 - val_accuracy: 0.9428 - val_loss: 2.1666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - Train Accuracy: 0.9928494691848755, Validation Accuracy: 0.9427753686904907\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 263ms/step\n",
      "Average Train Accuracy: 0.9939932823181152\n",
      "Average Validation Accuracy: 0.9178896307945251\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "\n",
    "# Mish activation function\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "# 이미지 로드 및 전처리 함수\n",
    "def load_and_preprocess_image(image_path, target_size=(128, 128)):\n",
    "    image = load_img(image_path, color_mode='grayscale', target_size=target_size)\n",
    "    image_array = img_to_array(image)\n",
    "    image_array /= 255.0\n",
    "    return image_array\n",
    "\n",
    "# ResNet 블록 (Residual Block)\n",
    "def resnet_block(x, filters, downsample=False):\n",
    "    shortcut = x\n",
    "    strides = (2, 2) if downsample else (1, 1)\n",
    "\n",
    "    # 첫 번째 Conv 레이어\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), strides=strides, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "\n",
    "    # 두 번째 Conv 레이어\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # shortcut 경로를 맞추기 위한 Conv 레이어\n",
    "    if downsample or shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n",
    "    \n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation(mish)(x)\n",
    "    return x\n",
    "\n",
    "# ResNet-18 모델 생성\n",
    "def create_resnet18_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # 초기 Conv 레이어\n",
    "    x = layers.Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same')(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    # Residual 블록들\n",
    "    filter_sizes = [64, 128, 256, 512]\n",
    "    num_blocks = [2, 2, 2, 2]  # ResNet-18은 각 필터 크기에서 2개의 블록이 존재\n",
    "    for filters, blocks in zip(filter_sizes, num_blocks):\n",
    "        for i in range(blocks):\n",
    "            x = resnet_block(x, filters, downsample=(i == 0 and filters != 64))  # 첫 블록만 다운샘플링\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)  # 최종적으로 Global Average Pooling\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# 파일 경로 설정 및 데이터 로드\n",
    "folder_path = r\"C:\\Users\\IMS\\Desktop\\Hwangsihoon\\WebCam\\Pupil\\crop_img\"\n",
    "results = pd.read_excel(\"results.xlsx\", header=None)\n",
    "files = glob.glob(os.path.join(folder_path, '*.jpg'))\n",
    "\n",
    "right_images = [file for file in files if 'right' in os.path.basename(file).lower()]\n",
    "left_images = [file for file in files if 'left' in os.path.basename(file).lower()]\n",
    "\n",
    "right_images = np.array([load_and_preprocess_image(img_path) for img_path in right_images])\n",
    "left_images = np.array([load_and_preprocess_image(img_path) for img_path in left_images])\n",
    "labels = results[0].values\n",
    "num_classes = len(np.unique(labels))\n",
    "labels = to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "# 교차 검증 설정\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_train_accuracies = []\n",
    "fold_val_accuracies = []\n",
    "\n",
    "# 모델 저장 폴더 생성\n",
    "model_save_folder = r\"C:\\Users\\IMS\\Desktop\\Hwangsihoon\\WebCam\\0930_resnet18_models\"\n",
    "if not os.path.exists(model_save_folder):\n",
    "    os.makedirs(model_save_folder)\n",
    "\n",
    "# 예측 결과 저장 폴더 생성\n",
    "csv_save_folder = r\"C:\\Users\\IMS\\Desktop\\Hwangsihoon\\WebCam\\0930_resnet18_predictions\"\n",
    "if not os.path.exists(csv_save_folder):\n",
    "    os.makedirs(csv_save_folder)\n",
    "\n",
    "# 각 폴드별로 학습\n",
    "for fold_idx, (train_index, val_index) in enumerate(kf.split(right_images)):\n",
    "    right_images_train, right_images_val = right_images[train_index], right_images[val_index]\n",
    "    left_images_train, left_images_val = left_images[train_index], left_images[val_index]\n",
    "    labels_train, labels_val = labels[train_index], labels[val_index]\n",
    "\n",
    "    # 오른쪽과 왼쪽 눈에 각각 ResNet-18 모델 적용\n",
    "    right_eye_model = create_resnet18_model((128, 128, 1))\n",
    "    left_eye_model = create_resnet18_model((128, 128, 1))\n",
    "\n",
    "    # 두 눈에서 추출한 특성 결합\n",
    "    combined_input = layers.concatenate([right_eye_model.output, left_eye_model.output])\n",
    "\n",
    "    # 완전 연결 레이어\n",
    "    x = layers.Dense(256, activation=mish)(combined_input)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output_layer = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # 최종 모델 정의\n",
    "    combined_model = Model(inputs=[right_eye_model.input, left_eye_model.input], outputs=output_layer)\n",
    "\n",
    "    # 모델 컴파일\n",
    "    combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # 모델 학습\n",
    "    history = combined_model.fit([right_images_train, left_images_train], labels_train, \n",
    "                                 epochs=20, batch_size=1, validation_data=([right_images_val, left_images_val], labels_val))\n",
    "\n",
    "    # 폴드별 정확도 기록\n",
    "    train_accuracy = history.history['accuracy'][-1]\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    fold_train_accuracies.append(train_accuracy)\n",
    "    fold_val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Fold {fold_idx+1} - Train Accuracy: {train_accuracy}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "    # 모델 저장\n",
    "    combined_model.save(os.path.join(model_save_folder, f\"fold_{fold_idx+1}_combined_model.h5\"))\n",
    "\n",
    "    # 예측 및 결과 저장\n",
    "    predictions = combined_model.predict([right_images_val, left_images_val])\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    actual_classes = np.argmax(labels_val, axis=1)\n",
    "\n",
    "    data = {\n",
    "        'Right Image': [os.path.basename(files[idx]) for idx in val_index],\n",
    "        'Left Image': [os.path.basename(files[idx]) for idx in val_index],\n",
    "        'Actual Class': actual_classes,\n",
    "        'Predicted Class': predicted_classes\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(os.path.join(csv_save_folder, f\"fold_{fold_idx+1}_predictions.csv\"), index=False)\n",
    "\n",
    "# 전체 평균 정확도 계산\n",
    "avg_train_accuracy = np.mean(fold_train_accuracies)\n",
    "avg_val_accuracy = np.mean(fold_val_accuracies)\n",
    "\n",
    "print(f'Average Train Accuracy: {avg_train_accuracy}')\n",
    "print(f'Average Validation Accuracy: {avg_val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea32f2d-dd1e-4631-a19a-c3960fe115a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
