{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78f645e-aa74-4df7-9a9c-4d0df475bd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on subject: hsh\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\sihoon\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor', 'keras_tensor_170', 'keras_tensor_340']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m803s\u001b[0m 330ms/step - accuracy: 0.0456 - loss: 4.5429\n",
      "Epoch 2/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 331ms/step - accuracy: 0.0579 - loss: 3.1504\n",
      "Epoch 3/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m758s\u001b[0m 330ms/step - accuracy: 0.0436 - loss: 3.1372\n",
      "Epoch 4/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m758s\u001b[0m 330ms/step - accuracy: 0.0450 - loss: 3.1367\n",
      "Epoch 5/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m768s\u001b[0m 334ms/step - accuracy: 0.0424 - loss: 3.1368\n",
      "Epoch 6/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m758s\u001b[0m 330ms/step - accuracy: 0.0427 - loss: 3.1369\n",
      "Epoch 7/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m759s\u001b[0m 330ms/step - accuracy: 0.0408 - loss: 3.1368\n",
      "Epoch 8/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m759s\u001b[0m 330ms/step - accuracy: 0.0443 - loss: 3.1367\n",
      "Epoch 9/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m759s\u001b[0m 330ms/step - accuracy: 0.0336 - loss: 3.1367\n",
      "Epoch 10/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 331ms/step - accuracy: 0.0367 - loss: 3.1366\n",
      "Epoch 11/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 331ms/step - accuracy: 0.0420 - loss: 3.1369\n",
      "Epoch 12/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m761s\u001b[0m 331ms/step - accuracy: 0.0392 - loss: 3.1366\n",
      "Epoch 13/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 331ms/step - accuracy: 0.0345 - loss: 3.1370\n",
      "Epoch 14/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 330ms/step - accuracy: 0.0376 - loss: 3.1367\n",
      "Epoch 15/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 330ms/step - accuracy: 0.0458 - loss: 3.1365\n",
      "Epoch 16/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 330ms/step - accuracy: 0.0365 - loss: 3.1368\n",
      "Epoch 17/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 330ms/step - accuracy: 0.0334 - loss: 3.1370\n",
      "Epoch 18/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m761s\u001b[0m 331ms/step - accuracy: 0.0351 - loss: 3.1367\n",
      "Epoch 19/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m761s\u001b[0m 331ms/step - accuracy: 0.0410 - loss: 3.1367\n",
      "Epoch 20/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m762s\u001b[0m 331ms/step - accuracy: 0.0360 - loss: 3.1367\n",
      "Epoch 21/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m761s\u001b[0m 331ms/step - accuracy: 0.0378 - loss: 3.1371\n",
      "Epoch 22/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m762s\u001b[0m 331ms/step - accuracy: 0.0342 - loss: 3.1369\n",
      "Epoch 23/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 332ms/step - accuracy: 0.0382 - loss: 3.1367\n",
      "Epoch 24/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m762s\u001b[0m 331ms/step - accuracy: 0.0358 - loss: 3.1369\n",
      "Epoch 25/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 332ms/step - accuracy: 0.0382 - loss: 3.1364\n",
      "Epoch 26/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 332ms/step - accuracy: 0.0392 - loss: 3.1365\n",
      "Epoch 27/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 332ms/step - accuracy: 0.0479 - loss: 3.1364\n",
      "Epoch 28/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m764s\u001b[0m 332ms/step - accuracy: 0.0349 - loss: 3.1368\n",
      "Epoch 29/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 332ms/step - accuracy: 0.0443 - loss: 3.1366\n",
      "Epoch 30/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m764s\u001b[0m 332ms/step - accuracy: 0.0371 - loss: 3.1366\n",
      "Epoch 31/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m765s\u001b[0m 332ms/step - accuracy: 0.0445 - loss: 3.1365\n",
      "Epoch 32/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m764s\u001b[0m 332ms/step - accuracy: 0.0435 - loss: 3.1368\n",
      "Epoch 33/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m764s\u001b[0m 332ms/step - accuracy: 0.0324 - loss: 3.1370\n",
      "Epoch 34/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m767s\u001b[0m 333ms/step - accuracy: 0.0434 - loss: 3.1361\n",
      "Epoch 35/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m764s\u001b[0m 332ms/step - accuracy: 0.0348 - loss: 3.1364\n",
      "Epoch 36/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m764s\u001b[0m 332ms/step - accuracy: 0.0404 - loss: 3.1369\n",
      "Epoch 37/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m765s\u001b[0m 333ms/step - accuracy: 0.0373 - loss: 3.1365\n",
      "Epoch 38/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m765s\u001b[0m 332ms/step - accuracy: 0.0402 - loss: 3.1367\n",
      "Epoch 39/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m765s\u001b[0m 333ms/step - accuracy: 0.0415 - loss: 3.1365\n",
      "Epoch 40/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m765s\u001b[0m 333ms/step - accuracy: 0.0327 - loss: 3.1366\n",
      "Epoch 41/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m765s\u001b[0m 333ms/step - accuracy: 0.0355 - loss: 3.1368\n",
      "Epoch 42/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m766s\u001b[0m 333ms/step - accuracy: 0.0315 - loss: 3.1369\n",
      "Epoch 43/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m765s\u001b[0m 333ms/step - accuracy: 0.0414 - loss: 3.1365\n",
      "Epoch 44/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m765s\u001b[0m 333ms/step - accuracy: 0.0378 - loss: 3.1366\n",
      "Epoch 45/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m767s\u001b[0m 333ms/step - accuracy: 0.0349 - loss: 3.1368\n",
      "Epoch 46/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m766s\u001b[0m 333ms/step - accuracy: 0.0445 - loss: 3.1367\n",
      "Epoch 47/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m808s\u001b[0m 351ms/step - accuracy: 0.0386 - loss: 3.1367\n",
      "Epoch 48/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m784s\u001b[0m 341ms/step - accuracy: 0.0294 - loss: 3.1366\n",
      "Epoch 49/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m791s\u001b[0m 344ms/step - accuracy: 0.0459 - loss: 3.1363\n",
      "Epoch 50/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m803s\u001b[0m 349ms/step - accuracy: 0.0424 - loss: 3.1370\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 642ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 528ms/step - accuracy: 0.0449 - loss: 3.1354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy for subject hsh: 0.0435\n",
      "Results for subject hsh saved.\n",
      "Model saved at: C:\\Users\\admin\\Desktop\\sihoon\\webcam\\results\\1125_model_hsh.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "\n",
    "# Mish activation function\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "def load_and_preprocess_image(image_path, target_size=(128, 128)):\n",
    "    image = load_img(image_path, color_mode='grayscale', target_size=target_size)\n",
    "    image_array = img_to_array(image)\n",
    "    image_array /= 255.0\n",
    "    return image_array\n",
    "\n",
    "# 파일 이름에서 session과 point 추출\n",
    "def extract_session_and_point(filename):\n",
    "    session_match = re.search(r'img_(\\d+)', filename)\n",
    "    point_match = re.search(r'\\((\\d+)\\)', filename)\n",
    "    session = int(session_match.group(1)) if session_match else None\n",
    "    point = int(point_match.group(1)) if point_match else None\n",
    "    return session, point\n",
    "\n",
    "# ResNet-50 모델 생성\n",
    "def create_resnet50_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = layers.Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same')(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    filter_sizes = [64, 128, 256, 512]\n",
    "    num_blocks = [3, 4, 6, 3]\n",
    "    for filters, blocks in zip(filter_sizes, num_blocks):\n",
    "        for i in range(blocks):\n",
    "            x = bottleneck_block(x, filters, downsample=(i == 0 and filters != 64))\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# 병목 구조\n",
    "def bottleneck_block(x, filters, downsample=False):\n",
    "    shortcut = x\n",
    "    strides = (2, 2) if downsample else (1, 1)\n",
    "    x = layers.Conv2D(filters, kernel_size=(1, 1), strides=strides, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "\n",
    "    x = layers.Conv2D(filters * 4, kernel_size=(1, 1), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if downsample or shortcut.shape[-1] != filters * 4:\n",
    "        shortcut = layers.Conv2D(filters * 4, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation(mish)(x)\n",
    "    return x\n",
    "\n",
    "# MLP 모델 생성\n",
    "def create_mlp_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = layers.Dense(128, activation=mish)(input_layer)\n",
    "    x = layers.Dense(64, activation=mish)(x)\n",
    "    x = layers.Dense(3, activation=mish)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# 유효한 라벨 정의\n",
    "valid_labels = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45]\n",
    "\n",
    "# 데이터 경로\n",
    "folder_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\img\"\n",
    "csv_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\results\"\n",
    "\n",
    "# 테스트할 데이터는 hsh 폴더\n",
    "test_subject = 'hsh'\n",
    "print(f\"Testing on subject: {test_subject}\")\n",
    "\n",
    "# 테스트 데이터 준비\n",
    "test_folder = os.path.join(folder_path, test_subject)\n",
    "test_csv = os.path.join(csv_path, f\"{test_subject}.csv\")\n",
    "test_images = glob.glob(os.path.join(test_folder, '*.jpg'))\n",
    "\n",
    "test_image_data = []\n",
    "for img in test_images:\n",
    "    session, point = extract_session_and_point(os.path.basename(img))\n",
    "    if point in valid_labels:\n",
    "        subject_name = os.path.basename(os.path.dirname(img))\n",
    "        unique_filename = f\"{subject_name}_{os.path.basename(img)}\"\n",
    "        test_image_data.append({\n",
    "            'Filename': os.path.abspath(img),\n",
    "            'UniqueFilename': unique_filename,\n",
    "            'Session': session,\n",
    "            'Point': point\n",
    "        })\n",
    "test_df = pd.DataFrame(test_image_data)\n",
    "\n",
    "if os.path.exists(test_csv):\n",
    "    test_csv_data = pd.read_csv(test_csv)\n",
    "    test_csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "    test_merged = pd.merge(test_df, test_csv_data, on=['Session', 'Point'])\n",
    "else:\n",
    "    print(f\"No CSV data for {test_subject}. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "test_merged = test_merged.drop_duplicates(subset=['UniqueFilename', 'Session', 'Point'])\n",
    "test_merged = test_merged[test_merged['Point'].isin(valid_labels)]\n",
    "test_left_features = test_merged[test_merged['Filename'].str.contains('left', case=False)]\n",
    "test_images_array = np.array([load_and_preprocess_image(path) for path in test_left_features['Filename']])\n",
    "test_features = test_left_features.drop(['Filename', 'UniqueFilename', 'Session', 'Point'], axis=1).values\n",
    "test_labels = test_left_features['Point'].map({label: idx for idx, label in enumerate(valid_labels)}).values\n",
    "test_labels = to_categorical(test_labels, num_classes=len(valid_labels))\n",
    "\n",
    "# 학습 데이터 준비\n",
    "subject_folders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "train_subjects = [s for s in subject_folders if s != test_subject]\n",
    "train_images = []\n",
    "train_csv_data = pd.DataFrame()\n",
    "for train_subject in train_subjects:\n",
    "    train_folder = os.path.join(folder_path, train_subject)\n",
    "    train_csv = os.path.join(csv_path, f\"{train_subject}.csv\")\n",
    "    train_images.extend(glob.glob(os.path.join(train_folder, '*.jpg')))\n",
    "    if os.path.exists(train_csv):\n",
    "        train_csv_data = pd.concat([train_csv_data, pd.read_csv(train_csv)])\n",
    "\n",
    "train_csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "train_image_data = []\n",
    "for img in train_images:\n",
    "    session, point = extract_session_and_point(os.path.basename(img))\n",
    "    if point in valid_labels:\n",
    "        subject_name = os.path.basename(os.path.dirname(img))\n",
    "        unique_filename = f\"{subject_name}_{os.path.basename(img)}\"\n",
    "        train_image_data.append({\n",
    "            'Filename': os.path.abspath(img),\n",
    "            'UniqueFilename': unique_filename,\n",
    "            'Session': session,\n",
    "            'Point': point\n",
    "        })\n",
    "train_df = pd.DataFrame(train_image_data)\n",
    "\n",
    "train_merged = pd.merge(train_df, train_csv_data, on=['Session', 'Point'])\n",
    "train_merged = train_merged.drop_duplicates(subset=['UniqueFilename', 'Session', 'Point'])\n",
    "train_left_features = train_merged[train_merged['Filename'].str.contains('left', case=False)]\n",
    "train_images_array = np.array([load_and_preprocess_image(path) for path in train_left_features['Filename']])\n",
    "train_features = train_left_features.drop(['Filename', 'Session', 'Point', 'UniqueFilename'], axis=1).values\n",
    "train_labels = train_left_features['Point'].map({label: idx for idx, label in enumerate(valid_labels)}).values\n",
    "train_labels = to_categorical(train_labels, num_classes=len(valid_labels))\n",
    "\n",
    "# 모델 생성\n",
    "right_eye_model = create_resnet50_model((128, 128, 1))\n",
    "left_eye_model = create_resnet50_model((128, 128, 1))\n",
    "mlp_model = create_mlp_model(train_features.shape[1:])\n",
    "combined_input = layers.concatenate([right_eye_model.output, left_eye_model.output, mlp_model.output])\n",
    "x = layers.Dense(256, activation=mish)(combined_input)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output_layer = layers.Dense(len(valid_labels), activation='softmax')(x)\n",
    "combined_model = Model(inputs=[right_eye_model.input, left_eye_model.input, mlp_model.input], outputs=output_layer)\n",
    "\n",
    "combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "combined_model.fit([train_images_array, train_images_array, train_features], train_labels, \n",
    "                   epochs=50, batch_size=1, verbose=1)\n",
    "\n",
    "# 테스트 데이터 평가\n",
    "predictions = combined_model.predict([test_images_array, test_images_array, test_features])\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "actual_classes = np.argmax(test_labels, axis=1)\n",
    "\n",
    "loss, accuracy = combined_model.evaluate([test_images_array, test_images_array, test_features], test_labels)\n",
    "print(f\"Testing accuracy for subject {test_subject}: {accuracy:.4f}\")\n",
    "\n",
    "# 결과 저장\n",
    "results = []\n",
    "for idx, (image_path, feature, pred_class, actual_class) in enumerate(zip(\n",
    "    test_left_features['Filename'], test_features, predicted_classes, actual_classes)):\n",
    "    results.append({\n",
    "        'Subject': test_subject,\n",
    "        'Test Accuracy': accuracy,\n",
    "        'Test Loss': loss,\n",
    "        'Image File': image_path,\n",
    "        'Feature': feature.tolist(),\n",
    "        'Predicted Class': pred_class,\n",
    "        'Actual Class': actual_class\n",
    "    })\n",
    "\n",
    "# 결과 CSV 저장\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(csv_path, f\"1125_hsh_result.csv\"), index=False, encoding='utf-8')\n",
    "print(f\"Results for subject {test_subject} saved.\")\n",
    "\n",
    "# 모델 저장\n",
    "model_save_path = os.path.join(csv_path, f\"1125_model_hsh.h5\")\n",
    "combined_model.save(model_save_path)\n",
    "print(f\"Model saved at: {model_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
