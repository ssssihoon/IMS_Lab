{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d49e3c8f-0bbf-45d2-b346-88e07309b7be",
   "metadata": {},
   "source": [
    "# hsh폴더에 대해서만 train, test \n",
    " 일반화를 하지 않고, 개인화에 초점을 맞춤 -> 한 서브젝트에 대해서만 분류 진행\n",
    "\n",
    " 이후 대조해 볼 것으로 모든 서브젝트에 대해서 분류 진행 -> 전의 경우는 서브젝트에 대해서 loso로 서브젝트 분리 후 train, test를 진행했었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa97a4f-743b-4bfa-895a-aea498e17c7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing from folder: hsh\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\sihoon\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_139', 'keras_tensor_204', 'keras_tensor_269']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 214ms/step - accuracy: 0.1441 - loss: 4.2612 - val_accuracy: 0.1556 - val_loss: 2.1907\n",
      "Epoch 2/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 211ms/step - accuracy: 0.4059 - loss: 1.4616 - val_accuracy: 0.4000 - val_loss: 1.4715\n",
      "Epoch 3/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 211ms/step - accuracy: 0.5766 - loss: 0.8945 - val_accuracy: 0.6444 - val_loss: 1.6981\n",
      "Epoch 4/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 211ms/step - accuracy: 0.7649 - loss: 0.6382 - val_accuracy: 0.8889 - val_loss: 0.3299\n",
      "Epoch 5/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 212ms/step - accuracy: 0.9324 - loss: 0.2057 - val_accuracy: 0.7556 - val_loss: 0.5023\n",
      "Epoch 6/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 212ms/step - accuracy: 0.9449 - loss: 0.1962 - val_accuracy: 0.7333 - val_loss: 1.2229\n",
      "Epoch 7/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 212ms/step - accuracy: 0.8839 - loss: 0.2702 - val_accuracy: 0.9778 - val_loss: 0.0945\n",
      "Epoch 8/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 222ms/step - accuracy: 0.9954 - loss: 0.0133 - val_accuracy: 0.9667 - val_loss: 0.0890\n",
      "Epoch 9/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 235ms/step - accuracy: 0.9784 - loss: 0.0559 - val_accuracy: 0.9556 - val_loss: 0.1495\n",
      "Epoch 10/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 219ms/step - accuracy: 0.9746 - loss: 0.0902 - val_accuracy: 0.6444 - val_loss: 1.4092\n",
      "Epoch 11/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 219ms/step - accuracy: 0.9513 - loss: 0.1861 - val_accuracy: 0.2556 - val_loss: 12.8727\n",
      "Epoch 12/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 219ms/step - accuracy: 0.9612 - loss: 0.0801 - val_accuracy: 0.8667 - val_loss: 0.4662\n",
      "Epoch 13/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 221ms/step - accuracy: 1.0000 - loss: 0.0182 - val_accuracy: 0.6111 - val_loss: 2.4029\n",
      "Epoch 14/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 0.0029 - val_accuracy: 0.9444 - val_loss: 0.1377\n",
      "Epoch 15/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.9556 - val_loss: 0.1288\n",
      "Epoch 16/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 221ms/step - accuracy: 1.0000 - loss: 4.5580e-04 - val_accuracy: 0.9556 - val_loss: 0.1277\n",
      "Epoch 17/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 222ms/step - accuracy: 1.0000 - loss: 4.2611e-04 - val_accuracy: 0.9667 - val_loss: 0.1354\n",
      "Epoch 18/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 4.9978e-04 - val_accuracy: 0.9667 - val_loss: 0.1230\n",
      "Epoch 19/50\n",
      "\u001b[1m299/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m12s\u001b[0m 210ms/step - accuracy: 1.0000 - loss: 6.1482e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 217\u001b[0m\n\u001b[0;32m    214\u001b[0m subject \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhsh\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# 동일 폴더에서 train/test 분리 및 실행\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m train_and_evaluate_from_single_folder(folder_path, csv_path, valid_labels, subject)\n",
      "Cell \u001b[1;32mIn[3], line 172\u001b[0m, in \u001b[0;36mtrain_and_evaluate_from_single_folder\u001b[1;34m(folder_path, csv_path, valid_labels, subject, test_size)\u001b[0m\n\u001b[0;32m    169\u001b[0m combined_model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39m[right_eye_model\u001b[38;5;241m.\u001b[39minput, left_eye_model\u001b[38;5;241m.\u001b[39minput, mlp_model\u001b[38;5;241m.\u001b[39minput], outputs\u001b[38;5;241m=\u001b[39moutput_layer)\n\u001b[0;32m    171\u001b[0m combined_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 172\u001b[0m history \u001b[38;5;241m=\u001b[39m combined_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    173\u001b[0m     [train_images_array, train_images_array, train_features], train_labels,\n\u001b[0;32m    174\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m([test_images_array, test_images_array, test_features], test_labels),\n\u001b[0;32m    175\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m    176\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    177\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    178\u001b[0m )\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# 평가\u001b[39;00m\n\u001b[0;32m    181\u001b[0m predictions \u001b[38;5;241m=\u001b[39m combined_model\u001b[38;5;241m.\u001b[39mpredict([test_images_array, test_images_array, test_features])\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1684\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1685\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1686\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1687\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1688\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1689\u001b[0m   )\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Mish activation function\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "def load_and_preprocess_image(image_path, target_size=(128, 128)):\n",
    "    image = load_img(image_path, color_mode='rgb', target_size=target_size)\n",
    "    image_array = img_to_array(image)\n",
    "    image_array /= 255.0\n",
    "    return image_array\n",
    "\n",
    "# 파일 이름에서 session과 point 추출\n",
    "def extract_session_and_point(filename):\n",
    "    session_match = re.search(r'img_(\\d+)', filename)\n",
    "    point_match = re.search(r'\\((\\d+)\\)', filename)\n",
    "    session = int(session_match.group(1)) if session_match else None\n",
    "    point = int(point_match.group(1)) if point_match else None\n",
    "    return session, point\n",
    "\n",
    "# ResNet18 모델 생성\n",
    "def create_resnet18_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = layers.Conv2D(64, kernel_size=(5, 7), padding='same')(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 3), padding='same')(x)\n",
    "    filter_sizes = [64, 128, 256, 512]\n",
    "    num_blocks = [2, 2, 2, 2]\n",
    "    for filters, blocks in zip(filter_sizes, num_blocks):\n",
    "        for i in range(blocks):\n",
    "            x = resnet_block(x, filters, downsample=(i == 0 and filters != 64))\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# ResNet 블록 생성\n",
    "def resnet_block(x, filters, downsample=False):\n",
    "    shortcut = x\n",
    "    strides = (2, 2) if downsample else (1, 1)\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), strides=strides, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if downsample or shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation(mish)(x)\n",
    "    return x\n",
    "\n",
    "# MLP 모델 생성\n",
    "def create_mlp_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = layers.Dense(128, activation=mish)(input_layer)\n",
    "    x = layers.Dense(64, activation=mish)(x)\n",
    "    x = layers.Dense(3, activation=mish)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "valid_labels = [1, 5, 9, 19, 23, 27, 37, 41, 45]\n",
    "\n",
    "def prepare_data(folder_path, csv_path, test_subject, valid_labels):\n",
    "    subject_folders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "    train_subjects = [s for s in subject_folders if s != test_subject]\n",
    "    \n",
    "    # 학습 데이터 로드\n",
    "    train_images, train_csv_data = [], pd.DataFrame()\n",
    "    for train_subject in train_subjects:\n",
    "        train_folder = os.path.join(folder_path, train_subject)\n",
    "        train_csv = os.path.join(csv_path, f\"{train_subject}.csv\")\n",
    "        train_images.extend(glob.glob(os.path.join(train_folder, '*.jpg')))\n",
    "        if os.path.exists(train_csv):\n",
    "            train_csv_data = pd.concat([train_csv_data, pd.read_csv(train_csv)])\n",
    "    train_csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "    \n",
    "    # 테스트 데이터 로드\n",
    "    test_folder = os.path.join(folder_path, test_subject)\n",
    "    test_csv = os.path.join(csv_path, f\"{test_subject}.csv\")\n",
    "    test_images = glob.glob(os.path.join(test_folder, '*.jpg'))\n",
    "    test_csv_data = pd.read_csv(test_csv) if os.path.exists(test_csv) else pd.DataFrame(columns=['Session', 'Point'])\n",
    "    test_csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "    \n",
    "    # 데이터 처리 함수\n",
    "    def process_images(image_paths, csv_data, valid_labels):\n",
    "        image_data = []\n",
    "        for img in image_paths:\n",
    "            session, point = extract_session_and_point(os.path.basename(img))\n",
    "            if point in valid_labels:\n",
    "                subject_name = os.path.basename(os.path.dirname(img))\n",
    "                unique_filename = f\"{subject_name}_{os.path.basename(img)}\"\n",
    "                image_data.append({\n",
    "                    'Filename': os.path.abspath(img),\n",
    "                    'UniqueFilename': unique_filename,\n",
    "                    'Session': session,\n",
    "                    'Point': point\n",
    "                })\n",
    "        df = pd.DataFrame(image_data)\n",
    "        merged = pd.merge(df, csv_data, on=['Session', 'Point'], how='inner')\n",
    "        merged = merged.drop_duplicates(subset=['UniqueFilename', 'Session', 'Point'])\n",
    "        merged = merged[merged['Point'].isin(valid_labels)]\n",
    "        return merged\n",
    "    \n",
    "    train_df = process_images(train_images, train_csv_data, valid_labels)\n",
    "    test_df = process_images(test_images, test_csv_data, valid_labels)\n",
    "    return train_df, test_df\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# hsh 폴더에서 train/test 분리 및 학습 진행\n",
    "def train_and_evaluate_from_single_folder(folder_path, csv_path, valid_labels, subject, test_size=0.2):\n",
    "    print(f\"Training and testing from folder: {subject}\")\n",
    "    \n",
    "    # hsh 폴더 데이터 로드\n",
    "    folder = os.path.join(folder_path, subject)\n",
    "    csv_file = os.path.join(csv_path, f\"{subject}.csv\")\n",
    "    image_paths = glob.glob(os.path.join(folder, '*.jpg'))\n",
    "    csv_data = pd.read_csv(csv_file) if os.path.exists(csv_file) else pd.DataFrame(columns=['Session', 'Point'])\n",
    "    csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "    \n",
    "    # 이미지와 라벨 처리\n",
    "    data = []\n",
    "    for img in image_paths:\n",
    "        session, point = extract_session_and_point(os.path.basename(img))\n",
    "        if point in valid_labels:\n",
    "            unique_filename = f\"{subject}_{os.path.basename(img)}\"\n",
    "            data.append({\n",
    "                'Filename': os.path.abspath(img),\n",
    "                'UniqueFilename': unique_filename,\n",
    "                'Session': session,\n",
    "                'Point': point\n",
    "            })\n",
    "    df = pd.DataFrame(data)\n",
    "    merged = pd.merge(df, csv_data, on=['Session', 'Point'], how='inner')\n",
    "    merged = merged.drop_duplicates(subset=['UniqueFilename', 'Session', 'Point'])\n",
    "    merged = merged[merged['Point'].isin(valid_labels)]\n",
    "    \n",
    "    # Train/Test Split\n",
    "    train_df, test_df = train_test_split(merged, test_size=test_size, random_state=42, stratify=merged['Point'])\n",
    "    \n",
    "    train_images_array = np.array([load_and_preprocess_image(path) for path in train_df['Filename']])\n",
    "    train_features = train_df.drop(['Filename', 'UniqueFilename', 'Session', 'Point'], axis=1).values\n",
    "    train_labels = train_df['Point'].map({label: idx for idx, label in enumerate(valid_labels)}).values\n",
    "    train_labels = to_categorical(train_labels, num_classes=len(valid_labels))\n",
    "    \n",
    "    test_images_array = np.array([load_and_preprocess_image(path) for path in test_df['Filename']])\n",
    "    test_features = test_df.drop(['Filename', 'UniqueFilename', 'Session', 'Point'], axis=1).values\n",
    "    test_labels = test_df['Point'].map({label: idx for idx, label in enumerate(valid_labels)}).values\n",
    "    test_labels = to_categorical(test_labels, num_classes=len(valid_labels))\n",
    "    \n",
    "    # 모델 생성\n",
    "    right_eye_model = create_resnet18_model((128, 128, 3))\n",
    "    left_eye_model = create_resnet18_model((128, 128, 3))\n",
    "    mlp_model = create_mlp_model(train_features.shape[1:])\n",
    "    combined_input = layers.concatenate([right_eye_model.output, left_eye_model.output, mlp_model.output])\n",
    "    x = layers.Dense(256, activation=mish)(combined_input)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    output_layer = layers.Dense(len(valid_labels), activation='softmax')(x)\n",
    "    combined_model = Model(inputs=[right_eye_model.input, left_eye_model.input, mlp_model.input], outputs=output_layer)\n",
    "\n",
    "    combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = combined_model.fit(\n",
    "        [train_images_array, train_images_array, train_features], train_labels,\n",
    "        validation_data=([test_images_array, test_images_array, test_features], test_labels),\n",
    "        epochs=50,\n",
    "        batch_size=1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 평가\n",
    "    predictions = combined_model.predict([test_images_array, test_images_array, test_features])\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    actual_classes = np.argmax(test_labels, axis=1)\n",
    "\n",
    "    loss, accuracy = combined_model.evaluate([test_images_array, test_images_array, test_features], test_labels)\n",
    "    print(f\"Testing accuracy for folder {subject}: {accuracy:.4f}\")\n",
    "    \n",
    "    # 결과 저장\n",
    "    results = []\n",
    "    for idx, (image_path, feature, pred_class, actual_class) in enumerate(zip(\n",
    "        test_df['Filename'], test_features, predicted_classes, actual_classes)):\n",
    "        results.append({\n",
    "            'Subject': subject,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Loss': loss,\n",
    "            'Image File': image_path,\n",
    "            'Feature': feature.tolist(),\n",
    "            'Predicted Class': pred_class,\n",
    "            'Actual Class': actual_class\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(os.path.join(csv_path, f\"0102_results_{subject}.csv\"), index=False, encoding='utf-8')\n",
    "    print(f\"Results for folder {subject} saved.\")\n",
    "    \n",
    "    # 모델 저장\n",
    "    model_save_path = os.path.join(csv_path, f\"0102_model_{subject}.h5\")\n",
    "    combined_model.save(model_save_path)\n",
    "    print(f\"Model saved at: {model_save_path}\")\n",
    "\n",
    "# 데이터 경로\n",
    "folder_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\img\"\n",
    "csv_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\results\"\n",
    "subject = 'hsh'\n",
    "\n",
    "# 동일 폴더에서 train/test 분리 및 실행\n",
    "train_and_evaluate_from_single_folder(folder_path, csv_path, valid_labels, subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abe5b575-2f41-4aea-952f-f059195afca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing from folder: hsh\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\sihoon\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_278', 'keras_tensor_343', 'keras_tensor_408']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 216ms/step - accuracy: 0.0699 - loss: 3.7767 - val_accuracy: 0.2615 - val_loss: 4.2111\n",
      "Epoch 2/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 211ms/step - accuracy: 0.3106 - loss: 1.8737 - val_accuracy: 0.4308 - val_loss: 2.3839\n",
      "Epoch 3/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 210ms/step - accuracy: 0.6179 - loss: 0.9717 - val_accuracy: 0.6308 - val_loss: 1.2357\n",
      "Epoch 4/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 210ms/step - accuracy: 0.7932 - loss: 0.6353 - val_accuracy: 0.6308 - val_loss: 1.5662\n",
      "Epoch 5/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 209ms/step - accuracy: 0.9059 - loss: 0.2960 - val_accuracy: 0.7769 - val_loss: 0.7745\n",
      "Epoch 6/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 210ms/step - accuracy: 0.9233 - loss: 0.2633 - val_accuracy: 0.7923 - val_loss: 0.6331\n",
      "Epoch 7/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 210ms/step - accuracy: 0.9480 - loss: 0.1990 - val_accuracy: 0.7769 - val_loss: 0.8418\n",
      "Epoch 8/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 210ms/step - accuracy: 0.9232 - loss: 0.2355 - val_accuracy: 0.8538 - val_loss: 0.4593\n",
      "Epoch 9/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 211ms/step - accuracy: 0.9959 - loss: 0.0266 - val_accuracy: 0.8154 - val_loss: 0.5752\n",
      "Epoch 10/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 210ms/step - accuracy: 0.9450 - loss: 0.1882 - val_accuracy: 0.7769 - val_loss: 0.8325\n",
      "Epoch 11/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 210ms/step - accuracy: 0.9464 - loss: 0.1744 - val_accuracy: 0.8000 - val_loss: 0.7290\n",
      "Epoch 12/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 210ms/step - accuracy: 0.9780 - loss: 0.0806 - val_accuracy: 0.7231 - val_loss: 1.1969\n",
      "Epoch 13/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 210ms/step - accuracy: 0.9891 - loss: 0.0345 - val_accuracy: 0.8923 - val_loss: 0.3753\n",
      "Epoch 14/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 210ms/step - accuracy: 1.0000 - loss: 0.0058 - val_accuracy: 0.9000 - val_loss: 0.4342\n",
      "Epoch 15/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 210ms/step - accuracy: 1.0000 - loss: 7.9473e-04 - val_accuracy: 0.9000 - val_loss: 0.4554\n",
      "Epoch 16/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 210ms/step - accuracy: 1.0000 - loss: 4.0923e-04 - val_accuracy: 0.9000 - val_loss: 0.4192\n",
      "Epoch 17/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 211ms/step - accuracy: 1.0000 - loss: 4.1490e-04 - val_accuracy: 0.9000 - val_loss: 0.4630\n",
      "Epoch 18/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 211ms/step - accuracy: 1.0000 - loss: 4.8227e-04 - val_accuracy: 0.8769 - val_loss: 0.3979\n",
      "Epoch 19/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 211ms/step - accuracy: 1.0000 - loss: 3.7868e-04 - val_accuracy: 0.9000 - val_loss: 0.4694\n",
      "Epoch 20/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 211ms/step - accuracy: 1.0000 - loss: 2.8220e-04 - val_accuracy: 0.9000 - val_loss: 0.4737\n",
      "Epoch 21/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 211ms/step - accuracy: 1.0000 - loss: 3.5865e-04 - val_accuracy: 0.9000 - val_loss: 0.4711\n",
      "Epoch 22/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 211ms/step - accuracy: 0.7864 - loss: 1.0726 - val_accuracy: 0.9000 - val_loss: 0.4047\n",
      "Epoch 23/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 211ms/step - accuracy: 0.9754 - loss: 0.0956 - val_accuracy: 0.9385 - val_loss: 0.3581\n",
      "Epoch 24/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 211ms/step - accuracy: 1.0000 - loss: 0.0126 - val_accuracy: 0.8769 - val_loss: 0.6498\n",
      "Epoch 25/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 212ms/step - accuracy: 0.9800 - loss: 0.0967 - val_accuracy: 0.8231 - val_loss: 0.8913\n",
      "Epoch 26/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 212ms/step - accuracy: 0.9439 - loss: 0.2199 - val_accuracy: 0.9000 - val_loss: 0.5924\n",
      "Epoch 27/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 212ms/step - accuracy: 0.9427 - loss: 0.2613 - val_accuracy: 0.9077 - val_loss: 0.4727\n",
      "Epoch 28/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 212ms/step - accuracy: 0.9885 - loss: 0.0508 - val_accuracy: 0.9077 - val_loss: 0.5838\n",
      "Epoch 29/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 212ms/step - accuracy: 0.9969 - loss: 0.0133 - val_accuracy: 0.6000 - val_loss: 2.3271\n",
      "Epoch 30/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 212ms/step - accuracy: 0.9866 - loss: 0.0517 - val_accuracy: 0.8308 - val_loss: 0.8575\n",
      "Epoch 31/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 212ms/step - accuracy: 0.9992 - loss: 0.0130 - val_accuracy: 0.8308 - val_loss: 0.7888\n",
      "Epoch 32/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 212ms/step - accuracy: 0.9851 - loss: 0.0571 - val_accuracy: 0.8308 - val_loss: 0.8587\n",
      "Epoch 33/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 212ms/step - accuracy: 0.9626 - loss: 0.1000 - val_accuracy: 0.9231 - val_loss: 0.4751\n",
      "Epoch 34/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 212ms/step - accuracy: 0.9948 - loss: 0.0274 - val_accuracy: 0.9308 - val_loss: 0.4299\n",
      "Epoch 35/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 212ms/step - accuracy: 0.9990 - loss: 0.0051 - val_accuracy: 0.8846 - val_loss: 0.5678\n",
      "Epoch 36/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 212ms/step - accuracy: 0.9903 - loss: 0.0374 - val_accuracy: 0.8692 - val_loss: 0.5151\n",
      "Epoch 37/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.9077 - val_loss: 0.4344\n",
      "Epoch 38/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 3.3139e-04 - val_accuracy: 0.9000 - val_loss: 0.4306\n",
      "Epoch 39/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 3.2297e-04 - val_accuracy: 0.9000 - val_loss: 0.5209\n",
      "Epoch 40/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 214ms/step - accuracy: 1.0000 - loss: 4.6283e-04 - val_accuracy: 0.8923 - val_loss: 0.5112\n",
      "Epoch 41/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 214ms/step - accuracy: 0.9491 - loss: 0.2833 - val_accuracy: 0.8077 - val_loss: 1.0506\n",
      "Epoch 42/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 214ms/step - accuracy: 0.9501 - loss: 0.4119 - val_accuracy: 0.8923 - val_loss: 0.4348\n",
      "Epoch 43/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 215ms/step - accuracy: 0.9968 - loss: 0.0161 - val_accuracy: 0.8308 - val_loss: 0.5465\n",
      "Epoch 44/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 215ms/step - accuracy: 0.9741 - loss: 0.1102 - val_accuracy: 0.8769 - val_loss: 0.4944\n",
      "Epoch 45/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 213ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.8615 - val_loss: 0.4141\n",
      "Epoch 46/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 214ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.9000 - val_loss: 0.4245\n",
      "Epoch 47/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 217ms/step - accuracy: 1.0000 - loss: 1.8257e-04 - val_accuracy: 0.9000 - val_loss: 0.4221\n",
      "Epoch 48/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 215ms/step - accuracy: 1.0000 - loss: 1.2163e-04 - val_accuracy: 0.9000 - val_loss: 0.4194\n",
      "Epoch 49/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 215ms/step - accuracy: 1.0000 - loss: 5.5025e-04 - val_accuracy: 0.8846 - val_loss: 0.5596\n",
      "Epoch 50/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 215ms/step - accuracy: 0.9887 - loss: 0.0543 - val_accuracy: 0.8231 - val_loss: 0.9877\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 572ms/step\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 403ms/step - accuracy: 0.8208 - loss: 0.9111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy for folder hsh: 0.8231\n",
      "Results for folder hsh saved.\n",
      "Model saved at: C:\\Users\\admin\\Desktop\\sihoon\\webcam\\results\\0102_model_hsh.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Mish activation function\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "def load_and_preprocess_image(image_path, target_size=(128, 128)):\n",
    "    image = load_img(image_path, color_mode='rgb', target_size=target_size)\n",
    "    image_array = img_to_array(image)\n",
    "    image_array /= 255.0\n",
    "    return image_array\n",
    "\n",
    "# 파일 이름에서 session과 point 추출\n",
    "def extract_session_and_point(filename):\n",
    "    session_match = re.search(r'img_(\\d+)', filename)\n",
    "    point_match = re.search(r'\\((\\d+)\\)', filename)\n",
    "    session = int(session_match.group(1)) if session_match else None\n",
    "    point = int(point_match.group(1)) if point_match else None\n",
    "    return session, point\n",
    "\n",
    "# ResNet18 모델 생성\n",
    "def create_resnet18_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = layers.Conv2D(64, kernel_size=(5, 7), padding='same')(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 3), padding='same')(x)\n",
    "    filter_sizes = [64, 128, 256, 512]\n",
    "    num_blocks = [2, 2, 2, 2]\n",
    "    for filters, blocks in zip(filter_sizes, num_blocks):\n",
    "        for i in range(blocks):\n",
    "            x = resnet_block(x, filters, downsample=(i == 0 and filters != 64))\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# ResNet 블록 생성\n",
    "def resnet_block(x, filters, downsample=False):\n",
    "    shortcut = x\n",
    "    strides = (2, 2) if downsample else (1, 1)\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), strides=strides, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if downsample or shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation(mish)(x)\n",
    "    return x\n",
    "\n",
    "# MLP 모델 생성\n",
    "def create_mlp_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = layers.Dense(128, activation=mish)(input_layer)\n",
    "    x = layers.Dense(64, activation=mish)(x)\n",
    "    x = layers.Dense(3, activation=mish)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "valid_labels = [1, 5, 9, 12, 16, 19, 23, 27, 30, 34, 37, 41, 45]\n",
    "\n",
    "def prepare_data(folder_path, csv_path, test_subject, valid_labels):\n",
    "    subject_folders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "    train_subjects = [s for s in subject_folders if s != test_subject]\n",
    "    \n",
    "    # 학습 데이터 로드\n",
    "    train_images, train_csv_data = [], pd.DataFrame()\n",
    "    for train_subject in train_subjects:\n",
    "        train_folder = os.path.join(folder_path, train_subject)\n",
    "        train_csv = os.path.join(csv_path, f\"{train_subject}.csv\")\n",
    "        train_images.extend(glob.glob(os.path.join(train_folder, '*.jpg')))\n",
    "        if os.path.exists(train_csv):\n",
    "            train_csv_data = pd.concat([train_csv_data, pd.read_csv(train_csv)])\n",
    "    train_csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "    \n",
    "    # 테스트 데이터 로드\n",
    "    test_folder = os.path.join(folder_path, test_subject)\n",
    "    test_csv = os.path.join(csv_path, f\"{test_subject}.csv\")\n",
    "    test_images = glob.glob(os.path.join(test_folder, '*.jpg'))\n",
    "    test_csv_data = pd.read_csv(test_csv) if os.path.exists(test_csv) else pd.DataFrame(columns=['Session', 'Point'])\n",
    "    test_csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "    \n",
    "    # 데이터 처리 함수\n",
    "    def process_images(image_paths, csv_data, valid_labels):\n",
    "        image_data = []\n",
    "        for img in image_paths:\n",
    "            session, point = extract_session_and_point(os.path.basename(img))\n",
    "            if point in valid_labels:\n",
    "                subject_name = os.path.basename(os.path.dirname(img))\n",
    "                unique_filename = f\"{subject_name}_{os.path.basename(img)}\"\n",
    "                image_data.append({\n",
    "                    'Filename': os.path.abspath(img),\n",
    "                    'UniqueFilename': unique_filename,\n",
    "                    'Session': session,\n",
    "                    'Point': point\n",
    "                })\n",
    "        df = pd.DataFrame(image_data)\n",
    "        merged = pd.merge(df, csv_data, on=['Session', 'Point'], how='inner')\n",
    "        merged = merged.drop_duplicates(subset=['UniqueFilename', 'Session', 'Point'])\n",
    "        merged = merged[merged['Point'].isin(valid_labels)]\n",
    "        return merged\n",
    "    \n",
    "    train_df = process_images(train_images, train_csv_data, valid_labels)\n",
    "    test_df = process_images(test_images, test_csv_data, valid_labels)\n",
    "    return train_df, test_df\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# hsh 폴더에서 train/test 분리 및 학습 진행\n",
    "def train_and_evaluate_from_single_folder(folder_path, csv_path, valid_labels, subject, test_size=0.2):\n",
    "    print(f\"Training and testing from folder: {subject}\")\n",
    "    \n",
    "    # hsh 폴더 데이터 로드\n",
    "    folder = os.path.join(folder_path, subject)\n",
    "    csv_file = os.path.join(csv_path, f\"{subject}.csv\")\n",
    "    image_paths = glob.glob(os.path.join(folder, '*.jpg'))\n",
    "    csv_data = pd.read_csv(csv_file) if os.path.exists(csv_file) else pd.DataFrame(columns=['Session', 'Point'])\n",
    "    csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "    \n",
    "    # 이미지와 라벨 처리\n",
    "    data = []\n",
    "    for img in image_paths:\n",
    "        session, point = extract_session_and_point(os.path.basename(img))\n",
    "        if point in valid_labels:\n",
    "            unique_filename = f\"{subject}_{os.path.basename(img)}\"\n",
    "            data.append({\n",
    "                'Filename': os.path.abspath(img),\n",
    "                'UniqueFilename': unique_filename,\n",
    "                'Session': session,\n",
    "                'Point': point\n",
    "            })\n",
    "    df = pd.DataFrame(data)\n",
    "    merged = pd.merge(df, csv_data, on=['Session', 'Point'], how='inner')\n",
    "    merged = merged.drop_duplicates(subset=['UniqueFilename', 'Session', 'Point'])\n",
    "    merged = merged[merged['Point'].isin(valid_labels)]\n",
    "    \n",
    "    # Train/Test Split\n",
    "    train_df, test_df = train_test_split(merged, test_size=test_size, random_state=42, stratify=merged['Point'])\n",
    "    \n",
    "    train_images_array = np.array([load_and_preprocess_image(path) for path in train_df['Filename']])\n",
    "    train_features = train_df.drop(['Filename', 'UniqueFilename', 'Session', 'Point'], axis=1).values\n",
    "    train_labels = train_df['Point'].map({label: idx for idx, label in enumerate(valid_labels)}).values\n",
    "    train_labels = to_categorical(train_labels, num_classes=len(valid_labels))\n",
    "    \n",
    "    test_images_array = np.array([load_and_preprocess_image(path) for path in test_df['Filename']])\n",
    "    test_features = test_df.drop(['Filename', 'UniqueFilename', 'Session', 'Point'], axis=1).values\n",
    "    test_labels = test_df['Point'].map({label: idx for idx, label in enumerate(valid_labels)}).values\n",
    "    test_labels = to_categorical(test_labels, num_classes=len(valid_labels))\n",
    "    \n",
    "    # 모델 생성\n",
    "    right_eye_model = create_resnet18_model((128, 128, 3))\n",
    "    left_eye_model = create_resnet18_model((128, 128, 3))\n",
    "    mlp_model = create_mlp_model(train_features.shape[1:])\n",
    "    combined_input = layers.concatenate([right_eye_model.output, left_eye_model.output, mlp_model.output])\n",
    "    x = layers.Dense(256, activation=mish)(combined_input)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    output_layer = layers.Dense(len(valid_labels), activation='softmax')(x)\n",
    "    combined_model = Model(inputs=[right_eye_model.input, left_eye_model.input, mlp_model.input], outputs=output_layer)\n",
    "\n",
    "    combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = combined_model.fit(\n",
    "        [train_images_array, train_images_array, train_features], train_labels,\n",
    "        validation_data=([test_images_array, test_images_array, test_features], test_labels),\n",
    "        epochs=50,\n",
    "        batch_size=1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 평가\n",
    "    predictions = combined_model.predict([test_images_array, test_images_array, test_features])\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    actual_classes = np.argmax(test_labels, axis=1)\n",
    "\n",
    "    loss, accuracy = combined_model.evaluate([test_images_array, test_images_array, test_features], test_labels)\n",
    "    print(f\"Testing accuracy for folder {subject}: {accuracy:.4f}\")\n",
    "    \n",
    "    # 결과 저장\n",
    "    results = []\n",
    "    for idx, (image_path, feature, pred_class, actual_class) in enumerate(zip(\n",
    "        test_df['Filename'], test_features, predicted_classes, actual_classes)):\n",
    "        results.append({\n",
    "            'Subject': subject,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Loss': loss,\n",
    "            'Image File': image_path,\n",
    "            'Feature': feature.tolist(),\n",
    "            'Predicted Class': pred_class,\n",
    "            'Actual Class': actual_class\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(os.path.join(csv_path, f\"0102_results_{subject}.csv\"), index=False, encoding='utf-8')\n",
    "    print(f\"Results for folder {subject} saved.\")\n",
    "    \n",
    "    # 모델 저장\n",
    "    model_save_path = os.path.join(csv_path, f\"0102_model_{subject}.h5\")\n",
    "    combined_model.save(model_save_path)\n",
    "    print(f\"Model saved at: {model_save_path}\")\n",
    "\n",
    "# 데이터 경로\n",
    "folder_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\img\"\n",
    "csv_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\results\"\n",
    "subject = 'hsh'\n",
    "\n",
    "# 동일 폴더에서 train/test 분리 및 실행\n",
    "train_and_evaluate_from_single_folder(folder_path, csv_path, valid_labels, subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657e02f-cc88-4cc2-882f-e83c192465c2",
   "metadata": {},
   "source": [
    "## Resnet 18, 9class, 13class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b87e9f5-c1b3-4514-9cc6-7a8d86630d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing from folder: hsh\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\sihoon\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_556', 'keras_tensor_621', 'keras_tensor_686']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 168ms/step - accuracy: 0.2126 - loss: 3.3935 - val_accuracy: 0.1889 - val_loss: 1.9803\n",
      "Epoch 2/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 163ms/step - accuracy: 0.6152 - loss: 1.0700 - val_accuracy: 0.1889 - val_loss: 3.6414\n",
      "Epoch 3/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 164ms/step - accuracy: 0.6717 - loss: 0.7357 - val_accuracy: 0.7333 - val_loss: 0.6641\n",
      "Epoch 4/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 165ms/step - accuracy: 0.7566 - loss: 0.6178 - val_accuracy: 0.8556 - val_loss: 0.4814\n",
      "Epoch 5/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 169ms/step - accuracy: 0.8310 - loss: 0.3939 - val_accuracy: 0.8667 - val_loss: 0.6367\n",
      "Epoch 6/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 166ms/step - accuracy: 0.8750 - loss: 0.3835 - val_accuracy: 0.8333 - val_loss: 0.6050\n",
      "Epoch 7/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 165ms/step - accuracy: 0.9127 - loss: 0.3093 - val_accuracy: 0.8333 - val_loss: 0.5625\n",
      "Epoch 8/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 169ms/step - accuracy: 0.9694 - loss: 0.0846 - val_accuracy: 0.7222 - val_loss: 0.9196\n",
      "Epoch 9/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 165ms/step - accuracy: 0.9241 - loss: 0.2592 - val_accuracy: 0.9000 - val_loss: 0.3356\n",
      "Epoch 10/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 166ms/step - accuracy: 0.9089 - loss: 0.2702 - val_accuracy: 0.9222 - val_loss: 0.2000\n",
      "Epoch 11/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 165ms/step - accuracy: 0.9934 - loss: 0.0193 - val_accuracy: 0.7111 - val_loss: 1.1721\n",
      "Epoch 12/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 164ms/step - accuracy: 0.8872 - loss: 0.3886 - val_accuracy: 0.8667 - val_loss: 0.5257\n",
      "Epoch 13/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 160ms/step - accuracy: 0.9813 - loss: 0.0657 - val_accuracy: 0.7000 - val_loss: 1.0561\n",
      "Epoch 14/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 159ms/step - accuracy: 0.9335 - loss: 0.3270 - val_accuracy: 0.9667 - val_loss: 0.1212\n",
      "Epoch 15/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 0.9667 - val_loss: 0.1177\n",
      "Epoch 16/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 160ms/step - accuracy: 0.9970 - loss: 0.0166 - val_accuracy: 0.8778 - val_loss: 0.3636\n",
      "Epoch 17/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 159ms/step - accuracy: 0.9244 - loss: 0.4283 - val_accuracy: 0.9667 - val_loss: 0.1405\n",
      "Epoch 18/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.9667 - val_loss: 0.1310\n",
      "Epoch 19/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.9667 - val_loss: 0.1712\n",
      "Epoch 20/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9556 - val_loss: 0.1550\n",
      "Epoch 21/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 4.0476e-04 - val_accuracy: 0.9556 - val_loss: 0.1509\n",
      "Epoch 22/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 3.2737e-04 - val_accuracy: 0.9667 - val_loss: 0.1345\n",
      "Epoch 23/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 7.5478e-04 - val_accuracy: 0.9778 - val_loss: 0.1227\n",
      "Epoch 24/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 1.2244e-04 - val_accuracy: 0.9667 - val_loss: 0.1660\n",
      "Epoch 25/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 9.6054e-05 - val_accuracy: 0.9667 - val_loss: 0.1568\n",
      "Epoch 26/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 1.0041e-04 - val_accuracy: 0.9778 - val_loss: 0.1459\n",
      "Epoch 27/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 161ms/step - accuracy: 0.9929 - loss: 0.0373 - val_accuracy: 0.2889 - val_loss: 9.9622\n",
      "Epoch 28/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 161ms/step - accuracy: 0.7411 - loss: 1.1225 - val_accuracy: 0.7333 - val_loss: 1.8663\n",
      "Epoch 29/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 0.0114 - val_accuracy: 0.7222 - val_loss: 2.8738\n",
      "Epoch 30/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 162ms/step - accuracy: 0.9976 - loss: 0.0077 - val_accuracy: 0.7667 - val_loss: 1.6164\n",
      "Epoch 31/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 162ms/step - accuracy: 0.9943 - loss: 0.0303 - val_accuracy: 0.6778 - val_loss: 3.4208\n",
      "Epoch 32/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 161ms/step - accuracy: 0.9639 - loss: 0.2139 - val_accuracy: 0.7667 - val_loss: 3.9143\n",
      "Epoch 33/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 162ms/step - accuracy: 0.9413 - loss: 0.1970 - val_accuracy: 0.8444 - val_loss: 1.5139\n",
      "Epoch 34/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 167ms/step - accuracy: 0.9937 - loss: 0.0588 - val_accuracy: 0.7889 - val_loss: 2.3633\n",
      "Epoch 35/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 0.8222 - val_loss: 2.1247\n",
      "Epoch 36/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.7333 - val_loss: 3.0832\n",
      "Epoch 37/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 4.9452e-04 - val_accuracy: 0.7889 - val_loss: 2.5815\n",
      "Epoch 38/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 2.5337e-04 - val_accuracy: 0.8000 - val_loss: 2.4313\n",
      "Epoch 39/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 164ms/step - accuracy: 1.0000 - loss: 2.0422e-04 - val_accuracy: 0.8000 - val_loss: 2.4308\n",
      "Epoch 40/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 1.7908e-04 - val_accuracy: 0.7889 - val_loss: 2.3522\n",
      "Epoch 41/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 220ms/step - accuracy: 1.0000 - loss: 3.3644e-04 - val_accuracy: 0.7889 - val_loss: 2.4815\n",
      "Epoch 42/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 308ms/step - accuracy: 1.0000 - loss: 3.5127e-04 - val_accuracy: 0.8222 - val_loss: 2.1460\n",
      "Epoch 43/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 245ms/step - accuracy: 1.0000 - loss: 8.2051e-05 - val_accuracy: 0.7889 - val_loss: 2.3823\n",
      "Epoch 44/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 1.3855e-04 - val_accuracy: 0.7889 - val_loss: 2.7751\n",
      "Epoch 45/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 165ms/step - accuracy: 1.0000 - loss: 9.2773e-05 - val_accuracy: 0.8000 - val_loss: 2.3065\n",
      "Epoch 46/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 171ms/step - accuracy: 1.0000 - loss: 3.1039e-05 - val_accuracy: 0.8444 - val_loss: 1.9653\n",
      "Epoch 47/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 167ms/step - accuracy: 1.0000 - loss: 4.3129e-05 - val_accuracy: 0.8444 - val_loss: 1.9595\n",
      "Epoch 48/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 4.7348e-05 - val_accuracy: 0.8444 - val_loss: 1.6902\n",
      "Epoch 49/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 291ms/step - accuracy: 1.0000 - loss: 5.7392e-05 - val_accuracy: 0.8444 - val_loss: 1.8725\n",
      "Epoch 50/50\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 298ms/step - accuracy: 0.9775 - loss: 0.1426 - val_accuracy: 0.6111 - val_loss: 2.1811\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step  \n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 227ms/step - accuracy: 0.6220 - loss: 2.1288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy for folder hsh: 0.6111\n",
      "Results for folder hsh saved.\n",
      "Model saved at: C:\\Users\\admin\\Desktop\\sihoon\\webcam\\results\\0102_model_hsh.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Mish activation function\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "def load_and_preprocess_image(image_path, target_size=(128, 128)):\n",
    "    image = load_img(image_path, color_mode='rgb', target_size=target_size)\n",
    "    image_array = img_to_array(image)\n",
    "    image_array /= 255.0\n",
    "    return image_array\n",
    "\n",
    "# 파일 이름에서 session과 point 추출\n",
    "def extract_session_and_point(filename):\n",
    "    session_match = re.search(r'img_(\\d+)', filename)\n",
    "    point_match = re.search(r'\\((\\d+)\\)', filename)\n",
    "    session = int(session_match.group(1)) if session_match else None\n",
    "    point = int(point_match.group(1)) if point_match else None\n",
    "    return session, point\n",
    "\n",
    "# ResNet18 모델 생성\n",
    "def create_resnet18_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = layers.Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same')(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    filter_sizes = [64, 128, 256, 512]\n",
    "    num_blocks = [2, 2, 2, 2]\n",
    "    for filters, blocks in zip(filter_sizes, num_blocks):\n",
    "        for i in range(blocks):\n",
    "            x = resnet_block(x, filters, downsample=(i == 0 and filters != 64))\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# ResNet 블록 생성\n",
    "def resnet_block(x, filters, downsample=False):\n",
    "    shortcut = x\n",
    "    strides = (2, 2) if downsample else (1, 1)\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), strides=strides, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if downsample or shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation(mish)(x)\n",
    "    return x\n",
    "\n",
    "# MLP 모델 생성\n",
    "def create_mlp_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = layers.Dense(128, activation=mish)(input_layer)\n",
    "    x = layers.Dense(64, activation=mish)(x)\n",
    "    x = layers.Dense(3, activation=mish)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "valid_labels = [1, 5, 9, 19, 23, 27, 37, 41, 45]\n",
    "\n",
    "def prepare_data(folder_path, csv_path, test_subject, valid_labels):\n",
    "    subject_folders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "    train_subjects = [s for s in subject_folders if s != test_subject]\n",
    "    \n",
    "    # 학습 데이터 로드\n",
    "    train_images, train_csv_data = [], pd.DataFrame()\n",
    "    for train_subject in train_subjects:\n",
    "        train_folder = os.path.join(folder_path, train_subject)\n",
    "        train_csv = os.path.join(csv_path, f\"{train_subject}.csv\")\n",
    "        train_images.extend(glob.glob(os.path.join(train_folder, '*.jpg')))\n",
    "        if os.path.exists(train_csv):\n",
    "            train_csv_data = pd.concat([train_csv_data, pd.read_csv(train_csv)])\n",
    "    train_csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "    \n",
    "    # 테스트 데이터 로드\n",
    "    test_folder = os.path.join(folder_path, test_subject)\n",
    "    test_csv = os.path.join(csv_path, f\"{test_subject}.csv\")\n",
    "    test_images = glob.glob(os.path.join(test_folder, '*.jpg'))\n",
    "    test_csv_data = pd.read_csv(test_csv) if os.path.exists(test_csv) else pd.DataFrame(columns=['Session', 'Point'])\n",
    "    test_csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "    \n",
    "    # 데이터 처리 함수\n",
    "    def process_images(image_paths, csv_data, valid_labels):\n",
    "        image_data = []\n",
    "        for img in image_paths:\n",
    "            session, point = extract_session_and_point(os.path.basename(img))\n",
    "            if point in valid_labels:\n",
    "                subject_name = os.path.basename(os.path.dirname(img))\n",
    "                unique_filename = f\"{subject_name}_{os.path.basename(img)}\"\n",
    "                image_data.append({\n",
    "                    'Filename': os.path.abspath(img),\n",
    "                    'UniqueFilename': unique_filename,\n",
    "                    'Session': session,\n",
    "                    'Point': point\n",
    "                })\n",
    "        df = pd.DataFrame(image_data)\n",
    "        merged = pd.merge(df, csv_data, on=['Session', 'Point'], how='inner')\n",
    "        merged = merged.drop_duplicates(subset=['UniqueFilename', 'Session', 'Point'])\n",
    "        merged = merged[merged['Point'].isin(valid_labels)]\n",
    "        return merged\n",
    "    \n",
    "    train_df = process_images(train_images, train_csv_data, valid_labels)\n",
    "    test_df = process_images(test_images, test_csv_data, valid_labels)\n",
    "    return train_df, test_df\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# hsh 폴더에서 train/test 분리 및 학습 진행\n",
    "def train_and_evaluate_from_single_folder(folder_path, csv_path, valid_labels, subject, test_size=0.2):\n",
    "    print(f\"Training and testing from folder: {subject}\")\n",
    "    \n",
    "    # hsh 폴더 데이터 로드\n",
    "    folder = os.path.join(folder_path, subject)\n",
    "    csv_file = os.path.join(csv_path, f\"{subject}.csv\")\n",
    "    image_paths = glob.glob(os.path.join(folder, '*.jpg'))\n",
    "    csv_data = pd.read_csv(csv_file) if os.path.exists(csv_file) else pd.DataFrame(columns=['Session', 'Point'])\n",
    "    csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "    \n",
    "    # 이미지와 라벨 처리\n",
    "    data = []\n",
    "    for img in image_paths:\n",
    "        session, point = extract_session_and_point(os.path.basename(img))\n",
    "        if point in valid_labels:\n",
    "            unique_filename = f\"{subject}_{os.path.basename(img)}\"\n",
    "            data.append({\n",
    "                'Filename': os.path.abspath(img),\n",
    "                'UniqueFilename': unique_filename,\n",
    "                'Session': session,\n",
    "                'Point': point\n",
    "            })\n",
    "    df = pd.DataFrame(data)\n",
    "    merged = pd.merge(df, csv_data, on=['Session', 'Point'], how='inner')\n",
    "    merged = merged.drop_duplicates(subset=['UniqueFilename', 'Session', 'Point'])\n",
    "    merged = merged[merged['Point'].isin(valid_labels)]\n",
    "    \n",
    "    # Train/Test Split\n",
    "    train_df, test_df = train_test_split(merged, test_size=test_size, random_state=42, stratify=merged['Point'])\n",
    "    \n",
    "    train_images_array = np.array([load_and_preprocess_image(path) for path in train_df['Filename']])\n",
    "    train_features = train_df.drop(['Filename', 'UniqueFilename', 'Session', 'Point'], axis=1).values\n",
    "    train_labels = train_df['Point'].map({label: idx for idx, label in enumerate(valid_labels)}).values\n",
    "    train_labels = to_categorical(train_labels, num_classes=len(valid_labels))\n",
    "    \n",
    "    test_images_array = np.array([load_and_preprocess_image(path) for path in test_df['Filename']])\n",
    "    test_features = test_df.drop(['Filename', 'UniqueFilename', 'Session', 'Point'], axis=1).values\n",
    "    test_labels = test_df['Point'].map({label: idx for idx, label in enumerate(valid_labels)}).values\n",
    "    test_labels = to_categorical(test_labels, num_classes=len(valid_labels))\n",
    "    \n",
    "    # 모델 생성\n",
    "    right_eye_model = create_resnet18_model((128, 128, 3))\n",
    "    left_eye_model = create_resnet18_model((128, 128, 3))\n",
    "    mlp_model = create_mlp_model(train_features.shape[1:])\n",
    "    combined_input = layers.concatenate([right_eye_model.output, left_eye_model.output, mlp_model.output])\n",
    "    x = layers.Dense(256, activation=mish)(combined_input)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    output_layer = layers.Dense(len(valid_labels), activation='softmax')(x)\n",
    "    combined_model = Model(inputs=[right_eye_model.input, left_eye_model.input, mlp_model.input], outputs=output_layer)\n",
    "\n",
    "    combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = combined_model.fit(\n",
    "        [train_images_array, train_images_array, train_features], train_labels,\n",
    "        validation_data=([test_images_array, test_images_array, test_features], test_labels),\n",
    "        epochs=50,\n",
    "        batch_size=1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 평가\n",
    "    predictions = combined_model.predict([test_images_array, test_images_array, test_features])\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    actual_classes = np.argmax(test_labels, axis=1)\n",
    "\n",
    "    loss, accuracy = combined_model.evaluate([test_images_array, test_images_array, test_features], test_labels)\n",
    "    print(f\"Testing accuracy for folder {subject}: {accuracy:.4f}\")\n",
    "    \n",
    "    # 결과 저장\n",
    "    results = []\n",
    "    for idx, (image_path, feature, pred_class, actual_class) in enumerate(zip(\n",
    "        test_df['Filename'], test_features, predicted_classes, actual_classes)):\n",
    "        results.append({\n",
    "            'Subject': subject,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Loss': loss,\n",
    "            'Image File': image_path,\n",
    "            'Feature': feature.tolist(),\n",
    "            'Predicted Class': pred_class,\n",
    "            'Actual Class': actual_class\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(os.path.join(csv_path, f\"0102_results_{subject}.csv\"), index=False, encoding='utf-8')\n",
    "    print(f\"Results for folder {subject} saved.\")\n",
    "    \n",
    "    # 모델 저장\n",
    "    model_save_path = os.path.join(csv_path, f\"0102_model_{subject}.h5\")\n",
    "    combined_model.save(model_save_path)\n",
    "    print(f\"Model saved at: {model_save_path}\")\n",
    "\n",
    "# 데이터 경로\n",
    "folder_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\img\"\n",
    "csv_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\results\"\n",
    "subject = 'hsh'\n",
    "\n",
    "# 동일 폴더에서 train/test 분리 및 실행\n",
    "train_and_evaluate_from_single_folder(folder_path, csv_path, valid_labels, subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eb2cd05-db1a-4f66-8899-31d90d2a7663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing from folder: hsh\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\sihoon\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_695', 'keras_tensor_760', 'keras_tensor_825']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 275ms/step - accuracy: 0.1565 - loss: 3.8154 - val_accuracy: 0.4231 - val_loss: 1.7898\n",
      "Epoch 2/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 303ms/step - accuracy: 0.5072 - loss: 1.3620 - val_accuracy: 0.7231 - val_loss: 0.8345\n",
      "Epoch 3/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 301ms/step - accuracy: 0.6054 - loss: 1.0471 - val_accuracy: 0.5846 - val_loss: 1.0482\n",
      "Epoch 4/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 302ms/step - accuracy: 0.7595 - loss: 0.6796 - val_accuracy: 0.8462 - val_loss: 0.5744\n",
      "Epoch 5/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 309ms/step - accuracy: 0.8845 - loss: 0.3233 - val_accuracy: 0.9077 - val_loss: 0.3480\n",
      "Epoch 6/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 302ms/step - accuracy: 0.8648 - loss: 0.3724 - val_accuracy: 0.8308 - val_loss: 0.5852\n",
      "Epoch 7/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 300ms/step - accuracy: 0.9185 - loss: 0.3970 - val_accuracy: 0.9462 - val_loss: 0.2462\n",
      "Epoch 8/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 300ms/step - accuracy: 0.9243 - loss: 0.2226 - val_accuracy: 0.8154 - val_loss: 0.4518\n",
      "Epoch 9/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 308ms/step - accuracy: 0.9162 - loss: 0.2683 - val_accuracy: 0.8846 - val_loss: 0.3009\n",
      "Epoch 10/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 299ms/step - accuracy: 0.9297 - loss: 0.1875 - val_accuracy: 0.8385 - val_loss: 0.5456\n",
      "Epoch 11/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 299ms/step - accuracy: 0.9362 - loss: 0.1944 - val_accuracy: 0.9692 - val_loss: 0.1044\n",
      "Epoch 12/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 306ms/step - accuracy: 0.9781 - loss: 0.0633 - val_accuracy: 0.9692 - val_loss: 0.1265\n",
      "Epoch 13/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 302ms/step - accuracy: 0.9888 - loss: 0.0582 - val_accuracy: 0.9692 - val_loss: 0.1176\n",
      "Epoch 14/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 300ms/step - accuracy: 0.9703 - loss: 0.1182 - val_accuracy: 0.9692 - val_loss: 0.0759\n",
      "Epoch 15/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 294ms/step - accuracy: 0.9965 - loss: 0.0087 - val_accuracy: 0.9308 - val_loss: 0.2820\n",
      "Epoch 16/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 251ms/step - accuracy: 0.9902 - loss: 0.0652 - val_accuracy: 0.9308 - val_loss: 0.2562\n",
      "Epoch 17/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 299ms/step - accuracy: 0.9269 - loss: 0.3344 - val_accuracy: 0.8769 - val_loss: 0.4781\n",
      "Epoch 18/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 310ms/step - accuracy: 0.9551 - loss: 0.1516 - val_accuracy: 0.9231 - val_loss: 0.2897\n",
      "Epoch 19/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 299ms/step - accuracy: 0.9815 - loss: 0.0841 - val_accuracy: 0.9308 - val_loss: 0.1815\n",
      "Epoch 20/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 302ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.9692 - val_loss: 0.1034\n",
      "Epoch 21/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 309ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.9769 - val_loss: 0.1226\n",
      "Epoch 22/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 300ms/step - accuracy: 1.0000 - loss: 5.0336e-04 - val_accuracy: 0.9692 - val_loss: 0.1096\n",
      "Epoch 23/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 308ms/step - accuracy: 1.0000 - loss: 3.3917e-04 - val_accuracy: 0.9769 - val_loss: 0.0889\n",
      "Epoch 24/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 299ms/step - accuracy: 1.0000 - loss: 1.7725e-04 - val_accuracy: 0.9692 - val_loss: 0.0835\n",
      "Epoch 25/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 300ms/step - accuracy: 1.0000 - loss: 1.0740e-04 - val_accuracy: 0.9846 - val_loss: 0.0738\n",
      "Epoch 26/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 309ms/step - accuracy: 1.0000 - loss: 7.6769e-05 - val_accuracy: 0.9769 - val_loss: 0.0758\n",
      "Epoch 27/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 299ms/step - accuracy: 1.0000 - loss: 2.1930e-04 - val_accuracy: 0.9846 - val_loss: 0.0899\n",
      "Epoch 28/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 298ms/step - accuracy: 1.0000 - loss: 5.4128e-05 - val_accuracy: 0.9846 - val_loss: 0.1035\n",
      "Epoch 29/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 308ms/step - accuracy: 1.0000 - loss: 2.9058e-05 - val_accuracy: 0.9846 - val_loss: 0.1050\n",
      "Epoch 30/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 299ms/step - accuracy: 1.0000 - loss: 3.9387e-05 - val_accuracy: 0.9769 - val_loss: 0.0963\n",
      "Epoch 31/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 299ms/step - accuracy: 1.0000 - loss: 4.1816e-05 - val_accuracy: 0.9846 - val_loss: 0.0758\n",
      "Epoch 32/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 310ms/step - accuracy: 0.9999 - loss: 0.0018 - val_accuracy: 0.9077 - val_loss: 0.6949\n",
      "Epoch 41/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 311ms/step - accuracy: 0.9917 - loss: 0.0204 - val_accuracy: 0.8846 - val_loss: 0.6591\n",
      "Epoch 44/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 306ms/step - accuracy: 0.9720 - loss: 0.0867 - val_accuracy: 0.8231 - val_loss: 1.9577\n",
      "Epoch 47/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 301ms/step - accuracy: 0.9988 - loss: 0.0106 - val_accuracy: 0.9769 - val_loss: 0.0669\n",
      "Epoch 49/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 301ms/step - accuracy: 0.9928 - loss: 0.0221 - val_accuracy: 0.9308 - val_loss: 0.4001\n",
      "Epoch 50/50\n",
      "\u001b[1m520/520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 300ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.9154 - val_loss: 0.8847\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002362FA2D580> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002362FA2D580> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 441ms/stepWARNING:tensorflow:6 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002362FA2D580> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002362FA2D580> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 358ms/step - accuracy: 0.9132 - loss: 0.7261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy for folder hsh: 0.9154\n",
      "Results for folder hsh saved.\n",
      "Model saved at: C:\\Users\\admin\\Desktop\\sihoon\\webcam\\results\\0102_model_hsh.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Mish activation function\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "def load_and_preprocess_image(image_path, target_size=(128, 128)):\n",
    "    image = load_img(image_path, color_mode='rgb', target_size=target_size)\n",
    "    image_array = img_to_array(image)\n",
    "    image_array /= 255.0\n",
    "    return image_array\n",
    "\n",
    "# 파일 이름에서 session과 point 추출\n",
    "def extract_session_and_point(filename):\n",
    "    session_match = re.search(r'img_(\\d+)', filename)\n",
    "    point_match = re.search(r'\\((\\d+)\\)', filename)\n",
    "    session = int(session_match.group(1)) if session_match else None\n",
    "    point = int(point_match.group(1)) if point_match else None\n",
    "    return session, point\n",
    "\n",
    "# ResNet18 모델 생성\n",
    "def create_resnet18_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = layers.Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same')(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    filter_sizes = [64, 128, 256, 512]\n",
    "    num_blocks = [2, 2, 2, 2]\n",
    "    for filters, blocks in zip(filter_sizes, num_blocks):\n",
    "        for i in range(blocks):\n",
    "            x = resnet_block(x, filters, downsample=(i == 0 and filters != 64))\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# ResNet 블록 생성\n",
    "def resnet_block(x, filters, downsample=False):\n",
    "    shortcut = x\n",
    "    strides = (2, 2) if downsample else (1, 1)\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), strides=strides, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if downsample or shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv2D(filters, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation(mish)(x)\n",
    "    return x\n",
    "\n",
    "# MLP 모델 생성\n",
    "def create_mlp_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = layers.Dense(128, activation=mish)(input_layer)\n",
    "    x = layers.Dense(64, activation=mish)(x)\n",
    "    x = layers.Dense(3, activation=mish)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "valid_labels = [1, 5, 9, 12, 16, 19, 23, 27, 30, 34, 37, 41, 45]\n",
    "\n",
    "def prepare_data(folder_path, csv_path, test_subject, valid_labels):\n",
    "    subject_folders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "    train_subjects = [s for s in subject_folders if s != test_subject]\n",
    "    \n",
    "    # 학습 데이터 로드\n",
    "    train_images, train_csv_data = [], pd.DataFrame()\n",
    "    for train_subject in train_subjects:\n",
    "        train_folder = os.path.join(folder_path, train_subject)\n",
    "        train_csv = os.path.join(csv_path, f\"{train_subject}.csv\")\n",
    "        train_images.extend(glob.glob(os.path.join(train_folder, '*.jpg')))\n",
    "        if os.path.exists(train_csv):\n",
    "            train_csv_data = pd.concat([train_csv_data, pd.read_csv(train_csv)])\n",
    "    train_csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "    \n",
    "    # 테스트 데이터 로드\n",
    "    test_folder = os.path.join(folder_path, test_subject)\n",
    "    test_csv = os.path.join(csv_path, f\"{test_subject}.csv\")\n",
    "    test_images = glob.glob(os.path.join(test_folder, '*.jpg'))\n",
    "    test_csv_data = pd.read_csv(test_csv) if os.path.exists(test_csv) else pd.DataFrame(columns=['Session', 'Point'])\n",
    "    test_csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "    \n",
    "    # 데이터 처리 함수\n",
    "    def process_images(image_paths, csv_data, valid_labels):\n",
    "        image_data = []\n",
    "        for img in image_paths:\n",
    "            session, point = extract_session_and_point(os.path.basename(img))\n",
    "            if point in valid_labels:\n",
    "                subject_name = os.path.basename(os.path.dirname(img))\n",
    "                unique_filename = f\"{subject_name}_{os.path.basename(img)}\"\n",
    "                image_data.append({\n",
    "                    'Filename': os.path.abspath(img),\n",
    "                    'UniqueFilename': unique_filename,\n",
    "                    'Session': session,\n",
    "                    'Point': point\n",
    "                })\n",
    "        df = pd.DataFrame(image_data)\n",
    "        merged = pd.merge(df, csv_data, on=['Session', 'Point'], how='inner')\n",
    "        merged = merged.drop_duplicates(subset=['UniqueFilename', 'Session', 'Point'])\n",
    "        merged = merged[merged['Point'].isin(valid_labels)]\n",
    "        return merged\n",
    "    \n",
    "    train_df = process_images(train_images, train_csv_data, valid_labels)\n",
    "    test_df = process_images(test_images, test_csv_data, valid_labels)\n",
    "    return train_df, test_df\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# hsh 폴더에서 train/test 분리 및 학습 진행\n",
    "def train_and_evaluate_from_single_folder(folder_path, csv_path, valid_labels, subject, test_size=0.2):\n",
    "    print(f\"Training and testing from folder: {subject}\")\n",
    "    \n",
    "    # hsh 폴더 데이터 로드\n",
    "    folder = os.path.join(folder_path, subject)\n",
    "    csv_file = os.path.join(csv_path, f\"{subject}.csv\")\n",
    "    image_paths = glob.glob(os.path.join(folder, '*.jpg'))\n",
    "    csv_data = pd.read_csv(csv_file) if os.path.exists(csv_file) else pd.DataFrame(columns=['Session', 'Point'])\n",
    "    csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "    \n",
    "    # 이미지와 라벨 처리\n",
    "    data = []\n",
    "    for img in image_paths:\n",
    "        session, point = extract_session_and_point(os.path.basename(img))\n",
    "        if point in valid_labels:\n",
    "            unique_filename = f\"{subject}_{os.path.basename(img)}\"\n",
    "            data.append({\n",
    "                'Filename': os.path.abspath(img),\n",
    "                'UniqueFilename': unique_filename,\n",
    "                'Session': session,\n",
    "                'Point': point\n",
    "            })\n",
    "    df = pd.DataFrame(data)\n",
    "    merged = pd.merge(df, csv_data, on=['Session', 'Point'], how='inner')\n",
    "    merged = merged.drop_duplicates(subset=['UniqueFilename', 'Session', 'Point'])\n",
    "    merged = merged[merged['Point'].isin(valid_labels)]\n",
    "    \n",
    "    # Train/Test Split\n",
    "    train_df, test_df = train_test_split(merged, test_size=test_size, random_state=42, stratify=merged['Point'])\n",
    "    \n",
    "    train_images_array = np.array([load_and_preprocess_image(path) for path in train_df['Filename']])\n",
    "    train_features = train_df.drop(['Filename', 'UniqueFilename', 'Session', 'Point'], axis=1).values\n",
    "    train_labels = train_df['Point'].map({label: idx for idx, label in enumerate(valid_labels)}).values\n",
    "    train_labels = to_categorical(train_labels, num_classes=len(valid_labels))\n",
    "    \n",
    "    test_images_array = np.array([load_and_preprocess_image(path) for path in test_df['Filename']])\n",
    "    test_features = test_df.drop(['Filename', 'UniqueFilename', 'Session', 'Point'], axis=1).values\n",
    "    test_labels = test_df['Point'].map({label: idx for idx, label in enumerate(valid_labels)}).values\n",
    "    test_labels = to_categorical(test_labels, num_classes=len(valid_labels))\n",
    "    \n",
    "    # 모델 생성\n",
    "    right_eye_model = create_resnet18_model((128, 128, 3))\n",
    "    left_eye_model = create_resnet18_model((128, 128, 3))\n",
    "    mlp_model = create_mlp_model(train_features.shape[1:])\n",
    "    combined_input = layers.concatenate([right_eye_model.output, left_eye_model.output, mlp_model.output])\n",
    "    x = layers.Dense(256, activation=mish)(combined_input)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    output_layer = layers.Dense(len(valid_labels), activation='softmax')(x)\n",
    "    combined_model = Model(inputs=[right_eye_model.input, left_eye_model.input, mlp_model.input], outputs=output_layer)\n",
    "\n",
    "    combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = combined_model.fit(\n",
    "        [train_images_array, train_images_array, train_features], train_labels,\n",
    "        validation_data=([test_images_array, test_images_array, test_features], test_labels),\n",
    "        epochs=50,\n",
    "        batch_size=1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # 평가\n",
    "    predictions = combined_model.predict([test_images_array, test_images_array, test_features])\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    actual_classes = np.argmax(test_labels, axis=1)\n",
    "\n",
    "    loss, accuracy = combined_model.evaluate([test_images_array, test_images_array, test_features], test_labels)\n",
    "    print(f\"Testing accuracy for folder {subject}: {accuracy:.4f}\")\n",
    "    \n",
    "    # 결과 저장\n",
    "    results = []\n",
    "    for idx, (image_path, feature, pred_class, actual_class) in enumerate(zip(\n",
    "        test_df['Filename'], test_features, predicted_classes, actual_classes)):\n",
    "        results.append({\n",
    "            'Subject': subject,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Loss': loss,\n",
    "            'Image File': image_path,\n",
    "            'Feature': feature.tolist(),\n",
    "            'Predicted Class': pred_class,\n",
    "            'Actual Class': actual_class\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(os.path.join(csv_path, f\"0102_results_{subject}.csv\"), index=False, encoding='utf-8')\n",
    "    print(f\"Results for folder {subject} saved.\")\n",
    "    \n",
    "    # 모델 저장\n",
    "    model_save_path = os.path.join(csv_path, f\"0102_model_{subject}.h5\")\n",
    "    combined_model.save(model_save_path)\n",
    "    print(f\"Model saved at: {model_save_path}\")\n",
    "\n",
    "# 데이터 경로\n",
    "folder_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\img\"\n",
    "csv_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\results\"\n",
    "subject = 'hsh'\n",
    "\n",
    "# 동일 폴더에서 train/test 분리 및 실행\n",
    "train_and_evaluate_from_single_folder(folder_path, csv_path, valid_labels, subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9124b404-4c9c-4010-99db-50e7cb324788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
