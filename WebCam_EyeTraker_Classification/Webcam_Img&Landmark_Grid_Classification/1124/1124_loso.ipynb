{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b603318-8dc0-4c43-b788-165bdf9d1726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on subject: hsb\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\sihoon\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor', 'keras_tensor_170', 'keras_tensor_340']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same shape. Received: target.shape=(1, 46), output.shape=(1, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 148\u001b[0m\n\u001b[0;32m    145\u001b[0m combined_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m combined_model\u001b[38;5;241m.\u001b[39mfit([train_images_array, train_images_array, train_features], train_labels, \n\u001b[0;32m    149\u001b[0m                    epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# 테스트 데이터 평가\u001b[39;00m\n\u001b[0;32m    152\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m combined_model\u001b[38;5;241m.\u001b[39mevaluate([test_images_array, test_images_array, test_features], test_labels)\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:587\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n\u001b[1;32m--> 587\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    588\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same shape. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    590\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    591\u001b[0m         )\n\u001b[0;32m    593\u001b[0m output, from_logits \u001b[38;5;241m=\u001b[39m _get_logits(\n\u001b[0;32m    594\u001b[0m     output, from_logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    595\u001b[0m )\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_logits:\n",
      "\u001b[1;31mValueError\u001b[0m: Arguments `target` and `output` must have the same shape. Received: target.shape=(1, 46), output.shape=(1, 2)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Mish activation function\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "def load_and_preprocess_image(image_path, target_size=(128, 128)):\n",
    "    image = load_img(image_path, color_mode='grayscale', target_size=target_size)\n",
    "    image_array = img_to_array(image)\n",
    "    image_array /= 255.0\n",
    "    return image_array\n",
    "\n",
    "# 파일 이름에서 session과 point 추출\n",
    "def extract_session_and_point(filename):\n",
    "    session_match = re.search(r'img_(\\d+)', filename)\n",
    "    point_match = re.search(r'\\((\\d+)\\)', filename)\n",
    "    session = int(session_match.group(1)) if session_match else None\n",
    "    point = int(point_match.group(1)) if point_match else None\n",
    "    return session, point\n",
    "\n",
    "# 병목 구조\n",
    "def bottleneck_block(x, filters, downsample=False):\n",
    "    shortcut = x\n",
    "    strides = (2, 2) if downsample else (1, 1)\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size=(1, 1), strides=strides, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "\n",
    "    x = layers.Conv2D(filters * 4, kernel_size=(1, 1), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if downsample or shortcut.shape[-1] != filters * 4:\n",
    "        shortcut = layers.Conv2D(filters * 4, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n",
    "    \n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation(mish)(x)\n",
    "    return x\n",
    "\n",
    "# ResNet-50\n",
    "def create_resnet50_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same')(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    filter_sizes = [64, 128, 256, 512]\n",
    "    num_blocks = [3, 4, 6, 3]  # ResNet-50에서 각 블록별 반복 횟수\n",
    "    for filters, blocks in zip(filter_sizes, num_blocks):\n",
    "        for i in range(blocks):\n",
    "            x = bottleneck_block(x, filters, downsample=(i == 0 and filters != 64))\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# MLP\n",
    "def create_mlp_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = layers.Dense(128, activation=mish)(input_layer)\n",
    "    x = layers.Dense(64, activation=mish)(x)\n",
    "    x = layers.Dense(3, activation=mish)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# LOSO 교차 검증\n",
    "folder_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\img\"\n",
    "csv_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\results\"\n",
    "\n",
    "subject_folders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "results = []\n",
    "\n",
    "for test_subject in subject_folders:\n",
    "    print(f\"Testing on subject: {test_subject}\")\n",
    "    \n",
    "    test_folder = os.path.join(folder_path, test_subject)\n",
    "    test_csv = os.path.join(csv_path, f\"{test_subject}.csv\")\n",
    "    test_images = glob.glob(os.path.join(test_folder, '*.jpg'))\n",
    "    \n",
    "    test_image_data = []\n",
    "    for img in test_images:\n",
    "        session, point = extract_session_and_point(os.path.basename(img))\n",
    "        test_image_data.append({'Filename': img, 'Session': session, 'Point': point})\n",
    "    test_df = pd.DataFrame(test_image_data)\n",
    "    \n",
    "    if os.path.exists(test_csv):\n",
    "        test_csv_data = pd.read_csv(test_csv)\n",
    "        test_merged = pd.merge(test_df, test_csv_data, left_on=['Session', 'Point'], right_on=['session', 'point'])\n",
    "    else:\n",
    "        print(f\"No CSV data for {test_subject}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    test_images_array = np.array([load_and_preprocess_image(path) for path in test_merged['Filename']])\n",
    "    test_features = test_merged.drop(['Filename', 'session', 'point', 'Session', 'Point'], axis=1).values\n",
    "    test_labels = to_categorical(test_merged['Point'].values)\n",
    "    \n",
    "    train_subjects = [s for s in subject_folders if s != test_subject]\n",
    "    train_images = []\n",
    "    train_csv_data = pd.DataFrame()\n",
    "    for train_subject in train_subjects:\n",
    "        train_folder = os.path.join(folder_path, train_subject)\n",
    "        train_csv = os.path.join(csv_path, f\"{train_subject}.csv\")\n",
    "        train_images.extend(glob.glob(os.path.join(train_folder, '*.jpg')))\n",
    "        if os.path.exists(train_csv):\n",
    "            train_csv_data = pd.concat([train_csv_data, pd.read_csv(train_csv)])\n",
    "    \n",
    "    train_image_data = []\n",
    "    for img in train_images:\n",
    "        session, point = extract_session_and_point(os.path.basename(img))\n",
    "        train_image_data.append({'Filename': img, 'Session': session, 'Point': point})\n",
    "    train_df = pd.DataFrame(train_image_data)\n",
    "    train_merged = pd.merge(train_df, train_csv_data, left_on=['Session', 'Point'], right_on=['session', 'point'])\n",
    "    \n",
    "    train_images_array = np.array([load_and_preprocess_image(path) for path in train_merged['Filename']])\n",
    "    train_features = train_merged.drop(['Filename', 'session', 'point', 'Session', 'Point'], axis=1).values\n",
    "    train_labels = to_categorical(train_merged['Point'].values)\n",
    "    \n",
    "    # 모델 구성\n",
    "    right_eye_model = create_resnet50_model((128, 128, 1))\n",
    "    left_eye_model = create_resnet50_model((128, 128, 1))\n",
    "    mlp_model = create_mlp_model(train_features.shape[1:])\n",
    "    \n",
    "    combined_input = layers.concatenate([right_eye_model.output, left_eye_model.output, mlp_model.output])\n",
    "    x = layers.Dense(256, activation=mish)(combined_input)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output_layer = layers.Dense(len(valid_labels), activation='softmax')(x)\n",
    "    \n",
    "    combined_model = Model(inputs=[right_eye_model.input, left_eye_model.input, mlp_model.input], outputs=output_layer)\n",
    "    \n",
    "    combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # 모델 학습\n",
    "    combined_model.fit([train_images_array, train_images_array, train_features], train_labels, \n",
    "                       epochs=10, batch_size=1, verbose=1)\n",
    "    \n",
    "    # 테스트 데이터 평가\n",
    "    loss, accuracy = combined_model.evaluate([test_images_array, test_images_array, test_features], test_labels)\n",
    "    print(f\"Subject {test_subject} - Accuracy: {accuracy:.4f}\")\n",
    "    results.append({'Subject': test_subject, 'Accuracy': accuracy})\n",
    "\n",
    "# 최종 결과\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(csv_path, \"loso_results.csv\"), index=False)\n",
    "print(\"LOSO results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2563dabd-1128-4d1a-b4c2-a4d5a9318a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on subject: hsb\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'valid_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 141\u001b[0m\n\u001b[0;32m    139\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m256\u001b[39m, activation\u001b[38;5;241m=\u001b[39mmish)(combined_input)\n\u001b[0;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.5\u001b[39m)(x)\n\u001b[1;32m--> 141\u001b[0m output_layer \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;28mlen\u001b[39m(valid_labels), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n\u001b[0;32m    143\u001b[0m combined_model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39m[right_eye_model\u001b[38;5;241m.\u001b[39minput, left_eye_model\u001b[38;5;241m.\u001b[39minput, mlp_model\u001b[38;5;241m.\u001b[39minput], outputs\u001b[38;5;241m=\u001b[39moutput_layer)\n\u001b[0;32m    145\u001b[0m combined_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_labels' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Mish activation function\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "def load_and_preprocess_image(image_path, target_size=(128, 128)):\n",
    "    image = load_img(image_path, color_mode='grayscale', target_size=target_size)\n",
    "    image_array = img_to_array(image)\n",
    "    image_array /= 255.0\n",
    "    return image_array\n",
    "\n",
    "# 파일 이름에서 session과 point 추출\n",
    "def extract_session_and_point(filename):\n",
    "    session_match = re.search(r'img_(\\d+)', filename)\n",
    "    point_match = re.search(r'\\((\\d+)\\)', filename)\n",
    "    session = int(session_match.group(1)) if session_match else None\n",
    "    point = int(point_match.group(1)) if point_match else None\n",
    "    return session, point\n",
    "\n",
    "# 병목 구조\n",
    "def bottleneck_block(x, filters, downsample=False):\n",
    "    shortcut = x\n",
    "    strides = (2, 2) if downsample else (1, 1)\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size=(1, 1), strides=strides, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "\n",
    "    x = layers.Conv2D(filters * 4, kernel_size=(1, 1), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if downsample or shortcut.shape[-1] != filters * 4:\n",
    "        shortcut = layers.Conv2D(filters * 4, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n",
    "    \n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation(mish)(x)\n",
    "    return x\n",
    "\n",
    "# ResNet-50\n",
    "def create_resnet50_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same')(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    filter_sizes = [64, 128, 256, 512]\n",
    "    num_blocks = [3, 4, 6, 3]  # ResNet-50에서 각 블록별 반복 횟수\n",
    "    for filters, blocks in zip(filter_sizes, num_blocks):\n",
    "        for i in range(blocks):\n",
    "            x = bottleneck_block(x, filters, downsample=(i == 0 and filters != 64))\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# MLP\n",
    "def create_mlp_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = layers.Dense(128, activation=mish)(input_layer)\n",
    "    x = layers.Dense(64, activation=mish)(x)\n",
    "    x = layers.Dense(3, activation=mish)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# LOSO 교차 검증\n",
    "folder_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\img\"\n",
    "csv_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\results\"\n",
    "\n",
    "subject_folders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "results = []\n",
    "\n",
    "for test_subject in subject_folders:\n",
    "    print(f\"Testing on subject: {test_subject}\")\n",
    "    \n",
    "    test_folder = os.path.join(folder_path, test_subject)\n",
    "    test_csv = os.path.join(csv_path, f\"{test_subject}.csv\")\n",
    "    test_images = glob.glob(os.path.join(test_folder, '*.jpg'))\n",
    "    \n",
    "    test_image_data = []\n",
    "    for img in test_images:\n",
    "        session, point = extract_session_and_point(os.path.basename(img))\n",
    "        test_image_data.append({'Filename': os.path.abspath(img), 'Session': session, 'Point': point})\n",
    "    test_df = pd.DataFrame(test_image_data)\n",
    "    \n",
    "    if os.path.exists(test_csv):\n",
    "        test_csv_data = pd.read_csv(test_csv)\n",
    "        test_merged = pd.merge(test_df, test_csv_data, left_on=['Session', 'Point'], right_on=['session', 'point'])\n",
    "    else:\n",
    "        print(f\"No CSV data for {test_subject}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    test_images_array = np.array([load_and_preprocess_image(path) for path in test_merged['Filename']])\n",
    "    test_features = test_merged.drop(['Filename', 'session', 'point', 'Session', 'Point'], axis=1).values\n",
    "    test_labels = to_categorical(test_merged['Point'].values)\n",
    "    \n",
    "    train_subjects = [s for s in subject_folders if s != test_subject]\n",
    "    train_images = []\n",
    "    train_csv_data = pd.DataFrame()\n",
    "    for train_subject in train_subjects:\n",
    "        train_folder = os.path.join(folder_path, train_subject)\n",
    "        train_csv = os.path.join(csv_path, f\"{train_subject}.csv\")\n",
    "        train_images.extend(glob.glob(os.path.join(train_folder, '*.jpg')))\n",
    "        if os.path.exists(train_csv):\n",
    "            train_csv_data = pd.concat([train_csv_data, pd.read_csv(train_csv)])\n",
    "    \n",
    "    train_image_data = []\n",
    "    for img in train_images:\n",
    "        session, point = extract_session_and_point(os.path.basename(img))\n",
    "        train_image_data.append({'Filename': os.path.abspath(img), 'Session': session, 'Point': point})\n",
    "    train_df = pd.DataFrame(train_image_data)\n",
    "    train_merged = pd.merge(train_df, train_csv_data, left_on=['Session', 'Point'], right_on=['session', 'point'])\n",
    "    \n",
    "    train_images_array = np.array([load_and_preprocess_image(path) for path in train_merged['Filename']])\n",
    "    train_features = train_merged.drop(['Filename', 'session', 'point', 'Session', 'Point'], axis=1).values\n",
    "    train_labels = to_categorical(train_merged['Point'].values)\n",
    "    \n",
    "    # 모델 구성\n",
    "    right_eye_model = create_resnet50_model((128, 128, 1))\n",
    "    left_eye_model = create_resnet50_model((128, 128, 1))\n",
    "    mlp_model = create_mlp_model(train_features.shape[1:])\n",
    "    \n",
    "    combined_input = layers.concatenate([right_eye_model.output, left_eye_model.output, mlp_model.output])\n",
    "    x = layers.Dense(256, activation=mish)(combined_input)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output_layer = layers.Dense(len(valid_labels), activation='softmax')(x)\n",
    "    \n",
    "    combined_model = Model(inputs=[right_eye_model.input, left_eye_model.input, mlp_model.input], outputs=output_layer)\n",
    "    \n",
    "    combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # 모델 학습\n",
    "    combined_model.fit([train_images_array, train_images_array, train_features], train_labels, \n",
    "                       epochs=10, batch_size=1, verbose=1)\n",
    "    \n",
    "    # 테스트 데이터 평가\n",
    "    predictions = combined_model.predict([test_images_array, test_images_array, test_features])\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    actual_classes = np.argmax(test_labels, axis=1)\n",
    "    \n",
    "    loss, accuracy = combined_model.evaluate([test_images_array, test_images_array, test_features], test_labels)\n",
    "    print(f\"Subject {test_subject} - Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # 결과 저장 (절대 경로, 예측값, 실제 값 포함)\n",
    "    for idx, (image_path, feature, pred_class, actual_class) in enumerate(zip(\n",
    "        test_merged['Filename'], test_features, predicted_classes, actual_classes)):\n",
    "        results.append({\n",
    "            'Subject': test_subject,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Loss': loss,\n",
    "            'Image File': image_path,  # 절대 경로\n",
    "            'Feature': feature.tolist(),  # Features\n",
    "            'Predicted Class': pred_class,  # 예측 값\n",
    "            'Actual Class': actual_class   # 실제 값\n",
    "        })\n",
    "\n",
    "# 최종 결과\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 결과 저장\n",
    "results_df.to_csv(os.path.join(csv_path, \"loso_results_with_details.csv\"), index=False, encoding='utf-8')\n",
    "print(\"LOSO results with detailed information saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c769f9-8c83-47a4-9eca-af5e7f8a828b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on subject: hsb\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\sihoon\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_697', 'keras_tensor_867', 'keras_tensor_1037']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  628/18400\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40:05\u001b[0m 338ms/step - accuracy: 0.0499 - loss: 7.6274"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 161\u001b[0m\n\u001b[0;32m    158\u001b[0m combined_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m combined_model\u001b[38;5;241m.\u001b[39mfit([train_images_array, train_images_array, train_features], train_labels, \n\u001b[0;32m    162\u001b[0m                    epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# 테스트 데이터 평가\u001b[39;00m\n\u001b[0;32m    165\u001b[0m predictions \u001b[38;5;241m=\u001b[39m combined_model\u001b[38;5;241m.\u001b[39mpredict([test_images_array, test_images_array, test_features])\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1684\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1685\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1686\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1687\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1688\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1689\u001b[0m   )\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32m~\\.conda\\envs\\sihoon\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Mish activation function\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "def load_and_preprocess_image(image_path, target_size=(128, 128)):\n",
    "    image = load_img(image_path, color_mode='grayscale', target_size=target_size)\n",
    "    image_array = img_to_array(image)\n",
    "    image_array /= 255.0\n",
    "    return image_array\n",
    "\n",
    "# 파일 이름에서 session과 point 추출\n",
    "def extract_session_and_point(filename):\n",
    "    session_match = re.search(r'img_(\\d+)', filename)\n",
    "    point_match = re.search(r'\\((\\d+)\\)', filename)\n",
    "    session = int(session_match.group(1)) if session_match else None\n",
    "    point = int(point_match.group(1)) if point_match else None\n",
    "    return session, point\n",
    "\n",
    "# 병목 구조\n",
    "def bottleneck_block(x, filters, downsample=False):\n",
    "    shortcut = x\n",
    "    strides = (2, 2) if downsample else (1, 1)\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size=(1, 1), strides=strides, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "\n",
    "    x = layers.Conv2D(filters * 4, kernel_size=(1, 1), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if downsample or shortcut.shape[-1] != filters * 4:\n",
    "        shortcut = layers.Conv2D(filters * 4, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n",
    "    \n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation(mish)(x)\n",
    "    return x\n",
    "\n",
    "# ResNet-50\n",
    "def create_resnet50_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same')(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    filter_sizes = [64, 128, 256, 512]\n",
    "    num_blocks = [3, 4, 6, 3]  # ResNet-50에서 각 블록별 반복 횟수\n",
    "    for filters, blocks in zip(filter_sizes, num_blocks):\n",
    "        for i in range(blocks):\n",
    "            x = bottleneck_block(x, filters, downsample=(i == 0 and filters != 64))\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# MLP\n",
    "def create_mlp_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = layers.Dense(128, activation=mish)(input_layer)\n",
    "    x = layers.Dense(64, activation=mish)(x)\n",
    "    x = layers.Dense(3, activation=mish)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# 유효한 라벨 정의\n",
    "valid_labels = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45]\n",
    "\n",
    "# LOSO 교차 검증\n",
    "folder_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\img\"\n",
    "csv_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\results\"\n",
    "\n",
    "subject_folders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "results = []\n",
    "\n",
    "for test_subject in subject_folders:\n",
    "    print(f\"Testing on subject: {test_subject}\")\n",
    "    \n",
    "    test_folder = os.path.join(folder_path, test_subject)\n",
    "    test_csv = os.path.join(csv_path, f\"{test_subject}.csv\")\n",
    "    test_images = glob.glob(os.path.join(test_folder, '*.jpg'))\n",
    "    \n",
    "    test_image_data = []\n",
    "    for img in test_images:\n",
    "        session, point = extract_session_and_point(os.path.basename(img))\n",
    "        if point in valid_labels:  # valid_labels로 필터링\n",
    "            test_image_data.append({'Filename': os.path.abspath(img), 'Session': session, 'Point': point})\n",
    "    test_df = pd.DataFrame(test_image_data)\n",
    "    \n",
    "    if os.path.exists(test_csv):\n",
    "        test_csv_data = pd.read_csv(test_csv)\n",
    "        test_merged = pd.merge(test_df, test_csv_data, left_on=['Session', 'Point'], right_on=['session', 'point'])\n",
    "    else:\n",
    "        print(f\"No CSV data for {test_subject}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # valid_labels로 다시 필터링\n",
    "    test_merged = test_merged[test_merged['Point'].isin(valid_labels)]\n",
    "    \n",
    "    test_images_array = np.array([load_and_preprocess_image(path) for path in test_merged['Filename']])\n",
    "    test_features = test_merged.drop(['Filename', 'session', 'point', 'Session', 'Point'], axis=1).values\n",
    "    test_labels = test_merged['Point'].map({label: idx for idx, label in enumerate(valid_labels)}).values\n",
    "    test_labels = to_categorical(test_labels, num_classes=len(valid_labels))\n",
    "    \n",
    "    train_subjects = [s for s in subject_folders if s != test_subject]\n",
    "    train_images = []\n",
    "    train_csv_data = pd.DataFrame()\n",
    "    for train_subject in train_subjects:\n",
    "        train_folder = os.path.join(folder_path, train_subject)\n",
    "        train_csv = os.path.join(csv_path, f\"{train_subject}.csv\")\n",
    "        train_images.extend(glob.glob(os.path.join(train_folder, '*.jpg')))\n",
    "        if os.path.exists(train_csv):\n",
    "            train_csv_data = pd.concat([train_csv_data, pd.read_csv(train_csv)])\n",
    "    \n",
    "    train_image_data = []\n",
    "    for img in train_images:\n",
    "        session, point = extract_session_and_point(os.path.basename(img))\n",
    "        if point in valid_labels:  # valid_labels로 필터링\n",
    "            train_image_data.append({'Filename': os.path.abspath(img), 'Session': session, 'Point': point})\n",
    "    train_df = pd.DataFrame(train_image_data)\n",
    "    train_merged = pd.merge(train_df, train_csv_data, left_on=['Session', 'Point'], right_on=['session', 'point'])\n",
    "    \n",
    "    # valid_labels로 다시 필터링\n",
    "    train_merged = train_merged[train_merged['Point'].isin(valid_labels)]\n",
    "    \n",
    "    train_images_array = np.array([load_and_preprocess_image(path) for path in train_merged['Filename']])\n",
    "    train_features = train_merged.drop(['Filename', 'session', 'point', 'Session', 'Point'], axis=1).values\n",
    "    train_labels = train_merged['Point'].map({label: idx for idx, label in enumerate(valid_labels)}).values\n",
    "    train_labels = to_categorical(train_labels, num_classes=len(valid_labels))\n",
    "    \n",
    "    # 모델 구성\n",
    "    right_eye_model = create_resnet50_model((128, 128, 1))\n",
    "    left_eye_model = create_resnet50_model((128, 128, 1))\n",
    "    mlp_model = create_mlp_model(train_features.shape[1:])\n",
    "    \n",
    "    combined_input = layers.concatenate([right_eye_model.output, left_eye_model.output, mlp_model.output])\n",
    "    x = layers.Dense(256, activation=mish)(combined_input)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output_layer = layers.Dense(len(valid_labels), activation='softmax')(x)\n",
    "    \n",
    "    combined_model = Model(inputs=[right_eye_model.input, left_eye_model.input, mlp_model.input], outputs=output_layer)\n",
    "    \n",
    "    combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # 모델 학습\n",
    "    combined_model.fit([train_images_array, train_images_array, train_features], train_labels, \n",
    "                       epochs=10, batch_size=1, verbose=1)\n",
    "    \n",
    "    # 테스트 데이터 평가\n",
    "    predictions = combined_model.predict([test_images_array, test_images_array, test_features])\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    actual_classes = np.argmax(test_labels, axis=1)\n",
    "    \n",
    "    loss, accuracy = combined_model.evaluate([test_images_array, test_images_array, test_features], test_labels)\n",
    "    print(f\"Subject {test_subject} - Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # 결과 저장 (절대 경로, 예측값, 실제 값 포함)\n",
    "    for idx, (image_path, feature, pred_class, actual_class) in enumerate(zip(\n",
    "        test_merged['Filename'], test_features, predicted_classes, actual_classes)):\n",
    "        results.append({\n",
    "            'Subject': test_subject,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Loss': loss,\n",
    "            'Image File': image_path,  # 절대 경로\n",
    "            'Feature': feature.tolist(),  # Features\n",
    "            'Predicted Class': pred_class,  # 예측 값\n",
    "            'Actual Class': actual_class   # 실제 값\n",
    "        })\n",
    "\n",
    "# 최종 결과\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 결과 저장\n",
    "results_df.to_csv(os.path.join(csv_path, \"loso_results_with_valid_labels.csv\"), index=False, encoding='utf-8')\n",
    "print(\"LOSO results with valid labels saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5512eb36-e2b3-4f12-b8ae-52d4182ad662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject hsb 파일 개수: 2250\n",
      "Subject hsh 파일 개수: 2250\n",
      "Subject hyh 파일 개수: 2250\n",
      "Subject lgj 파일 개수: 2250\n",
      "Subject scy 파일 개수: 2250\n",
      "train_merged 데이터 개수: 4600\n",
      "train_left_features 개수: 2300\n",
      "train_right_features 개수: 2300\n",
      "훈련 데이터 Left 이미지 개수: 2300\n",
      "훈련 데이터 Right 이미지 개수: 2300\n",
      "훈련 데이터 Features 개수: 2300\n"
     ]
    }
   ],
   "source": [
    "# 1. 각 서브젝트 폴더에서 수집된 파일 개수 확인\n",
    "for subject in subject_folders:\n",
    "    subject_folder = os.path.join(folder_path, subject)\n",
    "    subject_files = glob.glob(os.path.join(subject_folder, '*.jpg'))\n",
    "    print(f\"Subject {subject} 파일 개수: {len(subject_files)}\")\n",
    "\n",
    "# 2. train_merged 크기 확인\n",
    "print(f\"train_merged 데이터 개수: {train_merged.shape[0]}\")\n",
    "\n",
    "# 3. train_left_features와 train_right_features 분리 후 크기 확인\n",
    "train_left_features = train_merged[train_merged['Filename'].str.contains('left', case=False)]\n",
    "train_right_features = train_merged[train_merged['Filename'].str.contains('right', case=False)]\n",
    "print(f\"train_left_features 개수: {train_left_features.shape[0]}\")\n",
    "print(f\"train_right_features 개수: {train_right_features.shape[0]}\")\n",
    "\n",
    "# 4. 최종 배열 크기 확인\n",
    "print(f\"훈련 데이터 Left 이미지 개수: {train_left_images_array.shape[0]}\")\n",
    "print(f\"훈련 데이터 Right 이미지 개수: {train_right_images_array.shape[0]}\")\n",
    "print(f\"훈련 데이터 Features 개수: {train_features.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f645e-aa74-4df7-9a9c-4d0df475bd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on subject: hsb\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\sihoon\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor', 'keras_tensor_170', 'keras_tensor_340']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m799s\u001b[0m 329ms/step - accuracy: 0.0509 - loss: 4.6492\n",
      "Epoch 2/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m756s\u001b[0m 329ms/step - accuracy: 0.0416 - loss: 3.1582\n",
      "Epoch 3/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m757s\u001b[0m 329ms/step - accuracy: 0.0303 - loss: 3.1676\n",
      "Epoch 4/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m757s\u001b[0m 329ms/step - accuracy: 0.0433 - loss: 3.1710\n",
      "Epoch 5/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m757s\u001b[0m 329ms/step - accuracy: 0.0330 - loss: 3.1377\n",
      "Epoch 6/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m773s\u001b[0m 336ms/step - accuracy: 0.0421 - loss: 3.1368\n",
      "Epoch 7/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m766s\u001b[0m 333ms/step - accuracy: 0.0367 - loss: 3.1381\n",
      "Epoch 8/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m757s\u001b[0m 329ms/step - accuracy: 0.0507 - loss: 3.1363\n",
      "Epoch 9/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m757s\u001b[0m 329ms/step - accuracy: 0.0471 - loss: 3.1367\n",
      "Epoch 10/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m762s\u001b[0m 331ms/step - accuracy: 0.0443 - loss: 3.1369\n",
      "Epoch 11/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m759s\u001b[0m 330ms/step - accuracy: 0.0311 - loss: 3.1366\n",
      "Epoch 12/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 330ms/step - accuracy: 0.0447 - loss: 3.1366\n",
      "Epoch 13/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 331ms/step - accuracy: 0.0379 - loss: 3.1366\n",
      "Epoch 14/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 330ms/step - accuracy: 0.0387 - loss: 3.1367\n",
      "Epoch 15/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 331ms/step - accuracy: 0.0503 - loss: 3.1366\n",
      "Epoch 16/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m760s\u001b[0m 331ms/step - accuracy: 0.0371 - loss: 3.1367\n",
      "Epoch 17/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m761s\u001b[0m 331ms/step - accuracy: 0.0363 - loss: 3.1367\n",
      "Epoch 18/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m761s\u001b[0m 331ms/step - accuracy: 0.0349 - loss: 3.1367\n",
      "Epoch 19/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m761s\u001b[0m 331ms/step - accuracy: 0.0416 - loss: 3.1369\n",
      "Epoch 20/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m762s\u001b[0m 331ms/step - accuracy: 0.0345 - loss: 3.1367\n",
      "Epoch 21/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m762s\u001b[0m 331ms/step - accuracy: 0.0426 - loss: 3.1366\n",
      "Epoch 22/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m762s\u001b[0m 331ms/step - accuracy: 0.0430 - loss: 3.1368\n",
      "Epoch 23/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m762s\u001b[0m 331ms/step - accuracy: 0.0404 - loss: 3.1368\n",
      "Epoch 24/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m764s\u001b[0m 332ms/step - accuracy: 0.0328 - loss: 3.1370\n",
      "Epoch 25/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 332ms/step - accuracy: 0.0396 - loss: 3.1365\n",
      "Epoch 26/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 332ms/step - accuracy: 0.0417 - loss: 3.1364\n",
      "Epoch 27/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 332ms/step - accuracy: 0.0369 - loss: 3.1369\n",
      "Epoch 28/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 332ms/step - accuracy: 0.0414 - loss: 3.1368\n",
      "Epoch 29/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m764s\u001b[0m 332ms/step - accuracy: 0.0299 - loss: 3.1365\n",
      "Epoch 30/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m763s\u001b[0m 332ms/step - accuracy: 0.0350 - loss: 3.1368\n",
      "Epoch 31/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m764s\u001b[0m 332ms/step - accuracy: 0.0406 - loss: 3.1366\n",
      "Epoch 32/50\n",
      "\u001b[1m2300/2300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m773s\u001b[0m 336ms/step - accuracy: 0.0422 - loss: 3.1363\n",
      "Epoch 33/50\n",
      "\u001b[1m 627/2300\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9:44\u001b[0m 350ms/step - accuracy: 0.0650 - loss: 3.1355"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "\n",
    "# Mish activation function\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "def load_and_preprocess_image(image_path, target_size=(128, 128)):\n",
    "    image = load_img(image_path, color_mode='grayscale', target_size=target_size)\n",
    "    image_array = img_to_array(image)\n",
    "    image_array /= 255.0\n",
    "    return image_array\n",
    "\n",
    "# 파일 이름에서 session과 point 추출\n",
    "def extract_session_and_point(filename):\n",
    "    session_match = re.search(r'img_(\\d+)', filename)\n",
    "    point_match = re.search(r'\\((\\d+)\\)', filename)\n",
    "    session = int(session_match.group(1)) if session_match else None\n",
    "    point = int(point_match.group(1)) if point_match else None\n",
    "    return session, point\n",
    "\n",
    "# ResNet-50\n",
    "def create_resnet50_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = layers.Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same')(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    filter_sizes = [64, 128, 256, 512]\n",
    "    num_blocks = [3, 4, 6, 3]\n",
    "    for filters, blocks in zip(filter_sizes, num_blocks):\n",
    "        for i in range(blocks):\n",
    "            x = bottleneck_block(x, filters, downsample=(i == 0 and filters != 64))\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# 병목 구조\n",
    "def bottleneck_block(x, filters, downsample=False):\n",
    "    shortcut = x\n",
    "    strides = (2, 2) if downsample else (1, 1)\n",
    "    x = layers.Conv2D(filters, kernel_size=(1, 1), strides=strides, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "\n",
    "    x = layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(mish)(x)\n",
    "\n",
    "    x = layers.Conv2D(filters * 4, kernel_size=(1, 1), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if downsample or shortcut.shape[-1] != filters * 4:\n",
    "        shortcut = layers.Conv2D(filters * 4, kernel_size=(1, 1), strides=strides, padding='same')(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation(mish)(x)\n",
    "    return x\n",
    "\n",
    "# MLP\n",
    "def create_mlp_model(input_shape):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = layers.Dense(128, activation=mish)(input_layer)\n",
    "    x = layers.Dense(64, activation=mish)(x)\n",
    "    x = layers.Dense(3, activation=mish)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# 유효한 라벨 정의\n",
    "valid_labels = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45]\n",
    "\n",
    "# LOSO 교차 검증\n",
    "folder_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\img\"\n",
    "csv_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\results\"\n",
    "subject_folders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "results = []\n",
    "\n",
    "for test_subject in subject_folders:\n",
    "    print(f\"Testing on subject: {test_subject}\")\n",
    "\n",
    "    # 테스트 데이터 준비\n",
    "    test_folder = os.path.join(folder_path, test_subject)\n",
    "    test_csv = os.path.join(csv_path, f\"{test_subject}.csv\")\n",
    "    test_images = glob.glob(os.path.join(test_folder, '*.jpg'))\n",
    "\n",
    "    test_image_data = []\n",
    "    for img in test_images:\n",
    "        session, point = extract_session_and_point(os.path.basename(img))\n",
    "        if point in valid_labels:\n",
    "            subject_name = os.path.basename(os.path.dirname(img))  # 폴더 이름\n",
    "            unique_filename = f\"{subject_name}_{os.path.basename(img)}\"\n",
    "            test_image_data.append({\n",
    "                'Filename': os.path.abspath(img),\n",
    "                'UniqueFilename': unique_filename,\n",
    "                'Session': session,\n",
    "                'Point': point\n",
    "            })\n",
    "    test_df = pd.DataFrame(test_image_data)\n",
    "\n",
    "\n",
    "    if os.path.exists(test_csv):\n",
    "        test_csv_data = pd.read_csv(test_csv)\n",
    "        test_csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "        test_merged = pd.merge(test_df, test_csv_data, on=['Session', 'Point'])\n",
    "    else:\n",
    "        print(f\"No CSV data for {test_subject}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    test_merged = test_merged.drop_duplicates(subset=['UniqueFilename', 'Session', 'Point'])\n",
    "\n",
    "    # 테스트 데이터 필터링\n",
    "    test_merged = test_merged[test_merged['Point'].isin(valid_labels)]\n",
    "    test_left_features = test_merged[test_merged['Filename'].str.contains('left', case=False)]\n",
    "    test_images_array = np.array([load_and_preprocess_image(path) for path in test_left_features['Filename']])\n",
    "    test_features = test_left_features.drop(['Filename', 'UniqueFilename', 'Session', 'Point'], axis=1).values\n",
    "    test_labels = test_left_features['Point'].map({label: idx for idx, label in enumerate(valid_labels)}).values\n",
    "    test_labels = to_categorical(test_labels, num_classes=len(valid_labels))\n",
    "\n",
    "    # 학습 데이터 준비\n",
    "    train_subjects = [s for s in subject_folders if s != test_subject]\n",
    "    train_images = []\n",
    "    train_csv_data = pd.DataFrame()\n",
    "    for train_subject in train_subjects:\n",
    "        train_folder = os.path.join(folder_path, train_subject)\n",
    "        train_csv = os.path.join(csv_path, f\"{train_subject}.csv\")\n",
    "        train_images.extend(glob.glob(os.path.join(train_folder, '*.jpg')))\n",
    "        if os.path.exists(train_csv):\n",
    "            train_csv_data = pd.concat([train_csv_data, pd.read_csv(train_csv)])\n",
    "\n",
    "    train_csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "    train_image_data = []\n",
    "    for img in train_images:\n",
    "        session, point = extract_session_and_point(os.path.basename(img))\n",
    "        if point in valid_labels:\n",
    "            subject_name = os.path.basename(os.path.dirname(img))  # 폴더 이름\n",
    "            unique_filename = f\"{subject_name}_{os.path.basename(img)}\"\n",
    "            train_image_data.append({\n",
    "                'Filename': os.path.abspath(img),\n",
    "                'UniqueFilename': unique_filename,\n",
    "                'Session': session,\n",
    "                'Point': point\n",
    "            })\n",
    "    train_df = pd.DataFrame(train_image_data)\n",
    "\n",
    "    train_merged = pd.merge(train_df, train_csv_data, on=['Session', 'Point'])\n",
    "    train_merged = train_merged.drop_duplicates(subset=['UniqueFilename', 'Session', 'Point'])\n",
    "\n",
    "    # 학습 데이터 필터링\n",
    "    train_left_features = train_merged[train_merged['Filename'].str.contains('left', case=False)]\n",
    "    train_images_array = np.array([load_and_preprocess_image(path) for path in train_left_features['Filename']])\n",
    "    train_features = train_left_features.drop(['Filename', 'Session', 'Point', 'UniqueFilename'], axis=1).values\n",
    "    train_labels = train_left_features['Point'].map({label: idx for idx, label in enumerate(valid_labels)}).values\n",
    "    train_labels = to_categorical(train_labels, num_classes=len(valid_labels))\n",
    "\n",
    "    # 모델 구성\n",
    "    right_eye_model = create_resnet50_model((128, 128, 1))\n",
    "    left_eye_model = create_resnet50_model((128, 128, 1))\n",
    "    mlp_model = create_mlp_model(train_features.shape[1:])\n",
    "    combined_input = layers.concatenate([right_eye_model.output, left_eye_model.output, mlp_model.output])\n",
    "    x = layers.Dense(256, activation=mish)(combined_input)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output_layer = layers.Dense(len(valid_labels), activation='softmax')(x)\n",
    "    combined_model = Model(inputs=[right_eye_model.input, left_eye_model.input, mlp_model.input], outputs=output_layer)\n",
    "\n",
    "    combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # 모델 학습\n",
    "    combined_model.fit([train_images_array, train_images_array, train_features], train_labels, \n",
    "                       epochs=50, batch_size=1, verbose=1)\n",
    "\n",
    "    # 테스트 데이터 평가\n",
    "    predictions = combined_model.predict([test_images_array, test_images_array, test_features])\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    actual_classes = np.argmax(test_labels, axis=1)\n",
    "\n",
    "    loss, accuracy = combined_model.evaluate([test_images_array, test_images_array, test_features], test_labels)\n",
    "    print(f\"Subject {test_subject} - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    model_save_path = os.path.join(csv_path, f\"1124_model_subject_{test_subject}.h5\")\n",
    "    combined_model.save(model_save_path)\n",
    "    print(f\"Model for subject {test_subject} saved at: {model_save_path}\")\n",
    "\n",
    "    # 결과 저장\n",
    "    for idx, (image_path, feature, pred_class, actual_class) in enumerate(zip(\n",
    "        test_left_features['Filename'], test_features, predicted_classes, actual_classes)):\n",
    "        results.append({\n",
    "            'Subject': test_subject,\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Test Loss': loss,\n",
    "            'Image File': image_path,\n",
    "            'Feature': feature.tolist(),\n",
    "            'Predicted Class': pred_class,\n",
    "            'Actual Class': actual_class\n",
    "        })\n",
    "\n",
    "# 최종 결과 저장\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(csv_path, \"1124_result.csv\"), index=False, encoding='utf-8')\n",
    "print(\"LOSO results with valid labels saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
