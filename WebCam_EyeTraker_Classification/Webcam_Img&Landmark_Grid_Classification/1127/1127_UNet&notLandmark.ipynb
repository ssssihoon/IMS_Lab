{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda8c001-f296-4565-9cdc-fe84fa6cf059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on subject: lgj\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\.conda\\envs\\sihoon\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor', 'keras_tensor_25']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m933s\u001b[0m 515ms/step - accuracy: 0.1189 - loss: 2.9531 - val_accuracy: 0.1111 - val_loss: 2.1989\n",
      "Epoch 2/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m931s\u001b[0m 517ms/step - accuracy: 0.1157 - loss: 8.4873 - val_accuracy: 0.1111 - val_loss: 2.1991\n",
      "Epoch 3/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m930s\u001b[0m 517ms/step - accuracy: 0.0961 - loss: 2.2046 - val_accuracy: 0.1111 - val_loss: 2.2012\n",
      "Epoch 4/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m944s\u001b[0m 524ms/step - accuracy: 0.0970 - loss: 2.2060 - val_accuracy: 0.1111 - val_loss: 2.1978\n",
      "Epoch 5/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m953s\u001b[0m 530ms/step - accuracy: 0.0943 - loss: 2.2052 - val_accuracy: 0.1111 - val_loss: 2.2022\n",
      "Epoch 6/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m949s\u001b[0m 527ms/step - accuracy: 0.1250 - loss: 2.2035 - val_accuracy: 0.1111 - val_loss: 2.1993\n",
      "Epoch 7/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m940s\u001b[0m 523ms/step - accuracy: 0.0906 - loss: 2.2165 - val_accuracy: 0.1111 - val_loss: 2.1990\n",
      "Epoch 8/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m930s\u001b[0m 516ms/step - accuracy: 0.1192 - loss: 2.2053 - val_accuracy: 0.1111 - val_loss: 2.1986\n",
      "Epoch 9/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m927s\u001b[0m 515ms/step - accuracy: 0.0970 - loss: 2.2045 - val_accuracy: 0.1111 - val_loss: 2.2006\n",
      "Epoch 10/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m928s\u001b[0m 515ms/step - accuracy: 0.1060 - loss: 2.2066 - val_accuracy: 0.1111 - val_loss: 2.1993\n",
      "Epoch 11/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m927s\u001b[0m 515ms/step - accuracy: 0.1008 - loss: 2.2050 - val_accuracy: 0.1111 - val_loss: 2.1997\n",
      "Epoch 12/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m928s\u001b[0m 516ms/step - accuracy: 0.1007 - loss: 2.2057 - val_accuracy: 0.1111 - val_loss: 2.1985\n",
      "Epoch 13/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m927s\u001b[0m 515ms/step - accuracy: 0.1112 - loss: 2.2024 - val_accuracy: 0.1111 - val_loss: 2.2000\n",
      "Epoch 14/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m929s\u001b[0m 516ms/step - accuracy: 0.1122 - loss: 2.2062 - val_accuracy: 0.1111 - val_loss: 2.2033\n",
      "Epoch 15/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m931s\u001b[0m 517ms/step - accuracy: 0.1076 - loss: 2.2086 - val_accuracy: 0.1111 - val_loss: 2.1982\n",
      "Epoch 16/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m929s\u001b[0m 516ms/step - accuracy: 0.1086 - loss: 2.2007 - val_accuracy: 0.1111 - val_loss: 2.1986\n",
      "Epoch 17/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m931s\u001b[0m 517ms/step - accuracy: 0.1048 - loss: 2.2041 - val_accuracy: 0.1111 - val_loss: 2.2029\n",
      "Epoch 18/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m932s\u001b[0m 518ms/step - accuracy: 0.1295 - loss: 2.2045 - val_accuracy: 0.1111 - val_loss: 2.1989\n",
      "Epoch 19/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m940s\u001b[0m 522ms/step - accuracy: 0.1087 - loss: 2.2060 - val_accuracy: 0.1111 - val_loss: 2.1996\n",
      "Epoch 20/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m941s\u001b[0m 523ms/step - accuracy: 0.0975 - loss: 2.2017 - val_accuracy: 0.1111 - val_loss: 2.2128\n",
      "Epoch 21/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m939s\u001b[0m 522ms/step - accuracy: 0.1234 - loss: 2.2033 - val_accuracy: 0.1111 - val_loss: 2.2006\n",
      "Epoch 22/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m940s\u001b[0m 522ms/step - accuracy: 0.1060 - loss: 2.2091 - val_accuracy: 0.1111 - val_loss: 2.2008\n",
      "Epoch 23/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m940s\u001b[0m 522ms/step - accuracy: 0.0826 - loss: 2.2088 - val_accuracy: 0.1111 - val_loss: 2.1998\n",
      "Epoch 24/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m939s\u001b[0m 522ms/step - accuracy: 0.0980 - loss: 2.2068 - val_accuracy: 0.1111 - val_loss: 2.2036\n",
      "Epoch 25/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m940s\u001b[0m 522ms/step - accuracy: 0.1011 - loss: 2.2068 - val_accuracy: 0.1111 - val_loss: 2.2064\n",
      "Epoch 26/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m940s\u001b[0m 522ms/step - accuracy: 0.1083 - loss: 2.2072 - val_accuracy: 0.1111 - val_loss: 2.2012\n",
      "Epoch 27/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m941s\u001b[0m 523ms/step - accuracy: 0.1142 - loss: 2.2075 - val_accuracy: 0.1111 - val_loss: 2.2118\n",
      "Epoch 28/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m941s\u001b[0m 523ms/step - accuracy: 0.1041 - loss: 2.2088 - val_accuracy: 0.1111 - val_loss: 2.2006\n",
      "Epoch 29/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m944s\u001b[0m 524ms/step - accuracy: 0.1008 - loss: 2.2032 - val_accuracy: 0.1111 - val_loss: 2.1996\n",
      "Epoch 30/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m940s\u001b[0m 522ms/step - accuracy: 0.0813 - loss: 2.2078 - val_accuracy: 0.1111 - val_loss: 2.1994\n",
      "Epoch 31/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m942s\u001b[0m 523ms/step - accuracy: 0.1129 - loss: 2.2029 - val_accuracy: 0.1111 - val_loss: 2.1992\n",
      "Epoch 32/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m945s\u001b[0m 525ms/step - accuracy: 0.1023 - loss: 2.2070 - val_accuracy: 0.1111 - val_loss: 2.1981\n",
      "Epoch 33/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m944s\u001b[0m 524ms/step - accuracy: 0.1025 - loss: 2.2054 - val_accuracy: 0.1111 - val_loss: 2.1998\n",
      "Epoch 34/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m946s\u001b[0m 525ms/step - accuracy: 0.1031 - loss: 2.2059 - val_accuracy: 0.1111 - val_loss: 2.2000\n",
      "Epoch 35/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m947s\u001b[0m 526ms/step - accuracy: 0.0841 - loss: 2.2089 - val_accuracy: 0.1111 - val_loss: 2.2040\n",
      "Epoch 36/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m944s\u001b[0m 524ms/step - accuracy: 0.1183 - loss: 2.2044 - val_accuracy: 0.1111 - val_loss: 2.2063\n",
      "Epoch 37/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m943s\u001b[0m 524ms/step - accuracy: 0.1094 - loss: 2.2064 - val_accuracy: 0.1111 - val_loss: 2.2037\n",
      "Epoch 38/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m947s\u001b[0m 526ms/step - accuracy: 0.1155 - loss: 2.2074 - val_accuracy: 0.1111 - val_loss: 2.2010\n",
      "Epoch 39/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m944s\u001b[0m 525ms/step - accuracy: 0.1000 - loss: 2.2071 - val_accuracy: 0.1111 - val_loss: 2.1990\n",
      "Epoch 40/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m946s\u001b[0m 526ms/step - accuracy: 0.1223 - loss: 2.2025 - val_accuracy: 0.1111 - val_loss: 2.2057\n",
      "Epoch 41/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m944s\u001b[0m 524ms/step - accuracy: 0.1155 - loss: 2.2086 - val_accuracy: 0.1111 - val_loss: 2.2003\n",
      "Epoch 42/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m952s\u001b[0m 529ms/step - accuracy: 0.1083 - loss: 2.2062 - val_accuracy: 0.1111 - val_loss: 2.2056\n",
      "Epoch 43/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m949s\u001b[0m 527ms/step - accuracy: 0.1014 - loss: 2.2071 - val_accuracy: 0.1111 - val_loss: 2.1994\n",
      "Epoch 44/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m945s\u001b[0m 525ms/step - accuracy: 0.0956 - loss: 2.2084 - val_accuracy: 0.1111 - val_loss: 2.2013\n",
      "Epoch 45/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m945s\u001b[0m 525ms/step - accuracy: 0.1144 - loss: 2.2023 - val_accuracy: 0.1111 - val_loss: 2.2037\n",
      "Epoch 46/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m945s\u001b[0m 525ms/step - accuracy: 0.1140 - loss: 2.1994 - val_accuracy: 0.1111 - val_loss: 2.2064\n",
      "Epoch 47/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m945s\u001b[0m 525ms/step - accuracy: 0.1168 - loss: 2.2091 - val_accuracy: 0.1111 - val_loss: 2.2073\n",
      "Epoch 48/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m944s\u001b[0m 525ms/step - accuracy: 0.1149 - loss: 2.2059 - val_accuracy: 0.1111 - val_loss: 2.2000\n",
      "Epoch 49/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m945s\u001b[0m 525ms/step - accuracy: 0.1130 - loss: 2.2037 - val_accuracy: 0.1111 - val_loss: 2.1997\n",
      "Epoch 50/50\n",
      "\u001b[1m1800/1800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m948s\u001b[0m 527ms/step - accuracy: 0.0998 - loss: 2.2049 - val_accuracy: 0.1111 - val_loss: 2.2123\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - accuracy: 0.1090 - loss: 2.2125\n",
      "Testing accuracy for subject lgj: 0.1111\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predicted_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 196\u001b[0m\n\u001b[0;32m    193\u001b[0m valid_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m27\u001b[39m, \u001b[38;5;241m37\u001b[39m, \u001b[38;5;241m41\u001b[39m, \u001b[38;5;241m45\u001b[39m]\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# LOSO 수행\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m train_and_evaluate(folder_path, csv_path, valid_labels, test_subjects)\n",
      "Cell \u001b[1;32mIn[2], line 167\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(folder_path, csv_path, valid_labels, test_subjects)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n\u001b[0;32m    165\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (image_path, feature, pred_class, actual_class) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m     test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFilename\u001b[39m\u001b[38;5;124m'\u001b[39m], test_features, predicted_classes, actual_classes)):\n\u001b[0;32m    168\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubject\u001b[39m\u001b[38;5;124m'\u001b[39m: test_subject,\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual Class\u001b[39m\u001b[38;5;124m'\u001b[39m: actual_class\n\u001b[0;32m    175\u001b[0m     })\n\u001b[0;32m    177\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted_classes' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "\n",
    "# Mish activation function\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "# U-Net 모델 생성\n",
    "def create_unet_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Contracting Path\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation=mish, padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(64, (3, 3), activation=mish, padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation=mish, padding='same')(p1)\n",
    "    c2 = layers.Conv2D(128, (3, 3), activation=mish, padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation=mish, padding='same')(p2)\n",
    "    c3 = layers.Conv2D(256, (3, 3), activation=mish, padding='same')(c3)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    # Bottleneck\n",
    "    b = layers.Conv2D(512, (3, 3), activation=mish, padding='same')(p3)\n",
    "    b = layers.Conv2D(512, (3, 3), activation=mish, padding='same')(b)\n",
    "\n",
    "    # Expanding Path\n",
    "    u3 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(b)\n",
    "    u3 = layers.concatenate([u3, c3])\n",
    "    c4 = layers.Conv2D(256, (3, 3), activation=mish, padding='same')(u3)\n",
    "    c4 = layers.Conv2D(256, (3, 3), activation=mish, padding='same')(c4)\n",
    "\n",
    "    u2 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c4)\n",
    "    u2 = layers.concatenate([u2, c2])\n",
    "    c5 = layers.Conv2D(128, (3, 3), activation=mish, padding='same')(u2)\n",
    "    c5 = layers.Conv2D(128, (3, 3), activation=mish, padding='same')(c5)\n",
    "\n",
    "    u1 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u1 = layers.concatenate([u1, c1])\n",
    "    c6 = layers.Conv2D(64, (3, 3), activation=mish, padding='same')(u1)\n",
    "    c6 = layers.Conv2D(64, (3, 3), activation=mish, padding='same')(c6)\n",
    "\n",
    "    outputs = layers.GlobalAveragePooling2D()(c6)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# def create_mlp_model(input_shape):\n",
    "#     input_layer = Input(shape=input_shape)\n",
    "#     x = layers.Dense(128, activation=mish)(input_layer)\n",
    "#     x = layers.Dense(64, activation=mish)(x)\n",
    "#     x = layers.Dense(3, activation=mish)(x)\n",
    "#     x = layers.Flatten()(x)\n",
    "#     return Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "def load_and_preprocess_image(image_path, target_size=(128, 128)):\n",
    "    image = load_img(image_path, color_mode='rgb', target_size=target_size)\n",
    "    image_array = img_to_array(image)\n",
    "    image_array /= 255.0\n",
    "    return image_array\n",
    "\n",
    "# 파일 이름에서 session과 point 추출\n",
    "def extract_session_and_point(filename):\n",
    "    session_match = re.search(r'img_(\\d+)', filename)\n",
    "    point_match = re.search(r'\\((\\d+)\\)', filename)\n",
    "    session = int(session_match.group(1)) if session_match else None\n",
    "    point = int(point_match.group(1)) if point_match else None\n",
    "    return session, point\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "def prepare_data(folder_path, csv_path, test_subject, valid_labels):\n",
    "    subject_folders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "    train_subjects = [s for s in subject_folders if s != test_subject]\n",
    "    \n",
    "    # 학습 데이터 로드\n",
    "    train_images, train_csv_data = [], pd.DataFrame()\n",
    "    for train_subject in train_subjects:\n",
    "        train_folder = os.path.join(folder_path, train_subject)\n",
    "        train_csv = os.path.join(csv_path, f\"{train_subject}.csv\")\n",
    "        train_images.extend(glob.glob(os.path.join(train_folder, '*.jpg')))\n",
    "        if os.path.exists(train_csv):\n",
    "            train_csv_data = pd.concat([train_csv_data, pd.read_csv(train_csv)])\n",
    "    train_csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "    \n",
    "    # 테스트 데이터 로드\n",
    "    test_folder = os.path.join(folder_path, test_subject)\n",
    "    test_csv = os.path.join(csv_path, f\"{test_subject}.csv\")\n",
    "    test_images = glob.glob(os.path.join(test_folder, '*.jpg'))\n",
    "    test_csv_data = pd.read_csv(test_csv) if os.path.exists(test_csv) else pd.DataFrame(columns=['Session', 'Point'])\n",
    "    test_csv_data.rename(columns={'session': 'Session', 'point': 'Point'}, inplace=True)\n",
    "    \n",
    "    # 데이터 처리 함수\n",
    "    def process_images(image_paths, csv_data, valid_labels):\n",
    "        image_data = []\n",
    "        for img in image_paths:\n",
    "            session, point = extract_session_and_point(os.path.basename(img))\n",
    "            if point in valid_labels:\n",
    "                subject_name = os.path.basename(os.path.dirname(img))\n",
    "                unique_filename = f\"{subject_name}_{os.path.basename(img)}\"\n",
    "                image_data.append({\n",
    "                    'Filename': os.path.abspath(img),\n",
    "                    'UniqueFilename': unique_filename,\n",
    "                    'Session': session,\n",
    "                    'Point': point\n",
    "                })\n",
    "        df = pd.DataFrame(image_data)\n",
    "        merged = pd.merge(df, csv_data, on=['Session', 'Point'], how='inner')\n",
    "        merged = merged.drop_duplicates(subset=['UniqueFilename', 'Session', 'Point'])\n",
    "        merged = merged[merged['Point'].isin(valid_labels)]\n",
    "        return merged\n",
    "    \n",
    "    train_df = process_images(train_images, train_csv_data, valid_labels)\n",
    "    test_df = process_images(test_images, test_csv_data, valid_labels)\n",
    "    return train_df, test_df\n",
    "\n",
    "# 모델 훈련 및 평가\n",
    "def train_and_evaluate(folder_path, csv_path, valid_labels, test_subjects):\n",
    "    accuracies = []\n",
    "    for test_subject in test_subjects:\n",
    "        print(f\"Testing on subject: {test_subject}\")\n",
    "        train_df, test_df = prepare_data(folder_path, csv_path, test_subject, valid_labels)\n",
    "        \n",
    "        train_images_array = np.array([load_and_preprocess_image(path) for path in train_df['Filename']])\n",
    "        train_features = train_df.drop(['Filename', 'UniqueFilename', 'Session', 'Point'], axis=1).values\n",
    "        train_labels = train_df['Point'].map({label: idx for idx, label in enumerate(valid_labels)}).values\n",
    "        train_labels = to_categorical(train_labels, num_classes=len(valid_labels))\n",
    "        \n",
    "        test_images_array = np.array([load_and_preprocess_image(path) for path in test_df['Filename']])\n",
    "        test_features = test_df.drop(['Filename', 'UniqueFilename', 'Session', 'Point'], axis=1).values\n",
    "        test_labels = test_df['Point'].map({label: idx for idx, label in enumerate(valid_labels)}).values\n",
    "        test_labels = to_categorical(test_labels, num_classes=len(valid_labels))\n",
    "        \n",
    "        right_eye_model = create_unet_model((128, 128, 3))\n",
    "        left_eye_model = create_unet_model((128, 128, 3))\n",
    "        combined_input = layers.concatenate([right_eye_model.output, left_eye_model.output])\n",
    "        x = layers.Dense(256, activation=mish)(combined_input)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "\n",
    "        output_layer = layers.Dense(len(valid_labels), activation='softmax')(x)\n",
    "        combined_model = Model(inputs=[right_eye_model.input, left_eye_model.input], outputs=output_layer)\n",
    "\n",
    "        combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        combined_model.fit(\n",
    "            [train_images_array, train_images_array], train_labels,\n",
    "            validation_data=([test_images_array, test_images_array], test_labels),\n",
    "            epochs=50,\n",
    "            batch_size=1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        loss, accuracy = combined_model.evaluate([test_images_array, test_images_array], test_labels)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f\"Testing accuracy for subject {test_subject}: {accuracy:.4f}\")\n",
    "        \n",
    "        # 결과 저장\n",
    "        results = []\n",
    "        for idx, (image_path, feature, pred_class, actual_class) in enumerate(zip(\n",
    "            test_df['Filename'], test_features, predicted_classes, actual_classes)):\n",
    "            results.append({\n",
    "                'Subject': test_subject,\n",
    "                'Test Accuracy': accuracy,\n",
    "                'Test Loss': loss,\n",
    "                'Image File': image_path,\n",
    "                'Predicted Class': pred_class,\n",
    "                'Actual Class': actual_class\n",
    "            })\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(os.path.join(csv_path, f\"1127_UNet%notLandmark_results_{test_subject}.csv\"), index=False, encoding='utf-8')\n",
    "        print(f\"Results for subject {test_subject} saved.\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        model_save_path = os.path.join(csv_path, f\"1127_UNet%notLandmark_model_{test_subject}.h5\")\n",
    "        combined_model.save(model_save_path)\n",
    "        print(f\"Model saved at: {model_save_path}\")\n",
    "    \n",
    "    print(f\"Average Accuracy: {np.mean(accuracies):.4f}\")\n",
    "\n",
    "# 데이터 경로\n",
    "folder_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\img\"\n",
    "csv_path = r\"C:\\Users\\admin\\Desktop\\sihoon\\webcam\\results\"\n",
    "test_subjects = ['lgj', 'hsb', 'scy']  # 테스트로 사용할 대상\n",
    "\n",
    "valid_labels = [1, 5, 9, 19, 23, 27, 37, 41, 45]\n",
    "\n",
    "# LOSO 수행\n",
    "train_and_evaluate(folder_path, csv_path, valid_labels, test_subjects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9afcd4-5048-4d91-bc3b-c9d9a91f0f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
