{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GfKgDEls1hu"
      },
      "source": [
        "### Label Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Seq4oitve_g4",
        "outputId": "ae5267c0-ff32-4b50-9ec0-c95a631ead13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0     25\n",
              "1     25\n",
              "2     22\n",
              "3     25\n",
              "4     30\n",
              "5     13\n",
              "6     30\n",
              "7     19\n",
              "8     29\n",
              "9     28\n",
              "10    27\n",
              "11    27\n",
              "12    35\n",
              "13    33\n",
              "14    23\n",
              "15    15\n",
              "16    28\n",
              "17    35\n",
              "18    15\n",
              "19    38\n",
              "20    19\n",
              "21    41\n",
              "22    14\n",
              "23    19\n",
              "24    34\n",
              "25    18\n",
              "26    33\n",
              "27    25\n",
              "28    24\n",
              "29    24\n",
              "30    28\n",
              "31    39\n",
              "32    40\n",
              "33    38\n",
              "34    36\n",
              "35    29\n",
              "36    25\n",
              "37    31\n",
              "38    26\n",
              "39    37\n",
              "40    31\n",
              "41    25\n",
              "42    28\n",
              "43    23\n",
              "44    23\n",
              "45    26\n",
              "46    20\n",
              "47    27\n",
              "48    27\n",
              "Name: hamd0, dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# target 값 setting\n",
        "label_data = pd.read_excel(\"D:\\MDD\\subject_info_mdd.xlsx\")\n",
        "\n",
        "label = label_data['hamd0']\n",
        "label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADzMRVKQsxZQ"
      },
      "source": [
        "### Raw data Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIj9CvYbriMz",
        "outputId": "462b4b85-5ea9-4c17-c38c-7417421cdcac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "불러온 데이터 배열 형태: (49, 62, 10000)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import scipy.io\n",
        "\n",
        "# 데이터 폴더 경로 설정\n",
        "data_folder = 'D:\\MDD\\LowData_Alpha'\n",
        "\n",
        "# 파일 개수와 배열 크기 설정\n",
        "file_count = 49\n",
        "x_size = 10000\n",
        "y_size = 62\n",
        "\n",
        "# 데이터를 저장할 리스트 초기화\n",
        "data_list = []\n",
        "\n",
        "# 각 파일을 순회하면서 데이터 불러오기\n",
        "for i in range(1, file_count + 1):\n",
        "    # 파일 이름 생성 (숫자를 2자리로 포맷팅)\n",
        "    file_name = f'D:\\MDD\\LowData_Alpha\\MDD_{i:02}_alpha.mat'\n",
        "    file_path = os.path.join(data_folder, file_name)  # 파일 경로 설정\n",
        "    mat_contents = scipy.io.loadmat(file_path)  # .mat 파일 불러오기\n",
        "\n",
        "    # 데이터 배열 추출 및 리스트에 추가\n",
        "    data = mat_contents['Filtered_fp']  # Filtered_fp 데이터 변수 불러오기\n",
        "    data_list.append(data)\n",
        "\n",
        "# 리스트를 numpy 배열로 변환\n",
        "import numpy as np\n",
        "data_array = np.array(data_list)\n",
        "\n",
        "# 배열의 모양 확인\n",
        "print(\"불러온 데이터 배열 형태:\", data_array.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haLuGLE0F9ut"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x = data_array\n",
        "y = label\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s533z2ptG6oa",
        "outputId": "3b0185ec-f13b-4482-923a-dc64e9405d4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.16.1-cp39-cp39-win_amd64.whl (2.1 kB)\n",
            "Collecting tensorflow-intel==2.16.1\n",
            "  Downloading tensorflow_intel-2.16.1-cp39-cp39-win_amd64.whl (376.9 MB)\n",
            "     -------------------------------------- 376.9/376.9 MB 1.4 MB/s eta 0:00:00\n",
            "Collecting ml-dtypes~=0.3.1\n",
            "  Downloading ml_dtypes-0.3.2-cp39-cp39-win_amd64.whl (127 kB)\n",
            "     -------------------------------------- 127.7/127.7 KB 1.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: packaging in c:\\users\\ims\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
            "  Downloading protobuf-4.25.3-cp39-cp39-win_amd64.whl (413 kB)\n",
            "     -------------------------------------- 413.4/413.4 KB 2.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (58.1.0)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
            "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3\n",
            "  Downloading grpcio-1.62.1-cp39-cp39-win_amd64.whl (3.8 MB)\n",
            "     ---------------------------------------- 3.8/3.8 MB 2.6 MB/s eta 0:00:00\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ims\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.9.0)\n",
            "Collecting flatbuffers>=23.5.26\n",
            "  Downloading flatbuffers-24.3.7-py2.py3-none-any.whl (26 kB)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
            "     ---------------------------------------- 1.5/1.5 MB 2.8 MB/s eta 0:00:00\n",
            "Collecting tensorboard<2.17,>=2.16\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "     ---------------------------------------- 5.5/5.5 MB 2.5 MB/s eta 0:00:00\n",
            "Collecting google-pasta>=0.1.1\n",
            "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Collecting astunparse>=1.6.0\n",
            "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting h5py>=3.10.0\n",
            "  Downloading h5py-3.10.0-cp39-cp39-win_amd64.whl (2.7 MB)\n",
            "     ---------------------------------------- 2.7/2.7 MB 2.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ims\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\ims\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
            "Collecting keras>=3.0.0\n",
            "  Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n",
            "     ---------------------------------------- 1.0/1.0 MB 2.3 MB/s eta 0:00:00\n",
            "Collecting wrapt>=1.11.0\n",
            "  Downloading wrapt-1.16.0-cp39-cp39-win_amd64.whl (37 kB)\n",
            "Collecting absl-py>=1.0.0\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "     -------------------------------------- 133.7/133.7 KB 2.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\ims\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
            "Collecting libclang>=13.0.0\n",
            "  Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
            "     ---------------------------------------- 24.4/24.4 MB 3.7 MB/s eta 0:00:00\n",
            "Collecting wheel<1.0,>=0.23.0\n",
            "  Downloading wheel-0.43.0-py3-none-any.whl (65 kB)\n",
            "     ---------------------------------------- 65.8/65.8 KB 3.5 MB/s eta 0:00:00\n",
            "Collecting rich\n",
            "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
            "     -------------------------------------- 240.7/240.7 KB 7.2 MB/s eta 0:00:00\n",
            "Collecting dm-tree\n",
            "  Downloading dm_tree-0.1.8-cp39-cp39-win_amd64.whl (101 kB)\n",
            "     -------------------------------------- 101.5/101.5 KB 6.1 MB/s eta 0:00:00\n",
            "Collecting namex\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ims\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ims\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ims\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ims\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
            "     -------------------------------------- 103.9/103.9 KB 6.2 MB/s eta 0:00:00\n",
            "Collecting werkzeug>=1.0.1\n",
            "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
            "     -------------------------------------- 226.7/226.7 KB 7.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\ims\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (7.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ims\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n",
            "Collecting markdown-it-py>=2.2.0\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "     ---------------------------------------- 87.5/87.5 KB 4.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ims\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\ims\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.17.0)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: namex, libclang, flatbuffers, dm-tree, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, opt-einsum, ml-dtypes, mdurl, h5py, grpcio, google-pasta, gast, absl-py, markdown-it-py, markdown, astunparse, tensorboard, rich, keras, tensorflow-intel, tensorflow\n",
            "Successfully installed absl-py-2.1.0 astunparse-1.6.3 dm-tree-0.1.8 flatbuffers-24.3.7 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.1 h5py-3.10.0 keras-3.0.5 libclang-16.0.6 markdown-3.5.2 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.7 opt-einsum-3.3.0 protobuf-4.25.3 rich-13.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-intel-2.16.1 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0 werkzeug-3.0.1 wheel-0.43.0 wrapt-1.16.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
            "You should consider upgrading via the 'C:\\Users\\IMS\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-cFhQVFExe5",
        "outputId": "fe38890e-8afd-4d4e-ad69-9d8a4042cc7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\IMS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 - 6s - 692ms/step - loss: 801.0200 - val_loss: 699.5405\n",
            "Epoch 2/200\n",
            "8/8 - 4s - 466ms/step - loss: 756.7480 - val_loss: 663.0432\n",
            "Epoch 3/200\n",
            "8/8 - 4s - 460ms/step - loss: 724.2974 - val_loss: 622.7141\n",
            "Epoch 4/200\n",
            "8/8 - 4s - 462ms/step - loss: 671.8348 - val_loss: 579.3930\n",
            "Epoch 5/200\n",
            "8/8 - 4s - 458ms/step - loss: 619.4937 - val_loss: 534.3804\n",
            "Epoch 6/200\n",
            "8/8 - 4s - 464ms/step - loss: 608.8218 - val_loss: 485.9941\n",
            "Epoch 7/200\n",
            "8/8 - 4s - 460ms/step - loss: 538.3372 - val_loss: 437.6133\n",
            "Epoch 8/200\n",
            "8/8 - 4s - 457ms/step - loss: 491.8308 - val_loss: 395.7107\n",
            "Epoch 9/200\n",
            "8/8 - 4s - 460ms/step - loss: 450.5984 - val_loss: 362.8788\n",
            "Epoch 10/200\n",
            "8/8 - 4s - 458ms/step - loss: 430.1328 - val_loss: 337.3025\n",
            "Epoch 11/200\n",
            "8/8 - 4s - 456ms/step - loss: 389.2414 - val_loss: 316.9545\n",
            "Epoch 12/200\n",
            "8/8 - 4s - 457ms/step - loss: 380.2054 - val_loss: 300.1207\n",
            "Epoch 13/200\n",
            "8/8 - 4s - 465ms/step - loss: 366.0434 - val_loss: 285.0653\n",
            "Epoch 14/200\n",
            "8/8 - 4s - 458ms/step - loss: 347.3302 - val_loss: 271.4519\n",
            "Epoch 15/200\n",
            "8/8 - 4s - 466ms/step - loss: 329.1510 - val_loss: 258.7425\n",
            "Epoch 16/200\n",
            "8/8 - 4s - 462ms/step - loss: 320.7648 - val_loss: 247.0273\n",
            "Epoch 17/200\n",
            "8/8 - 4s - 459ms/step - loss: 291.8070 - val_loss: 235.9626\n",
            "Epoch 18/200\n",
            "8/8 - 4s - 459ms/step - loss: 288.6321 - val_loss: 225.8530\n",
            "Epoch 19/200\n",
            "8/8 - 4s - 461ms/step - loss: 270.2799 - val_loss: 216.0113\n",
            "Epoch 20/200\n",
            "8/8 - 4s - 461ms/step - loss: 261.9871 - val_loss: 206.9726\n",
            "Epoch 21/200\n",
            "8/8 - 4s - 463ms/step - loss: 261.0577 - val_loss: 198.5468\n",
            "Epoch 22/200\n",
            "8/8 - 4s - 456ms/step - loss: 259.2426 - val_loss: 190.0308\n",
            "Epoch 23/200\n",
            "8/8 - 4s - 480ms/step - loss: 245.4926 - val_loss: 181.8895\n",
            "Epoch 24/200\n",
            "8/8 - 4s - 477ms/step - loss: 246.2540 - val_loss: 174.1318\n",
            "Epoch 25/200\n",
            "8/8 - 4s - 462ms/step - loss: 227.0836 - val_loss: 166.6794\n",
            "Epoch 26/200\n",
            "8/8 - 4s - 467ms/step - loss: 225.5011 - val_loss: 159.5551\n",
            "Epoch 27/200\n",
            "8/8 - 4s - 466ms/step - loss: 206.1142 - val_loss: 152.6382\n",
            "Epoch 28/200\n",
            "8/8 - 4s - 463ms/step - loss: 201.6424 - val_loss: 146.2291\n",
            "Epoch 29/200\n",
            "8/8 - 4s - 457ms/step - loss: 203.9540 - val_loss: 140.1859\n",
            "Epoch 30/200\n",
            "8/8 - 4s - 467ms/step - loss: 190.8730 - val_loss: 134.1937\n",
            "Epoch 31/200\n",
            "8/8 - 4s - 461ms/step - loss: 189.9006 - val_loss: 128.5299\n",
            "Epoch 32/200\n",
            "8/8 - 4s - 456ms/step - loss: 182.6671 - val_loss: 123.1306\n",
            "Epoch 33/200\n",
            "8/8 - 4s - 464ms/step - loss: 169.3626 - val_loss: 117.7208\n",
            "Epoch 34/200\n",
            "8/8 - 4s - 459ms/step - loss: 167.3049 - val_loss: 112.6230\n",
            "Epoch 35/200\n",
            "8/8 - 4s - 468ms/step - loss: 157.9460 - val_loss: 107.8374\n",
            "Epoch 36/200\n",
            "8/8 - 4s - 461ms/step - loss: 151.5469 - val_loss: 103.3927\n",
            "Epoch 37/200\n",
            "8/8 - 4s - 463ms/step - loss: 149.8987 - val_loss: 99.1375\n",
            "Epoch 38/200\n",
            "8/8 - 4s - 464ms/step - loss: 144.5081 - val_loss: 94.9311\n",
            "Epoch 39/200\n",
            "8/8 - 4s - 458ms/step - loss: 138.2417 - val_loss: 90.9830\n",
            "Epoch 40/200\n",
            "8/8 - 4s - 462ms/step - loss: 138.9560 - val_loss: 87.2035\n",
            "Epoch 41/200\n",
            "8/8 - 4s - 463ms/step - loss: 137.7379 - val_loss: 83.6940\n",
            "Epoch 42/200\n",
            "8/8 - 4s - 466ms/step - loss: 127.2605 - val_loss: 80.1912\n",
            "Epoch 43/200\n",
            "8/8 - 4s - 462ms/step - loss: 129.0706 - val_loss: 76.7982\n",
            "Epoch 44/200\n",
            "8/8 - 4s - 462ms/step - loss: 126.1900 - val_loss: 73.6584\n",
            "Epoch 45/200\n",
            "8/8 - 4s - 464ms/step - loss: 124.2592 - val_loss: 70.6826\n",
            "Epoch 46/200\n",
            "8/8 - 4s - 463ms/step - loss: 119.5575 - val_loss: 67.6927\n",
            "Epoch 47/200\n",
            "8/8 - 4s - 459ms/step - loss: 111.8391 - val_loss: 64.9234\n",
            "Epoch 48/200\n",
            "8/8 - 4s - 463ms/step - loss: 107.3892 - val_loss: 62.3344\n",
            "Epoch 49/200\n",
            "8/8 - 4s - 459ms/step - loss: 105.1710 - val_loss: 59.7781\n",
            "Epoch 50/200\n",
            "8/8 - 4s - 462ms/step - loss: 110.4838 - val_loss: 57.4289\n",
            "Epoch 51/200\n",
            "8/8 - 4s - 459ms/step - loss: 99.9617 - val_loss: 55.1267\n",
            "Epoch 52/200\n",
            "8/8 - 4s - 464ms/step - loss: 96.1778 - val_loss: 53.0493\n",
            "Epoch 53/200\n",
            "8/8 - 4s - 460ms/step - loss: 92.8543 - val_loss: 50.9439\n",
            "Epoch 54/200\n",
            "8/8 - 4s - 460ms/step - loss: 95.2568 - val_loss: 49.0732\n",
            "Epoch 55/200\n",
            "8/8 - 4s - 463ms/step - loss: 96.7914 - val_loss: 47.2886\n",
            "Epoch 56/200\n",
            "8/8 - 4s - 459ms/step - loss: 87.1817 - val_loss: 45.5211\n",
            "Epoch 57/200\n",
            "8/8 - 4s - 465ms/step - loss: 86.6773 - val_loss: 43.8871\n",
            "Epoch 58/200\n",
            "8/8 - 4s - 461ms/step - loss: 84.5196 - val_loss: 42.3873\n",
            "Epoch 59/200\n",
            "8/8 - 4s - 465ms/step - loss: 85.2469 - val_loss: 40.8819\n",
            "Epoch 60/200\n",
            "8/8 - 4s - 462ms/step - loss: 79.4412 - val_loss: 39.5794\n",
            "Epoch 61/200\n",
            "8/8 - 4s - 458ms/step - loss: 79.5430 - val_loss: 38.2458\n",
            "Epoch 62/200\n",
            "8/8 - 4s - 460ms/step - loss: 82.3432 - val_loss: 37.0269\n",
            "Epoch 63/200\n",
            "8/8 - 4s - 462ms/step - loss: 75.3941 - val_loss: 35.8054\n",
            "Epoch 64/200\n",
            "8/8 - 4s - 462ms/step - loss: 75.1275 - val_loss: 34.6217\n",
            "Epoch 65/200\n",
            "8/8 - 4s - 463ms/step - loss: 74.9930 - val_loss: 33.6489\n",
            "Epoch 66/200\n",
            "8/8 - 4s - 463ms/step - loss: 75.1086 - val_loss: 32.7434\n",
            "Epoch 67/200\n",
            "8/8 - 4s - 461ms/step - loss: 71.9044 - val_loss: 31.7058\n",
            "Epoch 68/200\n",
            "8/8 - 4s - 484ms/step - loss: 70.1480 - val_loss: 30.8009\n",
            "Epoch 69/200\n",
            "8/8 - 4s - 471ms/step - loss: 70.7600 - val_loss: 30.0366\n",
            "Epoch 70/200\n",
            "8/8 - 4s - 490ms/step - loss: 72.5952 - val_loss: 29.2591\n",
            "Epoch 71/200\n",
            "8/8 - 4s - 480ms/step - loss: 67.7160 - val_loss: 28.5615\n",
            "Epoch 72/200\n",
            "8/8 - 4s - 466ms/step - loss: 67.1916 - val_loss: 27.9312\n",
            "Epoch 73/200\n",
            "8/8 - 4s - 464ms/step - loss: 68.4965 - val_loss: 27.3512\n",
            "Epoch 74/200\n",
            "8/8 - 4s - 462ms/step - loss: 65.0050 - val_loss: 26.7079\n",
            "Epoch 75/200\n",
            "8/8 - 4s - 458ms/step - loss: 66.6413 - val_loss: 26.1895\n",
            "Epoch 76/200\n",
            "8/8 - 4s - 463ms/step - loss: 62.4889 - val_loss: 25.6488\n",
            "Epoch 77/200\n",
            "8/8 - 4s - 458ms/step - loss: 64.5652 - val_loss: 25.1795\n",
            "Epoch 78/200\n",
            "8/8 - 4s - 459ms/step - loss: 64.0409 - val_loss: 24.7163\n",
            "Epoch 79/200\n",
            "8/8 - 4s - 456ms/step - loss: 62.6345 - val_loss: 24.3215\n",
            "Epoch 80/200\n",
            "8/8 - 4s - 457ms/step - loss: 59.2292 - val_loss: 23.9169\n",
            "Epoch 81/200\n",
            "8/8 - 4s - 457ms/step - loss: 63.5780 - val_loss: 23.5596\n",
            "Epoch 82/200\n",
            "8/8 - 4s - 461ms/step - loss: 61.1942 - val_loss: 23.2306\n",
            "Epoch 83/200\n",
            "8/8 - 4s - 455ms/step - loss: 60.2968 - val_loss: 22.9141\n",
            "Epoch 84/200\n",
            "8/8 - 4s - 462ms/step - loss: 60.6400 - val_loss: 22.6064\n",
            "Epoch 85/200\n",
            "8/8 - 4s - 456ms/step - loss: 56.6523 - val_loss: 22.3369\n",
            "Epoch 86/200\n",
            "8/8 - 4s - 459ms/step - loss: 60.2355 - val_loss: 22.0894\n",
            "Epoch 87/200\n",
            "8/8 - 4s - 457ms/step - loss: 61.2262 - val_loss: 21.9381\n",
            "Epoch 88/200\n",
            "8/8 - 4s - 457ms/step - loss: 59.3954 - val_loss: 21.7620\n",
            "Epoch 89/200\n",
            "8/8 - 4s - 465ms/step - loss: 60.6811 - val_loss: 21.5851\n",
            "Epoch 90/200\n",
            "8/8 - 4s - 459ms/step - loss: 58.0364 - val_loss: 21.4104\n",
            "Epoch 91/200\n",
            "8/8 - 4s - 456ms/step - loss: 56.9139 - val_loss: 21.2472\n",
            "Epoch 92/200\n",
            "8/8 - 4s - 461ms/step - loss: 55.3870 - val_loss: 21.0809\n",
            "Epoch 93/200\n",
            "8/8 - 4s - 460ms/step - loss: 57.3710 - val_loss: 20.9814\n",
            "Epoch 94/200\n",
            "8/8 - 4s - 461ms/step - loss: 57.0510 - val_loss: 20.8876\n",
            "Epoch 95/200\n",
            "8/8 - 4s - 464ms/step - loss: 54.2317 - val_loss: 20.8011\n",
            "Epoch 96/200\n",
            "8/8 - 4s - 462ms/step - loss: 55.3603 - val_loss: 20.7174\n",
            "Epoch 97/200\n",
            "8/8 - 4s - 460ms/step - loss: 57.7236 - val_loss: 20.6444\n",
            "Epoch 98/200\n",
            "8/8 - 4s - 456ms/step - loss: 56.1350 - val_loss: 20.5622\n",
            "Epoch 99/200\n",
            "8/8 - 4s - 463ms/step - loss: 55.9026 - val_loss: 20.5000\n",
            "Epoch 100/200\n",
            "8/8 - 4s - 456ms/step - loss: 54.7416 - val_loss: 20.4517\n",
            "Epoch 101/200\n",
            "8/8 - 4s - 460ms/step - loss: 53.3134 - val_loss: 20.4122\n",
            "Epoch 102/200\n",
            "8/8 - 4s - 468ms/step - loss: 53.9296 - val_loss: 20.3593\n",
            "Epoch 103/200\n",
            "8/8 - 4s - 463ms/step - loss: 56.5706 - val_loss: 20.3390\n",
            "Epoch 104/200\n",
            "8/8 - 4s - 463ms/step - loss: 57.4242 - val_loss: 20.3022\n",
            "Epoch 105/200\n",
            "8/8 - 4s - 457ms/step - loss: 53.5349 - val_loss: 20.2705\n",
            "Epoch 106/200\n",
            "8/8 - 4s - 464ms/step - loss: 56.3794 - val_loss: 20.2577\n",
            "Epoch 107/200\n",
            "8/8 - 4s - 467ms/step - loss: 52.4763 - val_loss: 20.2422\n",
            "Epoch 108/200\n",
            "8/8 - 4s - 464ms/step - loss: 56.5111 - val_loss: 20.2318\n",
            "Epoch 109/200\n",
            "8/8 - 4s - 468ms/step - loss: 53.1673 - val_loss: 20.2256\n",
            "Epoch 110/200\n",
            "8/8 - 4s - 465ms/step - loss: 53.1727 - val_loss: 20.2218\n",
            "Epoch 111/200\n",
            "8/8 - 4s - 463ms/step - loss: 57.9012 - val_loss: 20.2200\n",
            "Epoch 112/200\n",
            "8/8 - 4s - 460ms/step - loss: 52.1269 - val_loss: 20.2208\n",
            "Epoch 113/200\n",
            "8/8 - 4s - 460ms/step - loss: 56.5934 - val_loss: 20.2245\n",
            "Epoch 114/200\n",
            "8/8 - 4s - 463ms/step - loss: 55.9528 - val_loss: 20.2294\n",
            "Epoch 115/200\n",
            "8/8 - 4s - 464ms/step - loss: 53.9059 - val_loss: 20.2321\n",
            "Epoch 116/200\n",
            "8/8 - 4s - 462ms/step - loss: 54.3573 - val_loss: 20.2377\n",
            "Epoch 117/200\n",
            "8/8 - 4s - 462ms/step - loss: 55.3102 - val_loss: 20.2471\n",
            "Epoch 118/200\n",
            "8/8 - 4s - 464ms/step - loss: 55.1368 - val_loss: 20.2627\n",
            "Epoch 119/200\n",
            "8/8 - 4s - 461ms/step - loss: 55.4119 - val_loss: 20.2669\n",
            "Epoch 120/200\n",
            "8/8 - 4s - 460ms/step - loss: 52.3458 - val_loss: 20.2765\n",
            "Epoch 121/200\n",
            "8/8 - 4s - 461ms/step - loss: 52.7523 - val_loss: 20.2893\n",
            "Epoch 122/200\n",
            "8/8 - 4s - 461ms/step - loss: 54.8936 - val_loss: 20.2937\n",
            "Epoch 123/200\n",
            "8/8 - 4s - 464ms/step - loss: 55.4520 - val_loss: 20.3207\n",
            "Epoch 124/200\n",
            "8/8 - 4s - 463ms/step - loss: 54.1083 - val_loss: 20.3332\n",
            "Epoch 125/200\n",
            "8/8 - 4s - 459ms/step - loss: 57.1693 - val_loss: 20.3505\n",
            "Epoch 126/200\n",
            "8/8 - 4s - 458ms/step - loss: 53.6740 - val_loss: 20.3673\n",
            "Epoch 127/200\n",
            "8/8 - 4s - 463ms/step - loss: 51.6416 - val_loss: 20.3839\n",
            "Epoch 128/200\n",
            "8/8 - 4s - 461ms/step - loss: 51.7374 - val_loss: 20.3967\n",
            "Epoch 129/200\n",
            "8/8 - 4s - 460ms/step - loss: 52.5440 - val_loss: 20.4139\n",
            "Epoch 130/200\n",
            "8/8 - 4s - 462ms/step - loss: 52.6040 - val_loss: 20.4282\n",
            "Epoch 131/200\n",
            "8/8 - 4s - 457ms/step - loss: 54.0398 - val_loss: 20.4340\n",
            "Epoch 132/200\n",
            "8/8 - 4s - 457ms/step - loss: 56.5822 - val_loss: 20.4416\n",
            "Epoch 133/200\n",
            "8/8 - 4s - 457ms/step - loss: 51.7094 - val_loss: 20.4769\n",
            "Epoch 134/200\n",
            "8/8 - 4s - 463ms/step - loss: 57.7509 - val_loss: 20.4768\n",
            "Epoch 135/200\n",
            "8/8 - 4s - 458ms/step - loss: 56.4563 - val_loss: 20.5230\n",
            "Epoch 136/200\n",
            "8/8 - 4s - 457ms/step - loss: 51.7245 - val_loss: 20.5504\n",
            "Epoch 137/200\n",
            "8/8 - 4s - 460ms/step - loss: 51.5597 - val_loss: 20.5671\n",
            "Epoch 138/200\n",
            "8/8 - 4s - 459ms/step - loss: 54.8990 - val_loss: 20.5903\n",
            "Epoch 139/200\n",
            "8/8 - 4s - 462ms/step - loss: 56.4539 - val_loss: 20.6195\n",
            "Epoch 140/200\n",
            "8/8 - 4s - 456ms/step - loss: 54.3428 - val_loss: 20.6411\n",
            "Epoch 141/200\n",
            "8/8 - 4s - 458ms/step - loss: 52.3295 - val_loss: 20.6603\n",
            "Epoch 142/200\n",
            "8/8 - 4s - 458ms/step - loss: 54.6971 - val_loss: 20.6683\n",
            "Epoch 143/200\n",
            "8/8 - 4s - 456ms/step - loss: 53.2672 - val_loss: 20.6429\n",
            "Epoch 144/200\n",
            "8/8 - 4s - 459ms/step - loss: 52.0769 - val_loss: 20.6679\n",
            "Epoch 145/200\n",
            "8/8 - 4s - 457ms/step - loss: 51.9685 - val_loss: 20.6732\n",
            "Epoch 146/200\n",
            "8/8 - 4s - 463ms/step - loss: 57.0679 - val_loss: 20.6955\n",
            "Epoch 147/200\n",
            "8/8 - 4s - 460ms/step - loss: 51.5333 - val_loss: 20.7387\n",
            "Epoch 148/200\n",
            "8/8 - 4s - 461ms/step - loss: 52.0507 - val_loss: 20.7653\n",
            "Epoch 149/200\n",
            "8/8 - 4s - 457ms/step - loss: 55.4635 - val_loss: 20.7777\n",
            "Epoch 150/200\n",
            "8/8 - 4s - 459ms/step - loss: 51.5611 - val_loss: 20.8128\n",
            "Epoch 151/200\n",
            "8/8 - 4s - 464ms/step - loss: 52.9199 - val_loss: 20.8172\n",
            "Epoch 152/200\n",
            "8/8 - 4s - 460ms/step - loss: 57.0042 - val_loss: 20.8151\n",
            "Epoch 153/200\n",
            "8/8 - 4s - 461ms/step - loss: 53.1349 - val_loss: 20.8148\n",
            "Epoch 154/200\n",
            "8/8 - 4s - 468ms/step - loss: 52.6386 - val_loss: 20.8492\n",
            "Epoch 155/200\n",
            "8/8 - 4s - 465ms/step - loss: 54.7808 - val_loss: 20.8581\n",
            "Epoch 156/200\n",
            "8/8 - 4s - 461ms/step - loss: 53.3159 - val_loss: 20.8786\n",
            "Epoch 157/200\n",
            "8/8 - 4s - 461ms/step - loss: 54.3285 - val_loss: 20.8646\n",
            "Epoch 158/200\n",
            "8/8 - 4s - 459ms/step - loss: 52.7086 - val_loss: 20.8908\n",
            "Epoch 159/200\n",
            "8/8 - 4s - 461ms/step - loss: 57.5602 - val_loss: 20.9206\n",
            "Epoch 160/200\n",
            "8/8 - 4s - 458ms/step - loss: 53.5657 - val_loss: 20.9261\n",
            "Epoch 161/200\n",
            "8/8 - 4s - 461ms/step - loss: 53.4771 - val_loss: 20.9108\n",
            "Epoch 162/200\n",
            "8/8 - 4s - 463ms/step - loss: 51.2567 - val_loss: 20.9358\n",
            "Epoch 163/200\n",
            "8/8 - 4s - 462ms/step - loss: 51.8822 - val_loss: 20.9315\n",
            "Epoch 164/200\n",
            "8/8 - 4s - 458ms/step - loss: 54.6592 - val_loss: 20.9789\n",
            "Epoch 165/200\n",
            "8/8 - 4s - 461ms/step - loss: 52.4253 - val_loss: 20.9360\n",
            "Epoch 166/200\n",
            "8/8 - 4s - 465ms/step - loss: 55.9871 - val_loss: 20.9247\n",
            "Epoch 167/200\n",
            "8/8 - 4s - 460ms/step - loss: 57.3442 - val_loss: 20.9847\n",
            "Epoch 168/200\n",
            "8/8 - 4s - 460ms/step - loss: 52.6489 - val_loss: 20.9730\n",
            "Epoch 169/200\n",
            "8/8 - 4s - 463ms/step - loss: 52.5087 - val_loss: 20.9651\n",
            "Epoch 170/200\n",
            "8/8 - 4s - 461ms/step - loss: 54.6879 - val_loss: 20.9629\n",
            "Epoch 171/200\n",
            "8/8 - 4s - 462ms/step - loss: 55.0430 - val_loss: 21.0220\n",
            "Epoch 172/200\n",
            "8/8 - 4s - 460ms/step - loss: 52.5712 - val_loss: 21.0462\n",
            "Epoch 173/200\n",
            "8/8 - 4s - 463ms/step - loss: 54.6714 - val_loss: 21.0433\n",
            "Epoch 174/200\n",
            "8/8 - 4s - 461ms/step - loss: 52.5582 - val_loss: 21.0437\n",
            "Epoch 175/200\n",
            "8/8 - 4s - 466ms/step - loss: 53.8394 - val_loss: 20.7086\n",
            "Epoch 176/200\n",
            "8/8 - 4s - 460ms/step - loss: 52.6508 - val_loss: 20.9480\n",
            "Epoch 177/200\n",
            "8/8 - 4s - 464ms/step - loss: 52.5531 - val_loss: 21.0037\n",
            "Epoch 178/200\n",
            "8/8 - 4s - 460ms/step - loss: 52.2506 - val_loss: 20.8820\n",
            "Epoch 179/200\n",
            "8/8 - 4s - 460ms/step - loss: 53.4885 - val_loss: 20.8941\n",
            "Epoch 180/200\n",
            "8/8 - 4s - 458ms/step - loss: 52.7055 - val_loss: 20.8190\n",
            "Epoch 181/200\n",
            "8/8 - 4s - 458ms/step - loss: 58.8562 - val_loss: 20.8886\n",
            "Epoch 182/200\n",
            "8/8 - 4s - 458ms/step - loss: 54.9013 - val_loss: 20.8824\n",
            "Epoch 183/200\n",
            "8/8 - 4s - 464ms/step - loss: 51.9608 - val_loss: 21.0937\n",
            "Epoch 184/200\n",
            "8/8 - 4s - 461ms/step - loss: 53.2259 - val_loss: 21.1362\n",
            "Epoch 185/200\n",
            "8/8 - 4s - 460ms/step - loss: 54.3295 - val_loss: 21.1574\n",
            "Epoch 186/200\n",
            "8/8 - 4s - 461ms/step - loss: 50.7381 - val_loss: 21.1944\n",
            "Epoch 187/200\n",
            "8/8 - 4s - 457ms/step - loss: 52.0457 - val_loss: 21.3025\n",
            "Epoch 188/200\n",
            "8/8 - 4s - 465ms/step - loss: 53.1165 - val_loss: 21.3921\n",
            "Epoch 189/200\n",
            "8/8 - 4s - 461ms/step - loss: 52.0941 - val_loss: 21.5050\n",
            "Epoch 190/200\n",
            "8/8 - 4s - 460ms/step - loss: 50.0959 - val_loss: 21.5390\n",
            "Epoch 191/200\n",
            "8/8 - 4s - 459ms/step - loss: 50.5001 - val_loss: 21.6232\n",
            "Epoch 192/200\n",
            "8/8 - 4s - 460ms/step - loss: 52.6992 - val_loss: 21.6783\n",
            "Epoch 193/200\n",
            "8/8 - 4s - 463ms/step - loss: 52.3287 - val_loss: 21.6851\n",
            "Epoch 194/200\n",
            "8/8 - 4s - 460ms/step - loss: 52.8026 - val_loss: 21.7165\n",
            "Epoch 195/200\n",
            "8/8 - 4s - 460ms/step - loss: 49.2926 - val_loss: 21.8274\n",
            "Epoch 196/200\n",
            "8/8 - 4s - 461ms/step - loss: 48.8758 - val_loss: 21.8961\n",
            "Epoch 197/200\n",
            "8/8 - 4s - 462ms/step - loss: 51.6369 - val_loss: 21.9965\n",
            "Epoch 198/200\n",
            "8/8 - 4s - 494ms/step - loss: 47.8634 - val_loss: 22.0367\n",
            "Epoch 199/200\n",
            "8/8 - 4s - 490ms/step - loss: 52.3647 - val_loss: 22.0839\n",
            "Epoch 200/200\n",
            "8/8 - 4s - 477ms/step - loss: 49.8210 - val_loss: 22.0415\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
            "Fold 7 RMSE: 6.574817657470703\n",
            "\n",
            "Average RMSE across folds: 6.574817657470703\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Leave-One-Out Cross-Validation\n",
        "selected_fold_index = 7\n",
        "fold_rmse = []\n",
        "\n",
        "for i in range(num_samples):\n",
        "\n",
        "    if i != selected_fold_index:\n",
        "        continue\n",
        "\n",
        "    x_train = np.concatenate([x[:i], x[i+1:]], axis=0)\n",
        "    x_test = x[i:i+1]\n",
        "    y_train = np.concatenate([y[:i], y[i+1:]], axis=0)\n",
        "    y_test = y[i:i+1]\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(20, 3, activation='relu', input_shape=(10000, 1)))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Conv1D(40, 3, activation='relu'))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    history = model.fit(x_train, y_train, epochs=10, batch_size=5, validation_split=0.2, verbose=2)\n",
        "\n",
        "    predictions = model.predict(x_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "    fold_rmse.append(rmse)\n",
        "\n",
        "    print(f'Fold {i} RMSE: {rmse}\\n')\n",
        "\n",
        "print(f'Average RMSE across folds: {np.mean(fold_rmse)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65oWuQbMEx_C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhaXhpiNNuTw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}